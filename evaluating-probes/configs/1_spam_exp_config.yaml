run_name: "spam_experiment_gemma"
model_name: "gemma-2-9b" # 1.2 B multilignual model, 512 seq length
d_model: 3584 # 3584 for gemma-2-9b
device: "cuda:0"
use_grouped_training: True
cache_activations: True
log_file: "logs/spam_experiment_gemma_seeds_42_46.log"  # Custom log file path
# seed: 42  # Old single seed format (still supported for backward compatibility)
seeds: [42, 43, 44, 45, 46]  # New multi-seed format, 43, 44, 45, 46

layers: [20] # Where SAE probes found layers peform the bests
components:
  - "resid_post"

# Evaluation dataset configurations with thresholds for filtered scoring
eval_dataset_configs:
  94_better_spam:
    threshold_class_0: 2.0  # Threshold for class 0 (Ham) examples - only include if |logit_diff| > 2.0
    threshold_class_1: 2.0  # Threshold for class 1 (Spam) examples - only include if |logit_diff| > 2.0
  87_is_spam:
    threshold_class_0: 1.5  # Different thresholds for this dataset
    threshold_class_1: 1.5  # Lower thresholds = more examples included in filtered scoring

architectures:
  - name: "linear"
    config_name: "linear_mean"
  - name: "linear"
    config_name: "linear_max"
  - name: "linear"
    config_name: "linear_last"
  - name: "linear"
    config_name: "linear_softmax"
  # - name: "linear" # use linear max or something, and overwrite 2-exp results later wiht linear_mean normal, those are linear_mean high_reg right now I think
  #   config_name: "linear_mean_high_reg"
  - name: "attention"
    config_name: "default_attention"
  - name: "sae_16k_l0_408_last" # agg across seq dim
    config_name: "sae_16k_l0_408_last"
  - name: "sae_262k_l0_259_last" # agg across seq dim
    config_name: "sae_262k_l0_259_last"
  - name: "act_sim_last"
    config_name: "act_sim_last"
  - name: "act_sim_max"
    config_name: "act_sim_max"
  # add mean once you fix the nan issue


# ToDo: filtered scoring is currently broken, and I need to make sure it works for new ds's too

# List of experiments to run
experiments:
  - name: 1-spam-pred-auc
    metric: auc
    class_names: {0: "Ham", 1: "Spam"}
    train_on: 94_better_spam
    evaluate_on:
      - 94_better_spam
      - 87_is_spam
    score:
      - filtered
  - name: 2-spam-pred-auc-increasing-spam-fixed-total
    metric: auc
    class_names: {0: "Ham", 1: "Spam"}
    train_on: 94_better_spam
    evaluate_on:
      - 94_better_spam
      - 87_is_spam
    score:
      - filtered
    rebuild_config:
      increasing_spam_fixed_total:
        - {class_counts: {0: 3000, 1: 1}} # we should use all of our examples
        - {class_counts: {0: 3000, 1: 2}}
        - {class_counts: {0: 3000, 1: 5}}
        - {class_counts: {0: 3000, 1: 10}}
        - {class_counts: {0: 3000, 1: 15}}
        - {class_counts: {0: 3000, 1: 20}}
        - {class_counts: {0: 3000, 1: 25}}
        - {class_counts: {0: 3000, 1: 35}}
        - {class_counts: {0: 3000, 1: 45}}
        - {class_counts: {0: 3000, 1: 50}}
        - {class_counts: {0: 3000, 1: 100}}
        - {class_counts: {0: 3000, 1: 150}}
        - {class_counts: {0: 3000, 1: 250}}
        - {class_counts: {0: 3000, 1: 500}}
        # - {class_counts: {0: 3250, 1: 750}} don't have that many samples
  - name: 3-spam-pred-auc-llm-upsampling
    metric: auc
    class_names: {0: "Ham", 1: "Spam"}
    train_on: 94_better_spam
    evaluate_on:
      - 94_better_spam
    score:
      - all
      - filtered
    rebuild_config:
      llm_upsampling_experiments:
        # Fixed 3000 real negative samples, variable real positive samples (1, 2, 3, 4, 5, 10)
        # For each positive count, train 5 probes: 1x, 2x, 3x, 4x, 5x upsampling
  
        - {llm_upsampling: True, n_real_neg: 3000, n_real_pos: 10, upsampling_factor: 10}
        - {llm_upsampling: True, n_real_neg: 3000, n_real_pos: 10, upsampling_factor: 5}
        - {llm_upsampling: True, n_real_neg: 3000, n_real_pos: 10, upsampling_factor: 1}
        - {llm_upsampling: True, n_real_neg: 3000, n_real_pos: 10, upsampling_factor: 2}
        - {llm_upsampling: True, n_real_neg: 3000, n_real_pos: 10, upsampling_factor: 3}
        - {llm_upsampling: True, n_real_neg: 3000, n_real_pos: 10, upsampling_factor: 4}

        - {llm_upsampling: True, n_real_neg: 3000, n_real_pos: 1, upsampling_factor: 10}
        - {llm_upsampling: True, n_real_neg: 3000, n_real_pos: 1, upsampling_factor: 5}
        - {llm_upsampling: True, n_real_neg: 3000, n_real_pos: 1, upsampling_factor: 1}
        - {llm_upsampling: True, n_real_neg: 3000, n_real_pos: 1, upsampling_factor: 2}
        - {llm_upsampling: True, n_real_neg: 3000, n_real_pos: 1, upsampling_factor: 3}
        - {llm_upsampling: True, n_real_neg: 3000, n_real_pos: 1, upsampling_factor: 4}

        - {llm_upsampling: True, n_real_neg: 3000, n_real_pos: 2, upsampling_factor: 10}
        - {llm_upsampling: True, n_real_neg: 3000, n_real_pos: 2, upsampling_factor: 5}
        - {llm_upsampling: True, n_real_neg: 3000, n_real_pos: 2, upsampling_factor: 1}
        - {llm_upsampling: True, n_real_neg: 3000, n_real_pos: 2, upsampling_factor: 2}
        - {llm_upsampling: True, n_real_neg: 3000, n_real_pos: 2, upsampling_factor: 3}
        - {llm_upsampling: True, n_real_neg: 3000, n_real_pos: 2, upsampling_factor: 4}
        
        - {llm_upsampling: True, n_real_neg: 3000, n_real_pos: 3, upsampling_factor: 10}
        - {llm_upsampling: True, n_real_neg: 3000, n_real_pos: 3, upsampling_factor: 5}
        - {llm_upsampling: True, n_real_neg: 3000, n_real_pos: 3, upsampling_factor: 1}
        - {llm_upsampling: True, n_real_neg: 3000, n_real_pos: 3, upsampling_factor: 2}
        - {llm_upsampling: True, n_real_neg: 3000, n_real_pos: 3, upsampling_factor: 3}
        - {llm_upsampling: True, n_real_neg: 3000, n_real_pos: 3, upsampling_factor: 4}
        
        - {llm_upsampling: True, n_real_neg: 3000, n_real_pos: 4, upsampling_factor: 10}
        - {llm_upsampling: True, n_real_neg: 3000, n_real_pos: 4, upsampling_factor: 5}
        - {llm_upsampling: True, n_real_neg: 3000, n_real_pos: 4, upsampling_factor: 1}
        - {llm_upsampling: True, n_real_neg: 3000, n_real_pos: 4, upsampling_factor: 2}
        - {llm_upsampling: True, n_real_neg: 3000, n_real_pos: 4, upsampling_factor: 3}
        - {llm_upsampling: True, n_real_neg: 3000, n_real_pos: 4, upsampling_factor: 4}
        
        - {llm_upsampling: True, n_real_neg: 3000, n_real_pos: 5, upsampling_factor: 10}
        - {llm_upsampling: True, n_real_neg: 3000, n_real_pos: 5, upsampling_factor: 5}
        - {llm_upsampling: True, n_real_neg: 3000, n_real_pos: 5, upsampling_factor: 1}
        - {llm_upsampling: True, n_real_neg: 3000, n_real_pos: 5, upsampling_factor: 2}
        - {llm_upsampling: True, n_real_neg: 3000, n_real_pos: 5, upsampling_factor: 3}
        - {llm_upsampling: True, n_real_neg: 3000, n_real_pos: 5, upsampling_factor: 4}
        

model_check:
  - name: spam-pred
    hf_model_name: "google/gemma-2-9b-it"
    check_on: ["94_better_spam", "87_is_spam"]
    check_prompt: "Is this email spam? Answer with either Yes or No. \n {prompt}"
    class_names: {0: ["Yes", "yes", " Yes", " yes"], 1: ["No", "no", " No", " no"]}