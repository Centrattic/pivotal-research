# configs/main_config.yaml

run_name: "french_probing_gemma"
model_name: "gemma-2-9b" # 1.2 B multilignual model, 512 seq length
device: "cuda:1"
cache_activations: True
seed: "42"

layers: [20] # Where SAE probes found layers peform the best
components:
  - "resid_post"

architectures:
  - name: "linear"
    config_name: "default_linear"
  - name: "attention"
    config_name: "default_attention"

aggregations:
  - "mean"

# List of experiments to run
experiments:
  - name: french_questions
    train_on: 90_french_truthqa_tf
    evaluate_on:
      - 90_french_truthqa_tf

  - name: eng_french_language
    train_on: 91_eng-french
    evaluate_on:
      - 91_eng-french

  - name: english_questions
    train_on: 33_truthqa_tf
    evaluate_on:
      - 33_truthqa_tf

