run_name: "llama_honesty_experiment"
model_name: "meta-llama/Llama-3.3-70B-Instruct" # Llama-2-70b model
d_model: 8192 # 8192 for Llama-2-70b
device: "cuda:0"
use_grouped_training: True
cache_activations: True
log_file: "logs/llama_honesty_experiment_seeds_42_46.log"  # Custom log file path
seeds: [42, 43, 44, 45, 46]  # Multi-seed format

layers: [22] # Where SAE probes found layers perform the best
components:
  - "resid_post"

# Evaluation dataset configurations with thresholds for filtered scoring
eval_dataset_configs:

architectures:
  - name: "linear"
    config_name: "linear_mean"
  - name: "linear"
    config_name: "linear_max"
  - name: "linear"
    config_name: "linear_last"
  - name: "linear"
    config_name: "linear_softmax"

# List of experiments to run
experiments:
  - name: 1-honesty-pred-auc
    metric: auc
    class_names: {0: "Dishonest", 1: "Honest"}
    train_on: 97_llama_combined_honesty
    evaluate_on:
      - 97_llama_combined_honesty
    score:
      - all # mask does the filtering for us
  - name: 2-honesty-pred-auc-increasing-honest-fixed-total
    metric: auc
    class_names: {0: "Dishonest", 1: "Honest"}
    train_on: 97_llama_combined_honesty
    evaluate_on:
      - 97_llama_combined_honesty
    score:
      - all
    rebuild_config:
      increasing_honest_fixed_total:
        - {class_counts: {0: 3000, 1: 1}} # we should use all of our examples
        - {class_counts: {0: 3000, 1: 2}}
        - {class_counts: {0: 3000, 1: 5}}
        - {class_counts: {0: 3000, 1: 10}}
        - {class_counts: {0: 3000, 1: 15}}
        - {class_counts: {0: 3000, 1: 20}}
        - {class_counts: {0: 3000, 1: 25}}
        - {class_counts: {0: 3000, 1: 35}}
        - {class_counts: {0: 3000, 1: 45}}
        - {class_counts: {0: 3000, 1: 50}}
        - {class_counts: {0: 3000, 1: 100}}
        - {class_counts: {0: 3000, 1: 150}}
        - {class_counts: {0: 3000, 1: 250}}
        - {class_counts: {0: 3000, 1: 500}}
