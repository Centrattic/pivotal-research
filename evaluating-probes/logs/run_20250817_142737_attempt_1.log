2025-08-17 14:27:37 Logging attempt 1 to ./logs/run_20250817_142737_attempt_1.log
2025-08-17 14:27:37 Starting (attempt 1) → python -m src.main -c gemma_spam_cpu
Loaded environment variables from /home/riya/pivotal/pivotal-research/evaluating-probes/.env

=== DEBUG: Environment Variables ===
OPENAI_API_KEY=sk**************************************************************************************************************************************************************VwIA
OMP_NUM_THREADS=1
CUDA_VISIBLE_DEVICES=None
EP_BATCHED_NJOBS=32
====================================

Set CUDA_VISIBLE_DEVICES to 1, updated device to cuda:0

=== DEBUG: Environment Variables ===
OPENAI_API_KEY=sk**************************************************************************************************************************************************************VwIA
OMP_NUM_THREADS=1
CUDA_VISIBLE_DEVICES=1
EP_BATCHED_NJOBS=32
====================================

[model_check] Clearing CUDA memory at start...
Updated device from cuda:1 to cuda:0
Processing 10 seeds: [42, 43, 44, 45, 46, 47, 48, 49, 50, 51]

=== Skipping model_check: all runthrough directories already exist ===


=== Running LLM upsampling (external script) ===
Invoking: /home/riya/2025_env/bin/python -m src.llm_upsampling.llm_upsampling_script -c gemma_spam_cpu --api-key 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
Current working directory: /home/riya/pivotal/pivotal-research/evaluating-probes
Extracted from config:
  Seeds: [42, 43, 44, 45, 46, 47, 48, 49, 50, 51]
  Num real samples: [1, 2, 3, 4, 5, 10]
  Upsampling factors: [1, 2, 3, 4, 5, 10, 20]
  Train on dataset: 94_better_spam
Initializing LLM client with model: gpt-4o-mini
API key length: 164 characters
LLM client initialized successfully

================================================================================
Processing seed 42
================================================================================
Output directory: results/spam_gemma_9b/seed_42/llm_samples_94_better_spam
Extracting samples using seed 42...
Loading dataset: 94_better_spam
No model or model_name provided, not initializing activation manager.
Total positive samples in dataset: 4000
Extracted 1 samples for n_real_samples=1
Extracted 2 samples for n_real_samples=2
Extracted 3 samples for n_real_samples=3
Extracted 4 samples for n_real_samples=4
Extracted 5 samples for n_real_samples=5
Extracted 10 samples for n_real_samples=10

------------------------------------------------------------
Processing n_real_samples = 1
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_42/llm_samples_94_better_spam/samples_1.csv
   Found 20 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 1
   Starting with existing 20 samples
Maximum target samples needed: 20
📁 Using existing dataset for activation extraction
   Debug: final_df has 20 samples, original_df has 1 samples
   Debug: LLM samples (after original) should be 19
📊 Final dataset breakdown:
   - Original samples: 1
   - Total samples in final dataset: 20
   - Generated samples: 19
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 20 samples to: results/spam_gemma_9b/seed_42/llm_samples_94_better_spam/samples_1.csv
   (Includes 1 original + 19 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 2
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_42/llm_samples_94_better_spam/samples_2.csv
   Found 40 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 2
   Starting with existing 40 samples
Maximum target samples needed: 40
📁 Using existing dataset for activation extraction
   Debug: final_df has 40 samples, original_df has 2 samples
   Debug: LLM samples (after original) should be 38
📊 Final dataset breakdown:
   - Original samples: 2
   - Total samples in final dataset: 40
   - Generated samples: 38
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 40 samples to: results/spam_gemma_9b/seed_42/llm_samples_94_better_spam/samples_2.csv
   (Includes 2 original + 38 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 3
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_42/llm_samples_94_better_spam/samples_3.csv
   Found 60 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 3
   Starting with existing 60 samples
Maximum target samples needed: 60
📁 Using existing dataset for activation extraction
   Debug: final_df has 60 samples, original_df has 3 samples
   Debug: LLM samples (after original) should be 57
📊 Final dataset breakdown:
   - Original samples: 3
   - Total samples in final dataset: 60
   - Generated samples: 57
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 60 samples to: results/spam_gemma_9b/seed_42/llm_samples_94_better_spam/samples_3.csv
   (Includes 3 original + 57 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 4
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_42/llm_samples_94_better_spam/samples_4.csv
   Found 80 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 4
   Starting with existing 80 samples
Maximum target samples needed: 80
📁 Using existing dataset for activation extraction
   Debug: final_df has 80 samples, original_df has 4 samples
   Debug: LLM samples (after original) should be 76
📊 Final dataset breakdown:
   - Original samples: 4
   - Total samples in final dataset: 80
   - Generated samples: 76
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 80 samples to: results/spam_gemma_9b/seed_42/llm_samples_94_better_spam/samples_4.csv
   (Includes 4 original + 76 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 5
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_42/llm_samples_94_better_spam/samples_5.csv
   Found 100 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 5
   Starting with existing 100 samples
Maximum target samples needed: 100
📁 Using existing dataset for activation extraction
   Debug: final_df has 100 samples, original_df has 5 samples
   Debug: LLM samples (after original) should be 95
📊 Final dataset breakdown:
   - Original samples: 5
   - Total samples in final dataset: 100
   - Generated samples: 95
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 100 samples to: results/spam_gemma_9b/seed_42/llm_samples_94_better_spam/samples_5.csv
   (Includes 5 original + 95 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 10
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_42/llm_samples_94_better_spam/samples_10.csv
   Found 200 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 10
   Starting with existing 200 samples
Maximum target samples needed: 200
📁 Using existing dataset for activation extraction
   Debug: final_df has 200 samples, original_df has 10 samples
   Debug: LLM samples (after original) should be 190
📊 Final dataset breakdown:
   - Original samples: 10
   - Total samples in final dataset: 200
   - Generated samples: 190
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 200 samples to: results/spam_gemma_9b/seed_42/llm_samples_94_better_spam/samples_10.csv
   (Includes 10 original + 190 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

Seed 42 complete! Files saved to: results/spam_gemma_9b/seed_42/llm_samples_94_better_spam

================================================================================
Processing seed 43
================================================================================
Output directory: results/spam_gemma_9b/seed_43/llm_samples_94_better_spam
Extracting samples using seed 43...
Loading dataset: 94_better_spam
No model or model_name provided, not initializing activation manager.
Total positive samples in dataset: 4000
Extracted 1 samples for n_real_samples=1
Extracted 2 samples for n_real_samples=2
Extracted 3 samples for n_real_samples=3
Extracted 4 samples for n_real_samples=4
Extracted 5 samples for n_real_samples=5
Extracted 10 samples for n_real_samples=10

------------------------------------------------------------
Processing n_real_samples = 1
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_43/llm_samples_94_better_spam/samples_1.csv
   Found 20 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 1
   Starting with existing 20 samples
Maximum target samples needed: 20
📁 Using existing dataset for activation extraction
   Debug: final_df has 20 samples, original_df has 1 samples
   Debug: LLM samples (after original) should be 19
📊 Final dataset breakdown:
   - Original samples: 1
   - Total samples in final dataset: 20
   - Generated samples: 19
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 20 samples to: results/spam_gemma_9b/seed_43/llm_samples_94_better_spam/samples_1.csv
   (Includes 1 original + 19 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 2
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_43/llm_samples_94_better_spam/samples_2.csv
   Found 40 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 2
   Starting with existing 40 samples
Maximum target samples needed: 40
📁 Using existing dataset for activation extraction
   Debug: final_df has 40 samples, original_df has 2 samples
   Debug: LLM samples (after original) should be 38
📊 Final dataset breakdown:
   - Original samples: 2
   - Total samples in final dataset: 40
   - Generated samples: 38
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 40 samples to: results/spam_gemma_9b/seed_43/llm_samples_94_better_spam/samples_2.csv
   (Includes 2 original + 38 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 3
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_43/llm_samples_94_better_spam/samples_3.csv
   Found 60 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 3
   Starting with existing 60 samples
Maximum target samples needed: 60
📁 Using existing dataset for activation extraction
   Debug: final_df has 60 samples, original_df has 3 samples
   Debug: LLM samples (after original) should be 57
📊 Final dataset breakdown:
   - Original samples: 3
   - Total samples in final dataset: 60
   - Generated samples: 57
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 60 samples to: results/spam_gemma_9b/seed_43/llm_samples_94_better_spam/samples_3.csv
   (Includes 3 original + 57 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 4
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_43/llm_samples_94_better_spam/samples_4.csv
   Found 80 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 4
   Starting with existing 80 samples
Maximum target samples needed: 80
📁 Using existing dataset for activation extraction
   Debug: final_df has 80 samples, original_df has 4 samples
   Debug: LLM samples (after original) should be 76
📊 Final dataset breakdown:
   - Original samples: 4
   - Total samples in final dataset: 80
   - Generated samples: 76
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 80 samples to: results/spam_gemma_9b/seed_43/llm_samples_94_better_spam/samples_4.csv
   (Includes 4 original + 76 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 5
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_43/llm_samples_94_better_spam/samples_5.csv
   Found 100 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 5
   Starting with existing 100 samples
Maximum target samples needed: 100
📁 Using existing dataset for activation extraction
   Debug: final_df has 100 samples, original_df has 5 samples
   Debug: LLM samples (after original) should be 95
📊 Final dataset breakdown:
   - Original samples: 5
   - Total samples in final dataset: 100
   - Generated samples: 95
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 100 samples to: results/spam_gemma_9b/seed_43/llm_samples_94_better_spam/samples_5.csv
   (Includes 5 original + 95 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 10
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_43/llm_samples_94_better_spam/samples_10.csv
   Found 200 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 10
   Starting with existing 200 samples
Maximum target samples needed: 200
📁 Using existing dataset for activation extraction
   Debug: final_df has 200 samples, original_df has 10 samples
   Debug: LLM samples (after original) should be 190
📊 Final dataset breakdown:
   - Original samples: 10
   - Total samples in final dataset: 200
   - Generated samples: 190
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 200 samples to: results/spam_gemma_9b/seed_43/llm_samples_94_better_spam/samples_10.csv
   (Includes 10 original + 190 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

Seed 43 complete! Files saved to: results/spam_gemma_9b/seed_43/llm_samples_94_better_spam

================================================================================
Processing seed 44
================================================================================
Output directory: results/spam_gemma_9b/seed_44/llm_samples_94_better_spam
Extracting samples using seed 44...
Loading dataset: 94_better_spam
No model or model_name provided, not initializing activation manager.
Total positive samples in dataset: 4000
Extracted 1 samples for n_real_samples=1
Extracted 2 samples for n_real_samples=2
Extracted 3 samples for n_real_samples=3
Extracted 4 samples for n_real_samples=4
Extracted 5 samples for n_real_samples=5
Extracted 10 samples for n_real_samples=10

------------------------------------------------------------
Processing n_real_samples = 1
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_44/llm_samples_94_better_spam/samples_1.csv
   Found 20 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 1
   Starting with existing 20 samples
Maximum target samples needed: 20
📁 Using existing dataset for activation extraction
   Debug: final_df has 20 samples, original_df has 1 samples
   Debug: LLM samples (after original) should be 19
📊 Final dataset breakdown:
   - Original samples: 1
   - Total samples in final dataset: 20
   - Generated samples: 19
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 20 samples to: results/spam_gemma_9b/seed_44/llm_samples_94_better_spam/samples_1.csv
   (Includes 1 original + 19 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 2
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_44/llm_samples_94_better_spam/samples_2.csv
   Found 40 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 2
   Starting with existing 40 samples
Maximum target samples needed: 40
📁 Using existing dataset for activation extraction
   Debug: final_df has 40 samples, original_df has 2 samples
   Debug: LLM samples (after original) should be 38
📊 Final dataset breakdown:
   - Original samples: 2
   - Total samples in final dataset: 40
   - Generated samples: 38
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 40 samples to: results/spam_gemma_9b/seed_44/llm_samples_94_better_spam/samples_2.csv
   (Includes 2 original + 38 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 3
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_44/llm_samples_94_better_spam/samples_3.csv
   Found 60 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 3
   Starting with existing 60 samples
Maximum target samples needed: 60
📁 Using existing dataset for activation extraction
   Debug: final_df has 60 samples, original_df has 3 samples
   Debug: LLM samples (after original) should be 57
📊 Final dataset breakdown:
   - Original samples: 3
   - Total samples in final dataset: 60
   - Generated samples: 57
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 60 samples to: results/spam_gemma_9b/seed_44/llm_samples_94_better_spam/samples_3.csv
   (Includes 3 original + 57 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 4
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_44/llm_samples_94_better_spam/samples_4.csv
   Found 80 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 4
   Starting with existing 80 samples
Maximum target samples needed: 80
📁 Using existing dataset for activation extraction
   Debug: final_df has 80 samples, original_df has 4 samples
   Debug: LLM samples (after original) should be 76
📊 Final dataset breakdown:
   - Original samples: 4
   - Total samples in final dataset: 80
   - Generated samples: 76
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 80 samples to: results/spam_gemma_9b/seed_44/llm_samples_94_better_spam/samples_4.csv
   (Includes 4 original + 76 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 5
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_44/llm_samples_94_better_spam/samples_5.csv
   Found 100 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 5
   Starting with existing 100 samples
Maximum target samples needed: 100
📁 Using existing dataset for activation extraction
   Debug: final_df has 100 samples, original_df has 5 samples
   Debug: LLM samples (after original) should be 95
📊 Final dataset breakdown:
   - Original samples: 5
   - Total samples in final dataset: 100
   - Generated samples: 95
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 100 samples to: results/spam_gemma_9b/seed_44/llm_samples_94_better_spam/samples_5.csv
   (Includes 5 original + 95 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 10
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_44/llm_samples_94_better_spam/samples_10.csv
   Found 200 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 10
   Starting with existing 200 samples
Maximum target samples needed: 200
📁 Using existing dataset for activation extraction
   Debug: final_df has 200 samples, original_df has 10 samples
   Debug: LLM samples (after original) should be 190
📊 Final dataset breakdown:
   - Original samples: 10
   - Total samples in final dataset: 200
   - Generated samples: 190
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 200 samples to: results/spam_gemma_9b/seed_44/llm_samples_94_better_spam/samples_10.csv
   (Includes 10 original + 190 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

Seed 44 complete! Files saved to: results/spam_gemma_9b/seed_44/llm_samples_94_better_spam

================================================================================
Processing seed 45
================================================================================
Output directory: results/spam_gemma_9b/seed_45/llm_samples_94_better_spam
Extracting samples using seed 45...
Loading dataset: 94_better_spam
No model or model_name provided, not initializing activation manager.
Total positive samples in dataset: 4000
Extracted 1 samples for n_real_samples=1
Extracted 2 samples for n_real_samples=2
Extracted 3 samples for n_real_samples=3
Extracted 4 samples for n_real_samples=4
Extracted 5 samples for n_real_samples=5
Extracted 10 samples for n_real_samples=10

------------------------------------------------------------
Processing n_real_samples = 1
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_45/llm_samples_94_better_spam/samples_1.csv
   Found 20 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 1
   Starting with existing 20 samples
Maximum target samples needed: 20
📁 Using existing dataset for activation extraction
   Debug: final_df has 20 samples, original_df has 1 samples
   Debug: LLM samples (after original) should be 19
📊 Final dataset breakdown:
   - Original samples: 1
   - Total samples in final dataset: 20
   - Generated samples: 19
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 20 samples to: results/spam_gemma_9b/seed_45/llm_samples_94_better_spam/samples_1.csv
   (Includes 1 original + 19 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 2
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_45/llm_samples_94_better_spam/samples_2.csv
   Found 40 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 2
   Starting with existing 40 samples
Maximum target samples needed: 40
📁 Using existing dataset for activation extraction
   Debug: final_df has 40 samples, original_df has 2 samples
   Debug: LLM samples (after original) should be 38
📊 Final dataset breakdown:
   - Original samples: 2
   - Total samples in final dataset: 40
   - Generated samples: 38
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 40 samples to: results/spam_gemma_9b/seed_45/llm_samples_94_better_spam/samples_2.csv
   (Includes 2 original + 38 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 3
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_45/llm_samples_94_better_spam/samples_3.csv
   Found 60 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 3
   Starting with existing 60 samples
Maximum target samples needed: 60
📁 Using existing dataset for activation extraction
   Debug: final_df has 60 samples, original_df has 3 samples
   Debug: LLM samples (after original) should be 57
📊 Final dataset breakdown:
   - Original samples: 3
   - Total samples in final dataset: 60
   - Generated samples: 57
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 60 samples to: results/spam_gemma_9b/seed_45/llm_samples_94_better_spam/samples_3.csv
   (Includes 3 original + 57 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 4
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_45/llm_samples_94_better_spam/samples_4.csv
   Found 80 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 4
   Starting with existing 80 samples
Maximum target samples needed: 80
📁 Using existing dataset for activation extraction
   Debug: final_df has 80 samples, original_df has 4 samples
   Debug: LLM samples (after original) should be 76
📊 Final dataset breakdown:
   - Original samples: 4
   - Total samples in final dataset: 80
   - Generated samples: 76
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 80 samples to: results/spam_gemma_9b/seed_45/llm_samples_94_better_spam/samples_4.csv
   (Includes 4 original + 76 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 5
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_45/llm_samples_94_better_spam/samples_5.csv
   Found 100 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 5
   Starting with existing 100 samples
Maximum target samples needed: 100
📁 Using existing dataset for activation extraction
   Debug: final_df has 100 samples, original_df has 5 samples
   Debug: LLM samples (after original) should be 95
📊 Final dataset breakdown:
   - Original samples: 5
   - Total samples in final dataset: 100
   - Generated samples: 95
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 100 samples to: results/spam_gemma_9b/seed_45/llm_samples_94_better_spam/samples_5.csv
   (Includes 5 original + 95 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 10
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_45/llm_samples_94_better_spam/samples_10.csv
   Found 200 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 10
   Starting with existing 200 samples
Maximum target samples needed: 200
📁 Using existing dataset for activation extraction
   Debug: final_df has 200 samples, original_df has 10 samples
   Debug: LLM samples (after original) should be 190
📊 Final dataset breakdown:
   - Original samples: 10
   - Total samples in final dataset: 200
   - Generated samples: 190
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 200 samples to: results/spam_gemma_9b/seed_45/llm_samples_94_better_spam/samples_10.csv
   (Includes 10 original + 190 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

Seed 45 complete! Files saved to: results/spam_gemma_9b/seed_45/llm_samples_94_better_spam

================================================================================
Processing seed 46
================================================================================
Output directory: results/spam_gemma_9b/seed_46/llm_samples_94_better_spam
Extracting samples using seed 46...
Loading dataset: 94_better_spam
No model or model_name provided, not initializing activation manager.
Total positive samples in dataset: 4000
Extracted 1 samples for n_real_samples=1
Extracted 2 samples for n_real_samples=2
Extracted 3 samples for n_real_samples=3
Extracted 4 samples for n_real_samples=4
Extracted 5 samples for n_real_samples=5
Extracted 10 samples for n_real_samples=10

------------------------------------------------------------
Processing n_real_samples = 1
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_46/llm_samples_94_better_spam/samples_1.csv
   Found 20 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 1
   Starting with existing 20 samples
Maximum target samples needed: 20
📁 Using existing dataset for activation extraction
   Debug: final_df has 20 samples, original_df has 1 samples
   Debug: LLM samples (after original) should be 19
📊 Final dataset breakdown:
   - Original samples: 1
   - Total samples in final dataset: 20
   - Generated samples: 19
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 20 samples to: results/spam_gemma_9b/seed_46/llm_samples_94_better_spam/samples_1.csv
   (Includes 1 original + 19 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 2
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_46/llm_samples_94_better_spam/samples_2.csv
   Found 40 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 2
   Starting with existing 40 samples
Maximum target samples needed: 40
📁 Using existing dataset for activation extraction
   Debug: final_df has 40 samples, original_df has 2 samples
   Debug: LLM samples (after original) should be 38
📊 Final dataset breakdown:
   - Original samples: 2
   - Total samples in final dataset: 40
   - Generated samples: 38
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 40 samples to: results/spam_gemma_9b/seed_46/llm_samples_94_better_spam/samples_2.csv
   (Includes 2 original + 38 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 3
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_46/llm_samples_94_better_spam/samples_3.csv
   Found 60 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 3
   Starting with existing 60 samples
Maximum target samples needed: 60
📁 Using existing dataset for activation extraction
   Debug: final_df has 60 samples, original_df has 3 samples
   Debug: LLM samples (after original) should be 57
📊 Final dataset breakdown:
   - Original samples: 3
   - Total samples in final dataset: 60
   - Generated samples: 57
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 60 samples to: results/spam_gemma_9b/seed_46/llm_samples_94_better_spam/samples_3.csv
   (Includes 3 original + 57 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 4
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_46/llm_samples_94_better_spam/samples_4.csv
   Found 80 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 4
   Starting with existing 80 samples
Maximum target samples needed: 80
📁 Using existing dataset for activation extraction
   Debug: final_df has 80 samples, original_df has 4 samples
   Debug: LLM samples (after original) should be 76
📊 Final dataset breakdown:
   - Original samples: 4
   - Total samples in final dataset: 80
   - Generated samples: 76
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 80 samples to: results/spam_gemma_9b/seed_46/llm_samples_94_better_spam/samples_4.csv
   (Includes 4 original + 76 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 5
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_46/llm_samples_94_better_spam/samples_5.csv
   Found 100 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 5
   Starting with existing 100 samples
Maximum target samples needed: 100
📁 Using existing dataset for activation extraction
   Debug: final_df has 100 samples, original_df has 5 samples
   Debug: LLM samples (after original) should be 95
📊 Final dataset breakdown:
   - Original samples: 5
   - Total samples in final dataset: 100
   - Generated samples: 95
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 100 samples to: results/spam_gemma_9b/seed_46/llm_samples_94_better_spam/samples_5.csv
   (Includes 5 original + 95 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 10
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_46/llm_samples_94_better_spam/samples_10.csv
   Found 200 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 10
   Starting with existing 200 samples
Maximum target samples needed: 200
📁 Using existing dataset for activation extraction
   Debug: final_df has 200 samples, original_df has 10 samples
   Debug: LLM samples (after original) should be 190
📊 Final dataset breakdown:
   - Original samples: 10
   - Total samples in final dataset: 200
   - Generated samples: 190
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 200 samples to: results/spam_gemma_9b/seed_46/llm_samples_94_better_spam/samples_10.csv
   (Includes 10 original + 190 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

Seed 46 complete! Files saved to: results/spam_gemma_9b/seed_46/llm_samples_94_better_spam

================================================================================
Processing seed 47
================================================================================
Output directory: results/spam_gemma_9b/seed_47/llm_samples_94_better_spam
Extracting samples using seed 47...
Loading dataset: 94_better_spam
No model or model_name provided, not initializing activation manager.
Total positive samples in dataset: 4000
Extracted 1 samples for n_real_samples=1
Extracted 2 samples for n_real_samples=2
Extracted 3 samples for n_real_samples=3
Extracted 4 samples for n_real_samples=4
Extracted 5 samples for n_real_samples=5
Extracted 10 samples for n_real_samples=10

------------------------------------------------------------
Processing n_real_samples = 1
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_47/llm_samples_94_better_spam/samples_1.csv
   Found 20 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 1
   Starting with existing 20 samples
Maximum target samples needed: 20
📁 Using existing dataset for activation extraction
   Debug: final_df has 20 samples, original_df has 1 samples
   Debug: LLM samples (after original) should be 19
📊 Final dataset breakdown:
   - Original samples: 1
   - Total samples in final dataset: 20
   - Generated samples: 19
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 20 samples to: results/spam_gemma_9b/seed_47/llm_samples_94_better_spam/samples_1.csv
   (Includes 1 original + 19 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 2
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_47/llm_samples_94_better_spam/samples_2.csv
   Found 40 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 2
   Starting with existing 40 samples
Maximum target samples needed: 40
📁 Using existing dataset for activation extraction
   Debug: final_df has 40 samples, original_df has 2 samples
   Debug: LLM samples (after original) should be 38
📊 Final dataset breakdown:
   - Original samples: 2
   - Total samples in final dataset: 40
   - Generated samples: 38
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 40 samples to: results/spam_gemma_9b/seed_47/llm_samples_94_better_spam/samples_2.csv
   (Includes 2 original + 38 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 3
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_47/llm_samples_94_better_spam/samples_3.csv
   Found 60 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 3
   Starting with existing 60 samples
Maximum target samples needed: 60
📁 Using existing dataset for activation extraction
   Debug: final_df has 60 samples, original_df has 3 samples
   Debug: LLM samples (after original) should be 57
📊 Final dataset breakdown:
   - Original samples: 3
   - Total samples in final dataset: 60
   - Generated samples: 57
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 60 samples to: results/spam_gemma_9b/seed_47/llm_samples_94_better_spam/samples_3.csv
   (Includes 3 original + 57 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 4
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_47/llm_samples_94_better_spam/samples_4.csv
   Found 80 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 4
   Starting with existing 80 samples
Maximum target samples needed: 80
📁 Using existing dataset for activation extraction
   Debug: final_df has 80 samples, original_df has 4 samples
   Debug: LLM samples (after original) should be 76
📊 Final dataset breakdown:
   - Original samples: 4
   - Total samples in final dataset: 80
   - Generated samples: 76
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 80 samples to: results/spam_gemma_9b/seed_47/llm_samples_94_better_spam/samples_4.csv
   (Includes 4 original + 76 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 5
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_47/llm_samples_94_better_spam/samples_5.csv
   Found 100 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 5
   Starting with existing 100 samples
Maximum target samples needed: 100
📁 Using existing dataset for activation extraction
   Debug: final_df has 100 samples, original_df has 5 samples
   Debug: LLM samples (after original) should be 95
📊 Final dataset breakdown:
   - Original samples: 5
   - Total samples in final dataset: 100
   - Generated samples: 95
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 100 samples to: results/spam_gemma_9b/seed_47/llm_samples_94_better_spam/samples_5.csv
   (Includes 5 original + 95 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 10
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_47/llm_samples_94_better_spam/samples_10.csv
   Found 200 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 10
   Starting with existing 200 samples
Maximum target samples needed: 200
📁 Using existing dataset for activation extraction
   Debug: final_df has 200 samples, original_df has 10 samples
   Debug: LLM samples (after original) should be 190
📊 Final dataset breakdown:
   - Original samples: 10
   - Total samples in final dataset: 200
   - Generated samples: 190
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 200 samples to: results/spam_gemma_9b/seed_47/llm_samples_94_better_spam/samples_10.csv
   (Includes 10 original + 190 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

Seed 47 complete! Files saved to: results/spam_gemma_9b/seed_47/llm_samples_94_better_spam

================================================================================
Processing seed 48
================================================================================
Output directory: results/spam_gemma_9b/seed_48/llm_samples_94_better_spam
Extracting samples using seed 48...
Loading dataset: 94_better_spam
No model or model_name provided, not initializing activation manager.
Total positive samples in dataset: 4000
Extracted 1 samples for n_real_samples=1
Extracted 2 samples for n_real_samples=2
Extracted 3 samples for n_real_samples=3
Extracted 4 samples for n_real_samples=4
Extracted 5 samples for n_real_samples=5
Extracted 10 samples for n_real_samples=10

------------------------------------------------------------
Processing n_real_samples = 1
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_48/llm_samples_94_better_spam/samples_1.csv
   Found 20 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 1
   Starting with existing 20 samples
Maximum target samples needed: 20
📁 Using existing dataset for activation extraction
   Debug: final_df has 20 samples, original_df has 1 samples
   Debug: LLM samples (after original) should be 19
📊 Final dataset breakdown:
   - Original samples: 1
   - Total samples in final dataset: 20
   - Generated samples: 19
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 20 samples to: results/spam_gemma_9b/seed_48/llm_samples_94_better_spam/samples_1.csv
   (Includes 1 original + 19 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 2
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_48/llm_samples_94_better_spam/samples_2.csv
   Found 40 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 2
   Starting with existing 40 samples
Maximum target samples needed: 40
📁 Using existing dataset for activation extraction
   Debug: final_df has 40 samples, original_df has 2 samples
   Debug: LLM samples (after original) should be 38
📊 Final dataset breakdown:
   - Original samples: 2
   - Total samples in final dataset: 40
   - Generated samples: 38
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 40 samples to: results/spam_gemma_9b/seed_48/llm_samples_94_better_spam/samples_2.csv
   (Includes 2 original + 38 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 3
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_48/llm_samples_94_better_spam/samples_3.csv
   Found 60 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 3
   Starting with existing 60 samples
Maximum target samples needed: 60
📁 Using existing dataset for activation extraction
   Debug: final_df has 60 samples, original_df has 3 samples
   Debug: LLM samples (after original) should be 57
📊 Final dataset breakdown:
   - Original samples: 3
   - Total samples in final dataset: 60
   - Generated samples: 57
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 60 samples to: results/spam_gemma_9b/seed_48/llm_samples_94_better_spam/samples_3.csv
   (Includes 3 original + 57 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 4
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_48/llm_samples_94_better_spam/samples_4.csv
   Found 80 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 4
   Starting with existing 80 samples
Maximum target samples needed: 80
📁 Using existing dataset for activation extraction
   Debug: final_df has 80 samples, original_df has 4 samples
   Debug: LLM samples (after original) should be 76
📊 Final dataset breakdown:
   - Original samples: 4
   - Total samples in final dataset: 80
   - Generated samples: 76
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 80 samples to: results/spam_gemma_9b/seed_48/llm_samples_94_better_spam/samples_4.csv
   (Includes 4 original + 76 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 5
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_48/llm_samples_94_better_spam/samples_5.csv
   Found 100 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 5
   Starting with existing 100 samples
Maximum target samples needed: 100
📁 Using existing dataset for activation extraction
   Debug: final_df has 100 samples, original_df has 5 samples
   Debug: LLM samples (after original) should be 95
📊 Final dataset breakdown:
   - Original samples: 5
   - Total samples in final dataset: 100
   - Generated samples: 95
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 100 samples to: results/spam_gemma_9b/seed_48/llm_samples_94_better_spam/samples_5.csv
   (Includes 5 original + 95 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 10
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_48/llm_samples_94_better_spam/samples_10.csv
   Found 200 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 10
   Starting with existing 200 samples
Maximum target samples needed: 200
📁 Using existing dataset for activation extraction
   Debug: final_df has 200 samples, original_df has 10 samples
   Debug: LLM samples (after original) should be 190
📊 Final dataset breakdown:
   - Original samples: 10
   - Total samples in final dataset: 200
   - Generated samples: 190
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 200 samples to: results/spam_gemma_9b/seed_48/llm_samples_94_better_spam/samples_10.csv
   (Includes 10 original + 190 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

Seed 48 complete! Files saved to: results/spam_gemma_9b/seed_48/llm_samples_94_better_spam

================================================================================
Processing seed 49
================================================================================
Output directory: results/spam_gemma_9b/seed_49/llm_samples_94_better_spam
Extracting samples using seed 49...
Loading dataset: 94_better_spam
No model or model_name provided, not initializing activation manager.
Total positive samples in dataset: 4000
Extracted 1 samples for n_real_samples=1
Extracted 2 samples for n_real_samples=2
Extracted 3 samples for n_real_samples=3
Extracted 4 samples for n_real_samples=4
Extracted 5 samples for n_real_samples=5
Extracted 10 samples for n_real_samples=10

------------------------------------------------------------
Processing n_real_samples = 1
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_49/llm_samples_94_better_spam/samples_1.csv
   Found 20 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 1
   Starting with existing 20 samples
Maximum target samples needed: 20
📁 Using existing dataset for activation extraction
   Debug: final_df has 20 samples, original_df has 1 samples
   Debug: LLM samples (after original) should be 19
📊 Final dataset breakdown:
   - Original samples: 1
   - Total samples in final dataset: 20
   - Generated samples: 19
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 20 samples to: results/spam_gemma_9b/seed_49/llm_samples_94_better_spam/samples_1.csv
   (Includes 1 original + 19 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 2
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_49/llm_samples_94_better_spam/samples_2.csv
   Found 40 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 2
   Starting with existing 40 samples
Maximum target samples needed: 40
📁 Using existing dataset for activation extraction
   Debug: final_df has 40 samples, original_df has 2 samples
   Debug: LLM samples (after original) should be 38
📊 Final dataset breakdown:
   - Original samples: 2
   - Total samples in final dataset: 40
   - Generated samples: 38
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 40 samples to: results/spam_gemma_9b/seed_49/llm_samples_94_better_spam/samples_2.csv
   (Includes 2 original + 38 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 3
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_49/llm_samples_94_better_spam/samples_3.csv
   Found 60 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 3
   Starting with existing 60 samples
Maximum target samples needed: 60
📁 Using existing dataset for activation extraction
   Debug: final_df has 60 samples, original_df has 3 samples
   Debug: LLM samples (after original) should be 57
📊 Final dataset breakdown:
   - Original samples: 3
   - Total samples in final dataset: 60
   - Generated samples: 57
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 60 samples to: results/spam_gemma_9b/seed_49/llm_samples_94_better_spam/samples_3.csv
   (Includes 3 original + 57 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 4
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_49/llm_samples_94_better_spam/samples_4.csv
   Found 80 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 4
   Starting with existing 80 samples
Maximum target samples needed: 80
📁 Using existing dataset for activation extraction
   Debug: final_df has 80 samples, original_df has 4 samples
   Debug: LLM samples (after original) should be 76
📊 Final dataset breakdown:
   - Original samples: 4
   - Total samples in final dataset: 80
   - Generated samples: 76
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 80 samples to: results/spam_gemma_9b/seed_49/llm_samples_94_better_spam/samples_4.csv
   (Includes 4 original + 76 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 5
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_49/llm_samples_94_better_spam/samples_5.csv
   Found 100 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 5
   Starting with existing 100 samples
Maximum target samples needed: 100
📁 Using existing dataset for activation extraction
   Debug: final_df has 100 samples, original_df has 5 samples
   Debug: LLM samples (after original) should be 95
📊 Final dataset breakdown:
   - Original samples: 5
   - Total samples in final dataset: 100
   - Generated samples: 95
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 100 samples to: results/spam_gemma_9b/seed_49/llm_samples_94_better_spam/samples_5.csv
   (Includes 5 original + 95 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 10
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_49/llm_samples_94_better_spam/samples_10.csv
   Found 200 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 10
   Starting with existing 200 samples
Maximum target samples needed: 200
📁 Using existing dataset for activation extraction
   Debug: final_df has 200 samples, original_df has 10 samples
   Debug: LLM samples (after original) should be 190
📊 Final dataset breakdown:
   - Original samples: 10
   - Total samples in final dataset: 200
   - Generated samples: 190
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 200 samples to: results/spam_gemma_9b/seed_49/llm_samples_94_better_spam/samples_10.csv
   (Includes 10 original + 190 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

Seed 49 complete! Files saved to: results/spam_gemma_9b/seed_49/llm_samples_94_better_spam

================================================================================
Processing seed 50
================================================================================
Output directory: results/spam_gemma_9b/seed_50/llm_samples_94_better_spam
Extracting samples using seed 50...
Loading dataset: 94_better_spam
No model or model_name provided, not initializing activation manager.
Total positive samples in dataset: 4000
Extracted 1 samples for n_real_samples=1
Extracted 2 samples for n_real_samples=2
Extracted 3 samples for n_real_samples=3
Extracted 4 samples for n_real_samples=4
Extracted 5 samples for n_real_samples=5
Extracted 10 samples for n_real_samples=10

------------------------------------------------------------
Processing n_real_samples = 1
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_50/llm_samples_94_better_spam/samples_1.csv
   Found 20 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 1
   Starting with existing 20 samples
Maximum target samples needed: 20
📁 Using existing dataset for activation extraction
   Debug: final_df has 20 samples, original_df has 1 samples
   Debug: LLM samples (after original) should be 19
📊 Final dataset breakdown:
   - Original samples: 1
   - Total samples in final dataset: 20
   - Generated samples: 19
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 20 samples to: results/spam_gemma_9b/seed_50/llm_samples_94_better_spam/samples_1.csv
   (Includes 1 original + 19 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 2
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_50/llm_samples_94_better_spam/samples_2.csv
   Found 40 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 2
   Starting with existing 40 samples
Maximum target samples needed: 40
📁 Using existing dataset for activation extraction
   Debug: final_df has 40 samples, original_df has 2 samples
   Debug: LLM samples (after original) should be 38
📊 Final dataset breakdown:
   - Original samples: 2
   - Total samples in final dataset: 40
   - Generated samples: 38
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 40 samples to: results/spam_gemma_9b/seed_50/llm_samples_94_better_spam/samples_2.csv
   (Includes 2 original + 38 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 3
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_50/llm_samples_94_better_spam/samples_3.csv
   Found 60 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 3
   Starting with existing 60 samples
Maximum target samples needed: 60
📁 Using existing dataset for activation extraction
   Debug: final_df has 60 samples, original_df has 3 samples
   Debug: LLM samples (after original) should be 57
📊 Final dataset breakdown:
   - Original samples: 3
   - Total samples in final dataset: 60
   - Generated samples: 57
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 60 samples to: results/spam_gemma_9b/seed_50/llm_samples_94_better_spam/samples_3.csv
   (Includes 3 original + 57 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 4
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_50/llm_samples_94_better_spam/samples_4.csv
   Found 80 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 4
   Starting with existing 80 samples
Maximum target samples needed: 80
📁 Using existing dataset for activation extraction
   Debug: final_df has 80 samples, original_df has 4 samples
   Debug: LLM samples (after original) should be 76
📊 Final dataset breakdown:
   - Original samples: 4
   - Total samples in final dataset: 80
   - Generated samples: 76
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 80 samples to: results/spam_gemma_9b/seed_50/llm_samples_94_better_spam/samples_4.csv
   (Includes 4 original + 76 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 5
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_50/llm_samples_94_better_spam/samples_5.csv
   Found 100 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 5
   Starting with existing 100 samples
Maximum target samples needed: 100
📁 Using existing dataset for activation extraction
   Debug: final_df has 100 samples, original_df has 5 samples
   Debug: LLM samples (after original) should be 95
📊 Final dataset breakdown:
   - Original samples: 5
   - Total samples in final dataset: 100
   - Generated samples: 95
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 100 samples to: results/spam_gemma_9b/seed_50/llm_samples_94_better_spam/samples_5.csv
   (Includes 5 original + 95 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 10
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_50/llm_samples_94_better_spam/samples_10.csv
   Found 200 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 10
   Starting with existing 200 samples
Maximum target samples needed: 200
📁 Using existing dataset for activation extraction
   Debug: final_df has 200 samples, original_df has 10 samples
   Debug: LLM samples (after original) should be 190
📊 Final dataset breakdown:
   - Original samples: 10
   - Total samples in final dataset: 200
   - Generated samples: 190
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 200 samples to: results/spam_gemma_9b/seed_50/llm_samples_94_better_spam/samples_10.csv
   (Includes 10 original + 190 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

Seed 50 complete! Files saved to: results/spam_gemma_9b/seed_50/llm_samples_94_better_spam

================================================================================
Processing seed 51
================================================================================
Output directory: results/spam_gemma_9b/seed_51/llm_samples_94_better_spam
Extracting samples using seed 51...
Loading dataset: 94_better_spam
No model or model_name provided, not initializing activation manager.
Total positive samples in dataset: 4000
Extracted 1 samples for n_real_samples=1
Extracted 2 samples for n_real_samples=2
Extracted 3 samples for n_real_samples=3
Extracted 4 samples for n_real_samples=4
Extracted 5 samples for n_real_samples=5
Extracted 10 samples for n_real_samples=10

------------------------------------------------------------
Processing n_real_samples = 1
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_51/llm_samples_94_better_spam/samples_1.csv
   Found 20 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 1
   Starting with existing 20 samples
Maximum target samples needed: 20
📁 Using existing dataset for activation extraction
   Debug: final_df has 20 samples, original_df has 1 samples
   Debug: LLM samples (after original) should be 19
📊 Final dataset breakdown:
   - Original samples: 1
   - Total samples in final dataset: 20
   - Generated samples: 19
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 20 samples to: results/spam_gemma_9b/seed_51/llm_samples_94_better_spam/samples_1.csv
   (Includes 1 original + 19 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 2
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_51/llm_samples_94_better_spam/samples_2.csv
   Found 40 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 2
   Starting with existing 40 samples
Maximum target samples needed: 40
📁 Using existing dataset for activation extraction
   Debug: final_df has 40 samples, original_df has 2 samples
   Debug: LLM samples (after original) should be 38
📊 Final dataset breakdown:
   - Original samples: 2
   - Total samples in final dataset: 40
   - Generated samples: 38
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 40 samples to: results/spam_gemma_9b/seed_51/llm_samples_94_better_spam/samples_2.csv
   (Includes 2 original + 38 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 3
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_51/llm_samples_94_better_spam/samples_3.csv
   Found 60 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 3
   Starting with existing 60 samples
Maximum target samples needed: 60
📁 Using existing dataset for activation extraction
   Debug: final_df has 60 samples, original_df has 3 samples
   Debug: LLM samples (after original) should be 57
📊 Final dataset breakdown:
   - Original samples: 3
   - Total samples in final dataset: 60
   - Generated samples: 57
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 60 samples to: results/spam_gemma_9b/seed_51/llm_samples_94_better_spam/samples_3.csv
   (Includes 3 original + 57 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 4
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_51/llm_samples_94_better_spam/samples_4.csv
   Found 80 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 4
   Starting with existing 80 samples
Maximum target samples needed: 80
📁 Using existing dataset for activation extraction
   Debug: final_df has 80 samples, original_df has 4 samples
   Debug: LLM samples (after original) should be 76
📊 Final dataset breakdown:
   - Original samples: 4
   - Total samples in final dataset: 80
   - Generated samples: 76
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 80 samples to: results/spam_gemma_9b/seed_51/llm_samples_94_better_spam/samples_4.csv
   (Includes 4 original + 76 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 5
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_51/llm_samples_94_better_spam/samples_5.csv
   Found 100 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 5
   Starting with existing 100 samples
Maximum target samples needed: 100
📁 Using existing dataset for activation extraction
   Debug: final_df has 100 samples, original_df has 5 samples
   Debug: LLM samples (after original) should be 95
📊 Final dataset breakdown:
   - Original samples: 5
   - Total samples in final dataset: 100
   - Generated samples: 95
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 100 samples to: results/spam_gemma_9b/seed_51/llm_samples_94_better_spam/samples_5.csv
   (Includes 5 original + 95 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 10
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_51/llm_samples_94_better_spam/samples_10.csv
   Found 200 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 10
   Starting with existing 200 samples
Maximum target samples needed: 200
📁 Using existing dataset for activation extraction
   Debug: final_df has 200 samples, original_df has 10 samples
   Debug: LLM samples (after original) should be 190
📊 Final dataset breakdown:
   - Original samples: 10
   - Total samples in final dataset: 200
   - Generated samples: 190
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 200 samples to: results/spam_gemma_9b/seed_51/llm_samples_94_better_spam/samples_10.csv
   (Includes 10 original + 190 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

Seed 51 complete! Files saved to: results/spam_gemma_9b/seed_51/llm_samples_94_better_spam

================================================================================
LLM upsampling complete for all seeds!
Results saved to: results/spam_gemma_9b/seed_*/llm_samples/
Upsampling factors processed: [1, 2, 3, 4, 5, 10, 20]
================================================================================
=== LLM upsampling complete ===
Loading model 'google/gemma-2-9b' for activation extraction...
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:02<00:17,  2.44s/it]Loading checkpoint shards:  25%|██▌       | 2/8 [00:04<00:14,  2.38s/it]Loading checkpoint shards:  38%|███▊      | 3/8 [00:06<00:11,  2.30s/it]Loading checkpoint shards:  50%|█████     | 4/8 [00:09<00:09,  2.26s/it]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:11<00:06,  2.25s/it]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:13<00:04,  2.20s/it]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:15<00:02,  2.17s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:16<00:00,  1.79s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:16<00:00,  2.08s/it]

========================= ACTIVATION EXTRACTION PHASE =========================
  - Extracting activations for 94_better_spam, L20, resid_post (on_policy=False)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: Gemma2ForCausalLM(
  (model): Gemma2Model(
    (embed_tokens): Embedding(256000, 3584, padding_idx=0)
    (layers): ModuleList(
      (0-41): 42 x Gemma2DecoderLayer(
        (self_attn): Gemma2Attention(
          (q_proj): Linear(in_features=3584, out_features=4096, bias=False)
          (k_proj): Linear(in_features=3584, out_features=2048, bias=False)
          (v_proj): Linear(in_features=3584, out_features=2048, bias=False)
          (o_proj): Linear(in_features=4096, out_features=3584, bias=False)
        )
        (mlp): Gemma2MLP(
          (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)
          (up_proj): Linear(in_features=3584, out_features=14336, bias=False)
          (down_proj): Linear(in_features=14336, out_features=3584, bias=False)
          (act_fn): PytorchGELUTanh()
        )
        (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)
        (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)
        (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)
        (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)
      )
    )
    (norm): Gemma2RMSNorm((3584,), eps=1e-06)
    (rotary_emb): Gemma2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3584, out_features=256000, bias=False)
)
[DEBUG] model type: <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'>
Successfully initialized ActivationManager
    - Off-policy: extracting activations from prompts...
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 15028
[DEBUG] Processing 8000 full activations
[DEBUG] Estimated memory requirement: 19.66 GB
[WARNING] Large memory requirement (19.66 GB).
[DEBUG] Max sequence length: 1167
[DEBUG] Shape of first activation: (1, 47, 3584)
[DEBUG] Final array shape: (8000, 1167, 3584)
[DEBUG] Returning (8000, 1167, 3584) activations
    - Cached 0 new activations (dataset)
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 15028
[DEBUG] Processing 5000 full activations
[DEBUG] Estimated memory requirement: 5.41 GB
[WARNING] Large memory requirement (5.41 GB).
[DEBUG] Max sequence length: 935
[DEBUG] Shape of first activation: (1, 94, 3584)
[DEBUG] Final array shape: (5000, 935, 3584)
[DEBUG] Returning (5000, 935, 3584) activations
    - Cached 0 new activations (LLM)
    - Aggregated activations are up-to-date
  - Extracting activations for 87_is_spam, L20, resid_post (on_policy=False)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: Gemma2ForCausalLM(
  (model): Gemma2Model(
    (embed_tokens): Embedding(256000, 3584, padding_idx=0)
    (layers): ModuleList(
      (0-41): 42 x Gemma2DecoderLayer(
        (self_attn): Gemma2Attention(
          (q_proj): Linear(in_features=3584, out_features=4096, bias=False)
          (k_proj): Linear(in_features=3584, out_features=2048, bias=False)
          (v_proj): Linear(in_features=3584, out_features=2048, bias=False)
          (o_proj): Linear(in_features=4096, out_features=3584, bias=False)
        )
        (mlp): Gemma2MLP(
          (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)
          (up_proj): Linear(in_features=3584, out_features=14336, bias=False)
          (down_proj): Linear(in_features=14336, out_features=3584, bias=False)
          (act_fn): PytorchGELUTanh()
        )
        (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)
        (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)
        (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)
        (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)
      )
    )
    (norm): Gemma2RMSNorm((3584,), eps=1e-06)
    (rotary_emb): Gemma2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3584, out_features=256000, bias=False)
)
[DEBUG] model type: <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'>
Successfully initialized ActivationManager
    - Off-policy: extracting activations from prompts...
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 7507
[DEBUG] Processing 5157 full activations
[DEBUG] Estimated memory requirement: 1.63 GB
[WARNING] Large memory requirement (1.63 GB).
[DEBUG] Max sequence length: 237
[DEBUG] Shape of first activation: (1, 27, 3584)
[DEBUG] Final array shape: (5157, 237, 3584)
[DEBUG] Returning (5157, 237, 3584) activations
    - Cached 0 new activations (dataset)
    - Aggregated activations are up-to-date

========================= MODEL UNLOADING PHASE =========================
Unloading model to free GPU memory for parallel processing...
Model unloaded successfully

========================= JOB CREATION PHASE =========================
Created 5600 probe jobs

========================= PROBE PROCESSING PHASE (BATCHED) =========================
Processing 5600 probe jobs in batched groups...
Running 5600 jobs in 700 groups (batched)

=== Group 4/700 — ('94_better_spam', 20, 'resid_post', 45, '__none__', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
Split data: 2682 train, 358 val, 537 test
[DEBUG] Class distribution — train: {0: 2146, 1: 536}, val: {0: 287, 1: 71}, test: {0: 430, 1: 107}
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Split data (only_test): 0 train, 0 val, 5157 test (non-balanced)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 16/700 — ('94_better_spam', 20, 'resid_post', 47, '{"class_counts": {"0": 1750, "1": 1}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 25/700 — ('94_better_spam', 20, 'resid_post', 46, '{"class_counts": {"0": 1750, "1": 2}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set

=== Group 15/700 — ('94_better_spam', 20, 'resid_post', 46, '{"class_counts": {"0": 1750, "1": 1}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 

=== Group 13/700 — ('94_better_spam', 20, 'resid_post', 44, '{"class_counts": {"0": 1750, "1": 1}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 30/700 — ('94_better_spam', 20, 'resid_post', 51, '{"class_counts": {"0": 1750, "1": 2}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 

=== Group 28/700 — ('94_better_spam', 20, 'resid_post', 49, '{"class_counts": {"0": 1750, "1": 2}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_2_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_1_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 12/700 — ('94_better_spam', 20, 'resid_post', 43, '{"class_counts": {"0": 1750, "1": 1}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 

=== Group 3/700 — ('94_better_spam', 20, 'resid_post', 44, '__none__', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
Split data: 2682 train, 358 val, 537 test
[DEBUG] Class distribution — train: {0: 2146, 1: 536}, val: {0: 287, 1: 71}, test: {0: 430, 1: 107}
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Split data (only_test): 0 train, 0 val, 5157 test (non-balanced)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set

=== Group 9/700 — ('94_better_spam', 20, 'resid_post', 50, '__none__', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
Split data: 2682 train, 358 val, 537 test
[DEBUG] Class distribution — train: {0: 2146, 1: 536}, val: {0: 287, 1: 71}, test: {0: 430, 1: 107}
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Split data (only_test): 0 train, 0 val, 5157 test (non-balanced)
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 

=== Group 23/700 — ('94_better_spam', 20, 'resid_post', 44, '{"class_counts": {"0": 1750, "1": 2}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 19/700 — ('94_better_spam', 20, 'resid_post', 50, '{"class_counts": {"0": 1750, "1": 1}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 20/700 — ('94_better_spam', 20, 'resid_post', 51, '{"class_counts": {"0": 1750, "1": 1}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 8/700 — ('94_better_spam', 20, 'resid_post', 49, '__none__', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
Split data: 2682 train, 358 val, 537 test
[DEBUG] Class distribution — train: {0: 2146, 1: 536}, val: {0: 287, 1: 71}, test: {0: 430, 1: 107}
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Split data (only_test): 0 train, 0 val, 5157 test (non-balanced)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 

=== Group 21/700 — ('94_better_spam', 20, 'resid_post', 42, '{"class_counts": {"0": 1750, "1": 2}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set

=== Group 18/700 — ('94_better_spam', 20, 'resid_post', 49, '{"class_counts": {"0": 1750, "1": 1}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 33/700 — ('94_better_spam', 20, 'resid_post', 44, '{"class_counts": {"0": 1750, "1": 5}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 

=== Group 14/700 — ('94_better_spam', 20, 'resid_post', 45, '{"class_counts": {"0": 1750, "1": 1}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_1_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 22/700 — ('94_better_spam', 20, 'resid_post', 43, '{"class_counts": {"0": 1750, "1": 2}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 34/700 — ('94_better_spam', 20, 'resid_post', 45, '{"class_counts": {"0": 1750, "1": 5}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 29/700 — ('94_better_spam', 20, 'resid_post', 50, '{"class_counts": {"0": 1750, "1": 2}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 35/700 — ('94_better_spam', 20, 'resid_post', 46, '{"class_counts": {"0": 1750, "1": 5}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 

=== Group 36/700 — ('94_better_spam', 20, 'resid_post', 47, '{"class_counts": {"0": 1750, "1": 5}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_5_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 17/700 — ('94_better_spam', 20, 'resid_post', 48, '{"class_counts": {"0": 1750, "1": 1}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 

=== Group 38/700 — ('94_better_spam', 20, 'resid_post', 49, '{"class_counts": {"0": 1750, "1": 5}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 

=== Group 39/700 — ('94_better_spam', 20, 'resid_post', 50, '{"class_counts": {"0": 1750, "1": 5}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_5_state.npz
    [BATCH] Evaluating on 94_better_spam validation set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_5_state.npz
  🚀 [BATCH] Training probe: sklearn_linear
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_5_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 37/700 — ('94_better_spam', 20, 'resid_post', 48, '{"class_counts": {"0": 1750, "1": 5}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 40/700 — ('94_better_spam', 20, 'resid_post', 51, '{"class_counts": {"0": 1750, "1": 5}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 

=== Group 41/700 — ('94_better_spam', 20, 'resid_post', 42, '{"class_counts": {"0": 1750, "1": 10}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 

=== Group 11/700 — ('94_better_spam', 20, 'resid_post', 42, '{"class_counts": {"0": 1750, "1": 1}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 

=== Group 42/700 — ('94_better_spam', 20, 'resid_post', 43, '{"class_counts": {"0": 1750, "1": 10}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 

=== Group 43/700 — ('94_better_spam', 20, 'resid_post', 44, '{"class_counts": {"0": 1750, "1": 10}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 24/700 — ('94_better_spam', 20, 'resid_post', 45, '{"class_counts": {"0": 1750, "1": 2}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_10_state.npz
  🚀 [BATCH] Training probe: sklearn_linear
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 44/700 — ('94_better_spam', 20, 'resid_post', 45, '{"class_counts": {"0": 1750, "1": 10}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 46/700 — ('94_better_spam', 20, 'resid_post', 47, '{"class_counts": {"0": 1750, "1": 10}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 47/700 — ('94_better_spam', 20, 'resid_post', 48, '{"class_counts": {"0": 1750, "1": 10}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_10_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 48/700 — ('94_better_spam', 20, 'resid_post', 49, '{"class_counts": {"0": 1750, "1": 10}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_10_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 45/700 — ('94_better_spam', 20, 'resid_post', 46, '{"class_counts": {"0": 1750, "1": 10}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_10_state.npz
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_10_state.npz
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_10_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_10_state.npz
    [BATCH] Evaluating on 94_better_spam validation set
  🤔 [BATCH] Evaluating on 2 datasets…
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_10_state.npz
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set

=== Group 2/700 — ('94_better_spam', 20, 'resid_post', 43, '__none__', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
Split data: 2682 train, 358 val, 537 test
[DEBUG] Class distribution — train: {0: 2146, 1: 536}, val: {0: 287, 1: 71}, test: {0: 430, 1: 107}
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Split data (only_test): 0 train, 0 val, 5157 test (non-balanced)
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_10_state.npz
  🚀 [BATCH] Training probe: act_sim
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 

=== Group 51/700 — ('94_better_spam', 20, 'resid_post', 42, '{"class_counts": {"0": 1750, "1": 15}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…

=== Group 49/700 — ('94_better_spam', 20, 'resid_post', 50, '{"class_counts": {"0": 1750, "1": 10}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set

=== Group 50/700 — ('94_better_spam', 20, 'resid_post', 51, '{"class_counts": {"0": 1750, "1": 10}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_15_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_10_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: act_sim
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_10_state.npz
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_10_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 52/700 — ('94_better_spam', 20, 'resid_post', 43, '{"class_counts": {"0": 1750, "1": 15}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_15_state.npz
    [BATCH] Evaluating on 94_better_spam test set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 55/700 — ('94_better_spam', 20, 'resid_post', 46, '{"class_counts": {"0": 1750, "1": 15}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 58/700 — ('94_better_spam', 20, 'resid_post', 49, '{"class_counts": {"0": 1750, "1": 15}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 

=== Group 56/700 — ('94_better_spam', 20, 'resid_post', 47, '{"class_counts": {"0": 1750, "1": 15}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 57/700 — ('94_better_spam', 20, 'resid_post', 48, '{"class_counts": {"0": 1750, "1": 15}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 

=== Group 1/700 — ('94_better_spam', 20, 'resid_post', 42, '__none__', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
Split data: 2682 train, 358 val, 537 test
[DEBUG] Class distribution — train: {0: 2146, 1: 536}, val: {0: 287, 1: 71}, test: {0: 430, 1: 107}
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Split data (only_test): 0 train, 0 val, 5157 test (non-balanced)
  🚀 [BATCH] Training probe: sklearn_linear

=== Group 59/700 — ('94_better_spam', 20, 'resid_post', 50, '{"class_counts": {"0": 1750, "1": 15}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam test set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 10/700 — ('94_better_spam', 20, 'resid_post', 51, '__none__', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
Split data: 2682 train, 358 val, 537 test
[DEBUG] Class distribution — train: {0: 2146, 1: 536}, val: {0: 287, 1: 71}, test: {0: 430, 1: 107}
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Split data (only_test): 0 train, 0 val, 5157 test (non-balanced)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set

=== Group 53/700 — ('94_better_spam', 20, 'resid_post', 44, '{"class_counts": {"0": 1750, "1": 15}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_15_state.npz
    [BATCH] Evaluating on 94_better_spam validation set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 

=== Group 60/700 — ('94_better_spam', 20, 'resid_post', 51, '{"class_counts": {"0": 1750, "1": 15}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…

=== Group 54/700 — ('94_better_spam', 20, 'resid_post', 45, '{"class_counts": {"0": 1750, "1": 15}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_15_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_15_state.npz
  🚀 [BATCH] Training probe: sklearn_linear
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_15_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_15_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_15_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 61/700 — ('94_better_spam', 20, 'resid_post', 42, '{"class_counts": {"0": 1750, "1": 20}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 

=== Group 62/700 — ('94_better_spam', 20, 'resid_post', 43, '{"class_counts": {"0": 1750, "1": 20}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 64/700 — ('94_better_spam', 20, 'resid_post', 45, '{"class_counts": {"0": 1750, "1": 20}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 65/700 — ('94_better_spam', 20, 'resid_post', 46, '{"class_counts": {"0": 1750, "1": 20}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_20_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_20_state.npz
  🚀 [BATCH] Training probe: sklearn_linear
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_20_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set

=== Group 63/700 — ('94_better_spam', 20, 'resid_post', 44, '{"class_counts": {"0": 1750, "1": 20}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_20_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 

=== Group 68/700 — ('94_better_spam', 20, 'resid_post', 49, '{"class_counts": {"0": 1750, "1": 20}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 66/700 — ('94_better_spam', 20, 'resid_post', 47, '{"class_counts": {"0": 1750, "1": 20}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_20_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 

=== Group 67/700 — ('94_better_spam', 20, 'resid_post', 48, '{"class_counts": {"0": 1750, "1": 20}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 

=== Group 69/700 — ('94_better_spam', 20, 'resid_post', 50, '{"class_counts": {"0": 1750, "1": 20}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_20_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 

=== Group 70/700 — ('94_better_spam', 20, 'resid_post', 51, '{"class_counts": {"0": 1750, "1": 20}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_20_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_20_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_20_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_20_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_20_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 71/700 — ('94_better_spam', 20, 'resid_post', 42, '{"class_counts": {"0": 1750, "1": 25}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 72/700 — ('94_better_spam', 20, 'resid_post', 43, '{"class_counts": {"0": 1750, "1": 25}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 73/700 — ('94_better_spam', 20, 'resid_post', 44, '{"class_counts": {"0": 1750, "1": 25}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 

=== Group 75/700 — ('94_better_spam', 20, 'resid_post', 46, '{"class_counts": {"0": 1750, "1": 25}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_25_state.npz
  🚀 [BATCH] Training probe: sklearn_linear
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 

=== Group 76/700 — ('94_better_spam', 20, 'resid_post', 47, '{"class_counts": {"0": 1750, "1": 25}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 

=== Group 77/700 — ('94_better_spam', 20, 'resid_post', 48, '{"class_counts": {"0": 1750, "1": 25}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 

=== Group 78/700 — ('94_better_spam', 20, 'resid_post', 49, '{"class_counts": {"0": 1750, "1": 25}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_25_state.npz
    [BATCH] Evaluating on 94_better_spam validation set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_25_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_25_state.npz
    [BATCH] Evaluating on 94_better_spam validation set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 80/700 — ('94_better_spam', 20, 'resid_post', 51, '{"class_counts": {"0": 1750, "1": 25}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 

=== Group 81/700 — ('94_better_spam', 20, 'resid_post', 42, '{"class_counts": {"0": 1750, "1": 35}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 

=== Group 74/700 — ('94_better_spam', 20, 'resid_post', 45, '{"class_counts": {"0": 1750, "1": 25}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 5/700 — ('94_better_spam', 20, 'resid_post', 46, '__none__', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
Split data: 2682 train, 358 val, 537 test
[DEBUG] Class distribution — train: {0: 2146, 1: 536}, val: {0: 287, 1: 71}, test: {0: 430, 1: 107}
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Split data (only_test): 0 train, 0 val, 5157 test (non-balanced)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 

=== Group 82/700 — ('94_better_spam', 20, 'resid_post', 43, '{"class_counts": {"0": 1750, "1": 35}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set

=== Group 79/700 — ('94_better_spam', 20, 'resid_post', 50, '{"class_counts": {"0": 1750, "1": 25}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_25_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 27/700 — ('94_better_spam', 20, 'resid_post', 48, '{"class_counts": {"0": 1750, "1": 2}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 

=== Group 83/700 — ('94_better_spam', 20, 'resid_post', 44, '{"class_counts": {"0": 1750, "1": 35}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_25_state.npz
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_35_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 

=== Group 84/700 — ('94_better_spam', 20, 'resid_post', 45, '{"class_counts": {"0": 1750, "1": 35}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_25_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_25_state.npz
    [BATCH] Evaluating on 94_better_spam validation set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_2_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 

=== Group 85/700 — ('94_better_spam', 20, 'resid_post', 46, '{"class_counts": {"0": 1750, "1": 35}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_2_state.npz
  🚀 [BATCH] Training probe: act_sim
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_35_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_25_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_2_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam test set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: act_sim_softmax
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 88/700 — ('94_better_spam', 20, 'resid_post', 49, '{"class_counts": {"0": 1750, "1": 35}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 87/700 — ('94_better_spam', 20, 'resid_post', 48, '{"class_counts": {"0": 1750, "1": 35}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 89/700 — ('94_better_spam', 20, 'resid_post', 50, '{"class_counts": {"0": 1750, "1": 35}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: sklearn_linear
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_35_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 91/700 — ('94_better_spam', 20, 'resid_post', 42, '{"class_counts": {"0": 1750, "1": 45}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_35_state.npz
    [BATCH] Evaluating on 94_better_spam validation set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_35_state.npz
  🚀 [BATCH] Training probe: act_sim
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_35_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim

=== Group 90/700 — ('94_better_spam', 20, 'resid_post', 51, '{"class_counts": {"0": 1750, "1": 35}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_45_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 86/700 — ('94_better_spam', 20, 'resid_post', 47, '{"class_counts": {"0": 1750, "1": 35}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 92/700 — ('94_better_spam', 20, 'resid_post', 43, '{"class_counts": {"0": 1750, "1": 45}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 93/700 — ('94_better_spam', 20, 'resid_post', 44, '{"class_counts": {"0": 1750, "1": 45}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 94/700 — ('94_better_spam', 20, 'resid_post', 45, '{"class_counts": {"0": 1750, "1": 45}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 95/700 — ('94_better_spam', 20, 'resid_post', 46, '{"class_counts": {"0": 1750, "1": 45}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 

=== Group 96/700 — ('94_better_spam', 20, 'resid_post', 47, '{"class_counts": {"0": 1750, "1": 45}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 97/700 — ('94_better_spam', 20, 'resid_post', 48, '{"class_counts": {"0": 1750, "1": 45}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_45_state.npz
    [BATCH] Evaluating on 94_better_spam test set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_45_state.npz

=== Group 99/700 — ('94_better_spam', 20, 'resid_post', 50, '{"class_counts": {"0": 1750, "1": 45}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set

=== Group 110/700 — ('94_better_spam', 20, 'resid_post', 51, '{"class_counts": {"0": 1750, "1": 50}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_50_state.npz

=== Group 98/700 — ('94_better_spam', 20, 'resid_post', 49, '{"class_counts": {"0": 1750, "1": 45}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  🤔 [BATCH] Evaluating on 2 datasets…

=== Group 100/700 — ('94_better_spam', 20, 'resid_post', 51, '{"class_counts": {"0": 1750, "1": 45}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 101/700 — ('94_better_spam', 20, 'resid_post', 42, '{"class_counts": {"0": 1750, "1": 50}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear

=== Group 102/700 — ('94_better_spam', 20, 'resid_post', 43, '{"class_counts": {"0": 1750, "1": 50}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_45_state.npz
    [BATCH] Evaluating on 94_better_spam validation set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set

=== Group 105/700 — ('94_better_spam', 20, 'resid_post', 46, '{"class_counts": {"0": 1750, "1": 50}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 

=== Group 104/700 — ('94_better_spam', 20, 'resid_post', 45, '{"class_counts": {"0": 1750, "1": 50}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_50_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_50_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set

=== Group 106/700 — ('94_better_spam', 20, 'resid_post', 47, '{"class_counts": {"0": 1750, "1": 50}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 

=== Group 108/700 — ('94_better_spam', 20, 'resid_post', 49, '{"class_counts": {"0": 1750, "1": 50}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_50_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_50_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_50_state.npz
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_50_state.npz
    [BATCH] Evaluating on 94_better_spam test set
  🤔 [BATCH] Evaluating on 2 datasets…

=== Group 103/700 — ('94_better_spam', 20, 'resid_post', 44, '{"class_counts": {"0": 1750, "1": 50}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam validation set

=== Group 109/700 — ('94_better_spam', 20, 'resid_post', 50, '{"class_counts": {"0": 1750, "1": 50}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 

=== Group 111/700 — ('94_better_spam', 20, 'resid_post', 42, '{"class_counts": {"0": 1750, "1": 100}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_50_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_50_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_100_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_50_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_50_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_50_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_45_state.npz
    [BATCH] Evaluating on 94_better_spam test set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_45_state.npz
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_50_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_100_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: sklearn_linear
  🚀 [BATCH] Training probe: sklearn_linear
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_50_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_50_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_50_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  🤔 [BATCH] Evaluating on 2 datasets…
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_50_state.npz
  🚀 [BATCH] Training probe: sklearn_linear
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_50_state.npz
    [BATCH] Evaluating on 94_better_spam test set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_50_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_50_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_50_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_100_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_45_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_50_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_50_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: act_sim
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_50_state.npz
    [BATCH] Evaluating on 94_better_spam validation set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_50_state.npz
  🚀 [BATCH] Training probe: sklearn_linear
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_50_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_50_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: sklearn_linear
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_50_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_50_state.npz
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_50_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 114/700 — ('94_better_spam', 20, 'resid_post', 45, '{"class_counts": {"0": 1750, "1": 100}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_100_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_100_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_50_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_50_state.npz
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_50_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_45_state.npz
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_50_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_50_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_50_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_100_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_50_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_50_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_100_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_50_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_50_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_50_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_50_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_50_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_100_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 

=== Group 112/700 — ('94_better_spam', 20, 'resid_post', 43, '{"class_counts": {"0": 1750, "1": 100}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_45_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_100_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_100_state.npz
  🚀 [BATCH] Training probe: act_sim
  🤔 [BATCH] Evaluating on 2 datasets…
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_50_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_50_state.npz
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_50_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_50_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_50_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_50_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_50_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_50_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_50_state.npz
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_100_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_45_state.npz
    [BATCH] Evaluating on 94_better_spam test set
  🤔 [BATCH] Evaluating on 2 datasets…
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_100_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_100_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_50_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_50_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_50_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_50_state.npz
    [BATCH] Evaluating on 94_better_spam test set
  🤔 [BATCH] Evaluating on 2 datasets…
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_50_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_50_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_50_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_50_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_100_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_50_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_100_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_100_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_50_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_45_state.npz
    [BATCH] Evaluating on 94_better_spam test set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_50_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_50_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_100_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_100_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_50_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_50_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_50_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_45_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_100_state.npz
  🚀 [BATCH] Training probe: act_sim
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_100_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_50_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_50_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_50_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_50_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_50_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_100_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 

=== Group 107/700 — ('94_better_spam', 20, 'resid_post', 48, '{"class_counts": {"0": 1750, "1": 50}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_50_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_100_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_100_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_50_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 

=== Group 115/700 — ('94_better_spam', 20, 'resid_post', 46, '{"class_counts": {"0": 1750, "1": 100}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_100_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_100_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_100_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_50_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_100_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_50_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_100_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_100_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_50_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_100_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set

=== Group 116/700 — ('94_better_spam', 20, 'resid_post', 47, '{"class_counts": {"0": 1750, "1": 100}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_100_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_50_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_100_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_100_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_100_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_50_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_50_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 117/700 — ('94_better_spam', 20, 'resid_post', 48, '{"class_counts": {"0": 1750, "1": 100}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_100_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 

=== Group 118/700 — ('94_better_spam', 20, 'resid_post', 49, '{"class_counts": {"0": 1750, "1": 100}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_100_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_100_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 

=== Group 26/700 — ('94_better_spam', 20, 'resid_post', 47, '{"class_counts": {"0": 1750, "1": 2}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_100_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_100_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_100_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_100_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_100_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 

=== Group 119/700 — ('94_better_spam', 20, 'resid_post', 50, '{"class_counts": {"0": 1750, "1": 100}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_100_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_100_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_100_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 7/700 — ('94_better_spam', 20, 'resid_post', 48, '__none__', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
Split data: 2682 train, 358 val, 537 test
[DEBUG] Class distribution — train: {0: 2146, 1: 536}, val: {0: 287, 1: 71}, test: {0: 430, 1: 107}
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Split data (only_test): 0 train, 0 val, 5157 test (non-balanced)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_100_state.npz

=== Group 113/700 — ('94_better_spam', 20, 'resid_post', 44, '{"class_counts": {"0": 1750, "1": 100}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set

=== Group 32/700 — ('94_better_spam', 20, 'resid_post', 43, '{"class_counts": {"0": 1750, "1": 5}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 

=== Group 120/700 — ('94_better_spam', 20, 'resid_post', 51, '{"class_counts": {"0": 1750, "1": 100}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear

=== Group 122/700 — ('94_better_spam', 20, 'resid_post', 43, '{"class_counts": {"0": 1750, "1": 150}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear

=== Group 121/700 — ('94_better_spam', 20, 'resid_post', 42, '{"class_counts": {"0": 1750, "1": 150}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_5_state.npz
    [BATCH] Evaluating on 94_better_spam validation set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_100_state.npz

=== Group 124/700 — ('94_better_spam', 20, 'resid_post', 45, '{"class_counts": {"0": 1750, "1": 150}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  🤔 [BATCH] Evaluating on 2 datasets…
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_100_state.npz

=== Group 123/700 — ('94_better_spam', 20, 'resid_post', 44, '{"class_counts": {"0": 1750, "1": 150}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear

=== Group 126/700 — ('94_better_spam', 20, 'resid_post', 47, '{"class_counts": {"0": 1750, "1": 150}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_150_state.npz
  - 😋 Using cached evaluation result 

=== Group 137/700 — ('94_better_spam', 20, 'resid_post', 48, '{"class_counts": {"0": 1750, "1": 250}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_150_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 

=== Group 130/700 — ('94_better_spam', 20, 'resid_post', 51, '{"class_counts": {"0": 1750, "1": 150}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_150_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_150_state.npz
    [BATCH] Evaluating on 94_better_spam validation set
  🤔 [BATCH] Evaluating on 2 datasets…
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_150_state.npz
    [BATCH] Evaluating on 94_better_spam validation set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 136/700 — ('94_better_spam', 20, 'resid_post', 47, '{"class_counts": {"0": 1750, "1": 250}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_250_state.npz

=== Group 135/700 — ('94_better_spam', 20, 'resid_post', 46, '{"class_counts": {"0": 1750, "1": 250}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_150_state.npz
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam validation set

=== Group 138/700 — ('94_better_spam', 20, 'resid_post', 49, '{"class_counts": {"0": 1750, "1": 250}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam test set

=== Group 125/700 — ('94_better_spam', 20, 'resid_post', 46, '{"class_counts": {"0": 1750, "1": 150}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear

=== Group 139/700 — ('94_better_spam', 20, 'resid_post', 50, '{"class_counts": {"0": 1750, "1": 250}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear

=== Group 134/700 — ('94_better_spam', 20, 'resid_post', 45, '{"class_counts": {"0": 1750, "1": 250}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_250_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_250_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…

=== Group 129/700 — ('94_better_spam', 20, 'resid_post', 50, '{"class_counts": {"0": 1750, "1": 150}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set

=== Group 133/700 — ('94_better_spam', 20, 'resid_post', 44, '{"class_counts": {"0": 1750, "1": 250}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam validation set

=== Group 128/700 — ('94_better_spam', 20, 'resid_post', 49, '{"class_counts": {"0": 1750, "1": 150}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_250_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_150_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…

=== Group 132/700 — ('94_better_spam', 20, 'resid_post', 43, '{"class_counts": {"0": 1750, "1": 250}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_250_state.npz
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_250_state.npz

=== Group 131/700 — ('94_better_spam', 20, 'resid_post', 42, '{"class_counts": {"0": 1750, "1": 250}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_250_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_150_state.npz
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_250_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🤔 [BATCH] Evaluating on 2 datasets…
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_150_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_250_state.npz
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
  🤔 [BATCH] Evaluating on 2 datasets…

=== Group 127/700 — ('94_better_spam', 20, 'resid_post', 48, '{"class_counts": {"0": 1750, "1": 150}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_150_state.npz

=== Group 6/700 — ('94_better_spam', 20, 'resid_post', 47, '__none__', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
Split data: 2682 train, 358 val, 537 test
[DEBUG] Class distribution — train: {0: 2146, 1: 536}, val: {0: 287, 1: 71}, test: {0: 430, 1: 107}
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Split data (only_test): 0 train, 0 val, 5157 test (non-balanced)
  🚀 [BATCH] Training probe: sklearn_linear
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_100_state.npz
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_2_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_100_state.npz
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_100_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_100_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_100_state.npz
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_150_state.npz
  🚀 [BATCH] Training probe: sklearn_linear
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_150_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_150_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_250_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  🚀 [BATCH] Training probe: sklearn_linear
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_150_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_250_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🤔 [BATCH] Evaluating on 2 datasets…
  - Training new probe …
    [BATCH] Evaluating on 94_better_spam validation set
  [DEBUG] Starting dataset preparation...
  🚀 [BATCH] Training probe: sklearn_linear
  🚀 [BATCH] Training probe: sklearn_linear
  🚀 [BATCH] Training probe: sklearn_linear
  [DEBUG] Using activation_type: linear_max
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_100_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_150_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_150_state.npz
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_100_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  🤔 [BATCH] Evaluating on 2 datasets…
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_100_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_250_state.npz
  🚀 [BATCH] Training probe: sklearn_linear
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_150_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_100_state.npz
    [BATCH] Evaluating on 94_better_spam validation set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_250_state.npz
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_250_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_250_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_250_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_150_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_2_state.npz
  🚀 [BATCH] Training probe: sklearn_linear
  - Training new probe …
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
  [DEBUG] Starting dataset preparation...
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_150_state.npz
  [DEBUG] Using activation_type: linear_max
    [BATCH] Evaluating on 94_better_spam test set
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_250_state.npz
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_150_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_100_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_100_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_100_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_5_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_100_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_100_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_250_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_150_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_150_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: sklearn_linear
  🚀 [BATCH] Training probe: sklearn_linear
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_150_state.npz
  🚀 [BATCH] Training probe: sklearn_linear
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_250_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_250_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_250_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🤔 [BATCH] Evaluating on 2 datasets…
  🚀 [BATCH] Training probe: sklearn_linear
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_150_state.npz
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_150_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_250_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_150_state.npz
  🚀 [BATCH] Training probe: sklearn_linear
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_250_state.npz
    [BATCH] Evaluating on 94_better_spam test set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_150_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_100_state.npz
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_100_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_100_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  🚀 [BATCH] Training probe: sklearn_linear
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_150_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_150_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_250_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🤔 [BATCH] Evaluating on 2 datasets…
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_150_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_250_state.npz
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_250_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_100_state.npz
  🚀 [BATCH] Training probe: sklearn_linear
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_150_state.npz
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_150_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_250_state.npz
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_250_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_250_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: sklearn_linear
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_2_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_150_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_100_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_150_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_250_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_100_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_250_state.npz
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_100_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_150_state.npz
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_150_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_150_state.npz
  🚀 [BATCH] Training probe: sklearn_linear
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_150_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_150_state.npz
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_250_state.npz
    [BATCH] Evaluating on 94_better_spam test set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_100_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_250_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_150_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_250_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_150_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: act_sim
  🚀 [BATCH] Training probe: act_sim
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_100_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_100_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_150_state.npz
    [BATCH] Evaluating on 94_better_spam validation set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_250_state.npz
    [BATCH] Evaluating on 94_better_spam validation set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_250_state.npz
  🚀 [BATCH] Training probe: act_sim
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam test set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_250_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_150_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_250_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_250_state.npz
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_150_state.npz
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_150_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_150_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_250_state.npz
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_150_state.npz
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_150_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_150_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_100_state.npz
  🚀 [BATCH] Training probe: act_sim
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_100_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_150_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_250_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_150_state.npz
  🚀 [BATCH] Training probe: sklearn_linear
  🤔 [BATCH] Evaluating on 2 datasets…
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_250_state.npz
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_150_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_250_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_250_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_250_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_150_state.npz
  🚀 [BATCH] Training probe: act_sim
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_150_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_250_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_250_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_250_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_150_state.npz
    [BATCH] Evaluating on 94_better_spam test set
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_150_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_150_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_150_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_250_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_5_state.npz
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_150_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_250_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_250_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_150_state.npz
    [BATCH] Evaluating on 94_better_spam test set
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_250_state.npz
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: act_sim
  🚀 [BATCH] Training probe: act_sim
  🚀 [BATCH] Training probe: act_sim
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_150_state.npz
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_250_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_250_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_150_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_250_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_150_state.npz
    [BATCH] Evaluating on 94_better_spam test set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_250_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_150_state.npz
  🚀 [BATCH] Training probe: act_sim
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_150_state.npz
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_150_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 140/700 — ('94_better_spam', 20, 'resid_post', 51, '{"class_counts": {"0": 1750, "1": 250}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear

=== Group 142/700 — ('94_better_spam', 20, 'resid_post', 43, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 20}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim

=== Group 144/700 — ('94_better_spam', 20, 'resid_post', 45, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 20}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear

=== Group 141/700 — ('94_better_spam', 20, 'resid_post', 42, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 20}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  🚀 [BATCH] Training probe: act_sim

=== Group 145/700 — ('94_better_spam', 20, 'resid_post', 46, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 20}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_20x_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_250_state.npz

=== Group 143/700 — ('94_better_spam', 20, 'resid_post', 44, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 20}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  🤔 [BATCH] Evaluating on 2 datasets…
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_250_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam validation set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_250_state.npz
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_20x_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_20x_state.npz
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_250_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_150_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_150_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_250_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_250_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_150_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_250_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_250_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: act_sim
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_250_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_20x_state.npz
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_150_state.npz
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_150_state.npz
  🚀 [BATCH] Training probe: sklearn_linear
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_20x_state.npz
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_150_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_20x_state.npz
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_250_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_150_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos10_20x_state.npz
    [BATCH] Evaluating on 94_better_spam validation set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos10_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos10_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos10_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos10_20x_state.npz
    [BATCH] Evaluating on 94_better_spam test set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos10_20x_state.npz
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos10_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos10_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos10_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos10_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos10_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos10_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos10_20x_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos10_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos10_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos10_20x_state.npz
    [BATCH] Evaluating on 94_better_spam validation set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos10_20x_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos10_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos10_20x_state.npz
  🚀 [BATCH] Training probe: act_sim
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos10_20x_state.npz
  🚀 [BATCH] Training probe: act_sim
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos10_20x_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos10_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos10_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos10_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos10_20x_state.npz
    [BATCH] Evaluating on 94_better_spam validation set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos10_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos10_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos10_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos10_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos10_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 147/700 — ('94_better_spam', 20, 'resid_post', 48, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 20}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos10_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos10_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos10_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos10_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos10_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos10_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 31/700 — ('94_better_spam', 20, 'resid_post', 42, '{"class_counts": {"0": 1750, "1": 5}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set

=== Group 150/700 — ('94_better_spam', 20, 'resid_post', 51, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 20}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 152/700 — ('94_better_spam', 20, 'resid_post', 43, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 10}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos10_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos10_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos10_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos10_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos10_10x_state.npz
  🚀 [BATCH] Training probe: act_sim
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_class0_1750_class1_5_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 166/700 — ('94_better_spam', 20, 'resid_post', 47, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 5}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos10_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos10_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_5x_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos10_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos10_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos10_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos10_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos10_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 162/700 — ('94_better_spam', 20, 'resid_post', 43, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 5}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 155/700 — ('94_better_spam', 20, 'resid_post', 46, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 10}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos10_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos10_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos10_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 

=== Group 167/700 — ('94_better_spam', 20, 'resid_post', 48, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 5}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set

=== Group 151/700 — ('94_better_spam', 20, 'resid_post', 42, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 10}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos10_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_10x_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos10_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos10_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 148/700 — ('94_better_spam', 20, 'resid_post', 49, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 20}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos10_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos10_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos10_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 

=== Group 146/700 — ('94_better_spam', 20, 'resid_post', 47, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 20}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear

=== Group 158/700 — ('94_better_spam', 20, 'resid_post', 49, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 10}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_10x_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_20x_state.npz
    [BATCH] Evaluating on 94_better_spam test set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos10_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - Training new probe …
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: linear_max
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos10_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos10_10x_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 160/700 — ('94_better_spam', 20, 'resid_post', 51, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 10}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 

=== Group 153/700 — ('94_better_spam', 20, 'resid_post', 44, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 10}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set

=== Group 154/700 — ('94_better_spam', 20, 'resid_post', 45, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 10}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 

=== Group 149/700 — ('94_better_spam', 20, 'resid_post', 50, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 20}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos10_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos10_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos10_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 156/700 — ('94_better_spam', 20, 'resid_post', 47, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 10}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_10x_state.npz
  🚀 [BATCH] Training probe: sklearn_linear
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_10x_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos10_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos10_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set

=== Group 165/700 — ('94_better_spam', 20, 'resid_post', 46, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 5}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos10_10x_state.npz
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_5x_state.npz
    [BATCH] Evaluating on 94_better_spam validation set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set

=== Group 161/700 — ('94_better_spam', 20, 'resid_post', 42, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 5}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_5x_state.npz
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos10_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 

=== Group 157/700 — ('94_better_spam', 20, 'resid_post', 48, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 10}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 159/700 — ('94_better_spam', 20, 'resid_post', 50, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 10}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_10x_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos10_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos10_10x_state.npz
    [BATCH] Evaluating on 94_better_spam test set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos10_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos10_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos10_10x_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_5x_state.npz
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_5x_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos10_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos10_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos10_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  🚀 [BATCH] Training probe: act_sim
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos10_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos10_10x_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos10_10x_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos10_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos10_5x_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos10_10x_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos10_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos10_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos10_10x_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos10_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos10_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos10_20x_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos10_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos10_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos10_5x_state.npz
    [BATCH] Evaluating on 94_better_spam test set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos10_10x_state.npz
    [BATCH] Evaluating on 94_better_spam test set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos10_10x_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos10_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos10_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear

=== Group 163/700 — ('94_better_spam', 20, 'resid_post', 44, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 5}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos10_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_5x_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos10_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos10_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos10_10x_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos10_10x_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos10_10x_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos10_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos10_10x_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos10_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos10_5x_state.npz
    [BATCH] Evaluating on 94_better_spam test set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos10_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos10_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim

=== Group 164/700 — ('94_better_spam', 20, 'resid_post', 45, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 5}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos10_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos10_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos10_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos10_10x_state.npz
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos10_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos10_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos10_10x_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos10_10x_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos10_10x_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_5x_state.npz
    [BATCH] Evaluating on 94_better_spam validation set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos10_10x_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos10_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 168/700 — ('94_better_spam', 20, 'resid_post', 49, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 5}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 171/700 — ('94_better_spam', 20, 'resid_post', 42, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 1}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 

=== Group 170/700 — ('94_better_spam', 20, 'resid_post', 51, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 5}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 

=== Group 173/700 — ('94_better_spam', 20, 'resid_post', 44, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 1}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 

=== Group 172/700 — ('94_better_spam', 20, 'resid_post', 43, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 1}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos10_5x_state.npz
    [BATCH] Evaluating on 94_better_spam validation set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 

=== Group 175/700 — ('94_better_spam', 20, 'resid_post', 46, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 1}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: linear_max

=== Group 169/700 — ('94_better_spam', 20, 'resid_post', 50, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 5}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set

=== Group 174/700 — ('94_better_spam', 20, 'resid_post', 45, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 1}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 

=== Group 187/700 — ('94_better_spam', 20, 'resid_post', 48, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 2}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_2x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_2x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos10_2x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos10_2x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos10_2x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos10_2x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos10_2x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos10_2x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 178/700 — ('94_better_spam', 20, 'resid_post', 49, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 1}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 180/700 — ('94_better_spam', 20, 'resid_post', 51, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 1}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 176/700 — ('94_better_spam', 20, 'resid_post', 47, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 1}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 

=== Group 185/700 — ('94_better_spam', 20, 'resid_post', 46, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 2}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_2x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_2x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 

=== Group 177/700 — ('94_better_spam', 20, 'resid_post', 48, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 1}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos10_2x_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 

=== Group 189/700 — ('94_better_spam', 20, 'resid_post', 50, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 2}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_2x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos10_2x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 

=== Group 183/700 — ('94_better_spam', 20, 'resid_post', 44, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 2}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_2x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_2x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: linear_max
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos10_2x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos10_2x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 

=== Group 184/700 — ('94_better_spam', 20, 'resid_post', 45, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 2}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_2x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos10_2x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos10_2x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos10_1x_state.npz
    [BATCH] Evaluating on 94_better_spam test set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 179/700 — ('94_better_spam', 20, 'resid_post', 50, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 1}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_1x_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos10_2x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_2x_state.npz
  🚀 [BATCH] Training probe: act_sim
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos10_2x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos10_2x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos10_2x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_1x_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 

=== Group 181/700 — ('94_better_spam', 20, 'resid_post', 42, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 2}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_2x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos10_2x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos10_2x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos10_2x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos10_2x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos10_2x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 

=== Group 186/700 — ('94_better_spam', 20, 'resid_post', 47, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 2}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_2x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos10_2x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_2x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 191/700 — ('94_better_spam', 20, 'resid_post', 42, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 3}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos10_2x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 182/700 — ('94_better_spam', 20, 'resid_post', 43, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 2}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_2x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos10_2x_state.npz
    [BATCH] Evaluating on 94_better_spam test set
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos10_2x_state.npz
  🚀 [BATCH] Training probe: act_sim
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: linear_max
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos10_2x_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos10_3x_state.npz
    [BATCH] Evaluating on 94_better_spam test set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos10_2x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 

=== Group 188/700 — ('94_better_spam', 20, 'resid_post', 49, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 2}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_2x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos10_2x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_2x_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos10_2x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos10_2x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos10_2x_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos10_2x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos10_2x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos10_2x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos10_2x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos10_2x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 192/700 — ('94_better_spam', 20, 'resid_post', 43, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 3}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 

=== Group 194/700 — ('94_better_spam', 20, 'resid_post', 45, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 3}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 

=== Group 193/700 — ('94_better_spam', 20, 'resid_post', 44, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 3}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 

=== Group 195/700 — ('94_better_spam', 20, 'resid_post', 46, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 3}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 190/700 — ('94_better_spam', 20, 'resid_post', 51, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 2}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_2x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_2x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos10_2x_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set

=== Group 196/700 — ('94_better_spam', 20, 'resid_post', 47, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 3}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos10_2x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos10_2x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos10_2x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos10_2x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos10_2x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 197/700 — ('94_better_spam', 20, 'resid_post', 48, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 3}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set

=== Group 198/700 — ('94_better_spam', 20, 'resid_post', 49, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 3}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: linear_max

=== Group 200/700 — ('94_better_spam', 20, 'resid_post', 51, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 3}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos10_3x_state.npz

=== Group 202/700 — ('94_better_spam', 20, 'resid_post', 43, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 4}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_4x_state.npz
  - 😋 Using cached evaluation result 

=== Group 203/700 — ('94_better_spam', 20, 'resid_post', 44, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 4}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set

=== Group 199/700 — ('94_better_spam', 20, 'resid_post', 50, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 3}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 

=== Group 206/700 — ('94_better_spam', 20, 'resid_post', 47, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 4}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_4x_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…

=== Group 201/700 — ('94_better_spam', 20, 'resid_post', 42, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 4}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_4x_state.npz

=== Group 208/700 — ('94_better_spam', 20, 'resid_post', 49, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 4}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set

=== Group 209/700 — ('94_better_spam', 20, 'resid_post', 50, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 4}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_4x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 

=== Group 204/700 — ('94_better_spam', 20, 'resid_post', 45, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 4}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 

=== Group 207/700 — ('94_better_spam', 20, 'resid_post', 48, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 4}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_4x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_4x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…

=== Group 211/700 — ('94_better_spam', 20, 'resid_post', 42, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 1, "upsampling_factor": 20}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam validation set

=== Group 210/700 — ('94_better_spam', 20, 'resid_post', 51, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 4}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_4x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_4x_state.npz
    [BATCH] Evaluating on 94_better_spam validation set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos1_20x_state.npz

=== Group 213/700 — ('94_better_spam', 20, 'resid_post', 44, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 1, "upsampling_factor": 20}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 

=== Group 205/700 — ('94_better_spam', 20, 'resid_post', 46, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 10, "upsampling_factor": 4}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_4x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos1_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos10_4x_state.npz
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  🚀 [BATCH] Training probe: sklearn_linear
  - Training new probe …
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_4x_state.npz
  - 😋 Using cached evaluation result 
  [DEBUG] Starting dataset preparation...
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  [DEBUG] Using activation_type: linear_max
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos1_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_4x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_4x_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_4x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - Training new probe …
  - 😋 Using cached evaluation result 
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: linear_max
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_4x_state.npz
    [BATCH] Evaluating on 94_better_spam test set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_4x_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos1_20x_state.npz
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  - 😋 Using cached evaluation result 
  [DEBUG] Using activation_type: linear_max
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos10_4x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos10_4x_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos10_4x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos10_4x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos1_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos10_4x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos10_4x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos10_4x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: sklearn_linear
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos10_4x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos10_4x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: sklearn_linear
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos1_20x_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos10_4x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos10_4x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos10_4x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos10_4x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos1_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos10_4x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos10_4x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam test set
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos10_4x_state.npz
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos10_4x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos10_4x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos1_20x_state.npz
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos10_4x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos10_4x_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  🤔 [BATCH] Evaluating on 2 datasets…
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos10_4x_state.npz
  - 😋 Using cached evaluation result 
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos1_20x_state.npz
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos10_4x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos10_4x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos10_4x_state.npz
    [BATCH] Evaluating on 94_better_spam validation set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos1_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 212/700 — ('94_better_spam', 20, 'resid_post', 43, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 1, "upsampling_factor": 20}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos1_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos1_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos1_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos1_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos1_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos1_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos1_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos1_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 215/700 — ('94_better_spam', 20, 'resid_post', 46, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 1, "upsampling_factor": 20}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos1_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: linear_max

=== Group 221/700 — ('94_better_spam', 20, 'resid_post', 42, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 1, "upsampling_factor": 10}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: linear_mean

=== Group 220/700 — ('94_better_spam', 20, 'resid_post', 51, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 1, "upsampling_factor": 20}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos1_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 

=== Group 214/700 — ('94_better_spam', 20, 'resid_post', 45, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 1, "upsampling_factor": 20}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos1_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos1_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos1_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos1_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 218/700 — ('94_better_spam', 20, 'resid_post', 49, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 1, "upsampling_factor": 20}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos1_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos1_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos1_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos1_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 217/700 — ('94_better_spam', 20, 'resid_post', 48, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 1, "upsampling_factor": 20}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos1_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos1_20x_state.npz

=== Group 216/700 — ('94_better_spam', 20, 'resid_post', 47, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 1, "upsampling_factor": 20}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos1_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 

=== Group 219/700 — ('94_better_spam', 20, 'resid_post', 50, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 1, "upsampling_factor": 20}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos1_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos1_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos1_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos1_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos1_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos1_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos1_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos1_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos1_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 222/700 — ('94_better_spam', 20, 'resid_post', 43, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 1, "upsampling_factor": 10}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos1_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos1_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos1_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos1_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 223/700 — ('94_better_spam', 20, 'resid_post', 44, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 1, "upsampling_factor": 10}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos1_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos1_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos1_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos1_10x_state.npz
    [BATCH] Evaluating on 94_better_spam validation set
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos1_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos1_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos1_10x_state.npz
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos1_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos1_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos1_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos1_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos1_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 224/700 — ('94_better_spam', 20, 'resid_post', 45, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 1, "upsampling_factor": 10}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: linear_mean

=== Group 225/700 — ('94_better_spam', 20, 'resid_post', 46, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 1, "upsampling_factor": 10}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos1_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos1_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos1_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sklearn_linear
  - [SKIP] Probe already trained: train_on_94_better_spam_sklearn_linear_softmax_L20_resid_post_llm_neg1750_pos1_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_mean_L20_resid_post_bs1280_llm_neg1750_pos1_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_max_L20_resid_post_bs1280_llm_neg1750_pos1_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_last_L20_resid_post_bs1280_llm_neg1750_pos1_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: act_sim
  - [SKIP] Probe already trained: train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_llm_neg1750_pos1_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 

=== Group 226/700 — ('94_better_spam', 20, 'resid_post', 47, '{"llm_upsampling": true, "n_real_neg": 1750, "n_real_pos": 1, "upsampling_factor": 10}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sklearn_linear
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: linear_mean
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded linear_mean aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1063.000000, 70.750000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded linear_mean aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1523.000000, 113.562500]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded linear_mean aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1049.000000, 65.937500]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded linear_mean aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1063.000000, 70.750000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded linear_mean aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1049.000000, 65.937500]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded linear_max aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-150.125000, 236.875000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7519500780031201, 'auc': 0.8200598226737181, 'precision': 0.7428571428571429, 'recall': 0.7706708268330733, 'fpr': 0.2667706708268331}
  🚀 [BATCH] Training probe: sklearn_linear
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: linear_max
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5608424336973479, 'auc': 0.6424804262061278, 'precision': 0.8421052631578947, 'recall': 0.1497659906396256, 'fpr': 0.028081123244929798}
  🚀 [BATCH] Training probe: sklearn_linear
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: linear_max
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6973478939157566, 'auc': 0.8384033333252207, 'precision': 0.9438596491228071, 'recall': 0.41965678627145087, 'fpr': 0.0249609984399376}
  🚀 [BATCH] Training probe: sklearn_linear
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: linear_max
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6981279251170047, 'auc': 0.7900146271061451, 'precision': 0.6624040920716112, 'recall': 0.8081123244929798, 'fpr': 0.4118564742589704}
  🚀 [BATCH] Training probe: sklearn_linear
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: linear_max
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8712948517940717, 'auc': 0.9350760925912855, 'precision': 0.9146341463414634, 'recall': 0.8190327613104524, 'fpr': 0.07644305772230889}
  🚀 [BATCH] Training probe: sklearn_linear
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: linear_max
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7371294851794071, 'auc': 0.8845894553410842, 'precision': 0.919889502762431, 'recall': 0.5195007800312013, 'fpr': 0.0452418096723869}
  🚀 [BATCH] Training probe: sklearn_linear
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: linear_last
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded linear_mean aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1523.000000, 113.562500]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded linear_mean aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1063.000000, 70.750000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded linear_mean aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1063.000000, 70.750000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5202808112324493, 'auc': 0.4128372935229422, 'precision': 0.5107615894039735, 'recall': 0.9625585023400937, 'fpr': 0.921996879875195}
  🚀 [BATCH] Training probe: sklearn_linear
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: linear_max
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7566302652106084, 'auc': 0.83718400218068, 'precision': 0.787085514834206, 'recall': 0.7035881435257411, 'fpr': 0.19032761310452417}
  🚀 [BATCH] Training probe: sklearn_linear
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: linear_max
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7550702028081123, 'auc': 0.8428936845461338, 'precision': 0.7142857142857143, 'recall': 0.8502340093603744, 'fpr': 0.34009360374414976}
  🚀 [BATCH] Training probe: sklearn_linear
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: linear_max
[DEBUG] Newly added count: 0
[DEBUG] Loading act_sim_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded act_sim_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-162.500000, 236.875000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
  - ❤️‍🔥 Success! Metrics: {'accuracy': 0.6865671641791045, 'auc': 0.9540125863221208}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded linear_max aggregated activations: (2000, 3584)
[DEBUG] Returning (2000, 3584) activations
[DEBUG] train activations shape: (2000, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 236.875000]
Train activations: (2000, 3584)
[DEBUG] Pre-fit sample counts — X: 2000, y: 2000
[DEBUG] y_train class distribution: {0: 1750, 1: 250}
  [DEBUG] Creating probe for architecture: sklearn_linear
  [DEBUG] Creating sklearn linear probe
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded linear_max aggregated activations: (1770, 3584)
[DEBUG] Returning (1770, 3584) activations
[DEBUG] train activations shape: (1770, 3584), dtype: float16
[DEBUG] train activations range: [-162.500000, 236.875000]
Train activations: (1770, 3584)
[DEBUG] Pre-fit sample counts — X: 1770, y: 1770
[DEBUG] y_train class distribution: {0: 1750, 1: 20}
  [DEBUG] Creating probe for architecture: sklearn_linear
  [DEBUG] Creating sklearn linear probe
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded linear_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-106.125000, 236.875000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9216417910447762, 'auc': 0.9965888839385164, 'precision': 0.991304347826087, 'recall': 0.8507462686567164, 'fpr': 0.007462686567164179}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded linear_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-106.125000, 236.875000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9776119402985075, 'auc': 0.9988583203386054, 'precision': 0.9885496183206107, 'recall': 0.9664179104477612, 'fpr': 0.011194029850746268}
[DEBUG] Newly added count: 0
[DEBUG] Loading act_sim_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded act_sim_softmax aggregated activations: (1785, 3584)
[DEBUG] Returning (1785, 3584) activations
[DEBUG] train activations shape: (1785, 3584), dtype: float16
[DEBUG] train activations range: [-1338.000000, 237.625000]
Train activations: (1785, 3584)
[DEBUG] Pre-fit sample counts — X: 1785, y: 1785
[DEBUG] y_train class distribution: {0: 1750, 1: 35}
  [DEBUG] Creating probe for architecture: act_sim
  [DEBUG] Created sklearn linear probe
  [DEBUG] Fitting probe normally...
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded linear_max aggregated activations: (1790, 3584)
[DEBUG] Returning (1790, 3584) activations
[DEBUG] train activations shape: (1790, 3584), dtype: float16
[DEBUG] train activations range: [-162.500000, 236.875000]
Train activations: (1790, 3584)
[DEBUG] Pre-fit sample counts — X: 1790, y: 1790
[DEBUG] y_train class distribution: {0: 1750, 1: 40}
  [DEBUG] Creating probe for architecture: sklearn_linear
  [DEBUG] Creating sklearn linear probe
  [DEBUG] Created sklearn linear probe
  [DEBUG] Fitting probe normally...
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded linear_mean aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-593.000000, 51.843750]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9720149253731343, 'auc': 0.9997354644687013, 'precision': 1.0, 'recall': 0.9440298507462687, 'fpr': 0.0}
  [DEBUG] Fitting activation similarity probe...

=== ACTIVATION SIMILARITY PROBE FITTING ===
Input X shape: (1785, 3584)
Input y shape: (1785,)
Task type: classification
Aggregation: softmax
Binary classification - Positive class: 1, Negative class: 0
Positive samples: 35, Negative samples: 1750
=== ACTIVATION SIMILARITY PROBE FITTING COMPLETE ===

  - 🔥 Probe state saved to train_on_94_better_spam_act_sim_softmax_L20_resid_post_bs1280_class0_1750_class1_35_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded linear_mean aggregated activations: (1760, 3584)
[DEBUG] Returning (1760, 3584) activations
[DEBUG] train activations shape: (1760, 3584), dtype: float16
[DEBUG] train activations range: [-1526.000000, 117.875000]
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sklearn_linear
  [DEBUG] Creating sklearn linear probe
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded linear_mean aggregated activations: (1760, 3584)
[DEBUG] Returning (1760, 3584) activations
[DEBUG] train activations shape: (1760, 3584), dtype: float16
[DEBUG] train activations range: [-1506.000000, 112.187500]
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sklearn_linear
  [DEBUG] Creating sklearn linear probe
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded linear_max aggregated activations: (1790, 3584)
[DEBUG] Returning (1790, 3584) activations
[DEBUG] train activations shape: (1790, 3584), dtype: float16
[DEBUG] train activations range: [-1338.000000, 236.875000]
Train activations: (1790, 3584)
[DEBUG] Pre-fit sample counts — X: 1790, y: 1790
[DEBUG] y_train class distribution: {0: 1750, 1: 40}
  [DEBUG] Creating probe for architecture: sklearn_linear
  [DEBUG] Creating sklearn linear probe
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded linear_mean aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-813.000000, 51.906250]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5279850746268657, 'auc': 0.7045277344620183, 'precision': 1.0, 'recall': 0.055970149253731345, 'fpr': 0.0}
  [DEBUG] Fitted probe normally
  - 🔥 Probe state saved to train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_250_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded linear_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-157.750000, 236.875000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
  [DEBUG] Created sklearn linear probe
  [DEBUG] Fitting probe normally...
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9309701492537313, 'auc': 0.9976331031410114, 'precision': 1.0, 'recall': 0.8619402985074627, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded linear_max aggregated activations: (1790, 3584)
[DEBUG] Returning (1790, 3584) activations
[DEBUG] train activations shape: (1790, 3584), dtype: float16
[DEBUG] train activations range: [-1338.000000, 236.875000]
Train activations: (1790, 3584)
[DEBUG] Pre-fit sample counts — X: 1790, y: 1790
[DEBUG] y_train class distribution: {0: 1750, 1: 40}
  [DEBUG] Creating probe for architecture: sklearn_linear
  [DEBUG] Creating sklearn linear probe
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded linear_mean aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-567.000000, 51.500000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded linear_max aggregated activations: (1900, 3584)
[DEBUG] Returning (1900, 3584) activations
[DEBUG] train activations shape: (1900, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 236.875000]
Train activations: (1900, 3584)
[DEBUG] Pre-fit sample counts — X: 1900, y: 1900
[DEBUG] y_train class distribution: {0: 1750, 1: 150}
  [DEBUG] Creating probe for architecture: sklearn_linear
  [DEBUG] Creating sklearn linear probe
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9458955223880597, 'auc': 0.9889451993762531, 'precision': 0.995850622406639, 'recall': 0.8955223880597015, 'fpr': 0.0037313432835820895}
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded linear_max aggregated activations: (1950, 3584)
[DEBUG] Returning (1950, 3584) activations
[DEBUG] train activations shape: (1950, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 236.875000]
Train activations: (1950, 3584)
[DEBUG] Pre-fit sample counts — X: 1950, y: 1950
[DEBUG] y_train class distribution: {0: 1750, 1: 200}
  [DEBUG] Creating probe for architecture: sklearn_linear
  [DEBUG] Creating sklearn linear probe
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded linear_max aggregated activations: (1760, 3584)
[DEBUG] Returning (1760, 3584) activations
[DEBUG] train activations shape: (1760, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 236.875000]
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sklearn_linear
  [DEBUG] Creating sklearn linear probe
  [DEBUG] Created sklearn linear probe
  [DEBUG] Fitting probe normally...
  [DEBUG] Fitted probe normally
  - 🔥 Probe state saved to train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_2x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  [DEBUG] Created sklearn linear probe
  [DEBUG] Fitting probe normally...
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded linear_max aggregated activations: (1770, 3584)
[DEBUG] Returning (1770, 3584) activations
[DEBUG] train activations shape: (1770, 3584), dtype: float16
[DEBUG] train activations range: [-1338.000000, 236.875000]
Train activations: (1770, 3584)
[DEBUG] Pre-fit sample counts — X: 1770, y: 1770
[DEBUG] y_train class distribution: {0: 1750, 1: 20}
  [DEBUG] Creating probe for architecture: sklearn_linear
  [DEBUG] Creating sklearn linear probe
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded linear_max aggregated activations: (1760, 3584)
[DEBUG] Returning (1760, 3584) activations
[DEBUG] train activations shape: (1760, 3584), dtype: float16
[DEBUG] train activations range: [-1338.000000, 236.875000]
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sklearn_linear
  [DEBUG] Creating sklearn linear probe
  [DEBUG] Created sklearn linear probe
  [DEBUG] Fitting probe normally...
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded linear_mean aggregated activations: (1760, 3584)
[DEBUG] Returning (1760, 3584) activations
[DEBUG] train activations shape: (1760, 3584), dtype: float16
[DEBUG] train activations range: [-1526.000000, 117.875000]
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sklearn_linear
  [DEBUG] Creating sklearn linear probe
  [DEBUG] Created sklearn linear probe
  [DEBUG] Fitting probe normally...
  [DEBUG] Created sklearn linear probe
  [DEBUG] Fitting probe normally...
  [DEBUG] Created sklearn linear probe
  [DEBUG] Fitting probe normally...
  [DEBUG] Created sklearn linear probe
  [DEBUG] Fitting probe normally...
  [DEBUG] Created sklearn linear probe
  [DEBUG] Fitting probe normally...
  [DEBUG] Fitted probe normally
  - 🔥 Probe state saved to train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_4x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  [DEBUG] Created sklearn linear probe
  [DEBUG] Fitting probe normally...
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded linear_max aggregated activations: (1780, 3584)
[DEBUG] Returning (1780, 3584) activations
[DEBUG] train activations shape: (1780, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 236.875000]
Train activations: (1780, 3584)
[DEBUG] Pre-fit sample counts — X: 1780, y: 1780
[DEBUG] y_train class distribution: {0: 1750, 1: 30}
  [DEBUG] Creating probe for architecture: sklearn_linear
  [DEBUG] Creating sklearn linear probe
  [DEBUG] Created sklearn linear probe
  [DEBUG] Fitting probe normally...
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded linear_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-157.750000, 236.875000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6772388059701493, 'auc': 0.9527873691245267, 'precision': 1.0, 'recall': 0.35447761194029853, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
  [DEBUG] Fitted probe normally
  - 🔥 Probe state saved to train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_4x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  [DEBUG] Fitted probe normally
  - 🔥 Probe state saved to train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_4x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  [DEBUG] Fitted probe normally
  - 🔥 Probe state saved to train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  [DEBUG] Created sklearn linear probe
  [DEBUG] Fitting probe normally...
  [DEBUG] Fitted probe normally
  - 🔥 Probe state saved to train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_150_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  [DEBUG] Fitted probe normally
  - 🔥 Probe state saved to train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos1_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  [DEBUG] Fitted probe normally
  - 🔥 Probe state saved to train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos1_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  [DEBUG] Fitted probe normally
  - 🔥 Probe state saved to train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos1_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  [DEBUG] Fitted probe normally
  - 🔥 Probe state saved to train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  [DEBUG] Fitted probe normally
  - 🔥 Probe state saved to train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  [DEBUG] Fitted probe normally
  - 🔥 Probe state saved to train_on_94_better_spam_sklearn_linear_mean_L20_resid_post_llm_neg1750_pos1_10x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  [DEBUG] Fitted probe normally
  - 🔥 Probe state saved to train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
  - ❤️‍🔥 Success! Metrics: {'accuracy': 0.6865671641791045, 'auc': 0.9540125863221208}
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9216417910447762, 'auc': 0.9965888839385164, 'precision': 0.991304347826087, 'recall': 0.8507462686567164, 'fpr': 0.007462686567164179}
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded linear_max aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-177.375000, 237.625000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded linear_max aggregated activations: (1770, 3584)
[DEBUG] Returning (1770, 3584) activations
[DEBUG] train activations shape: (1770, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 236.875000]
Train activations: (1770, 3584)
[DEBUG] Pre-fit sample counts — X: 1770, y: 1770
[DEBUG] y_train class distribution: {0: 1750, 1: 20}
  [DEBUG] Creating probe for architecture: sklearn_linear
  [DEBUG] Creating sklearn linear probe
  [DEBUG] Created sklearn linear probe
  [DEBUG] Fitting probe normally...
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded linear_max aggregated activations: (2000, 3584)
[DEBUG] Returning (2000, 3584) activations
[DEBUG] train activations shape: (2000, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 236.875000]
Train activations: (2000, 3584)
[DEBUG] Pre-fit sample counts — X: 2000, y: 2000
[DEBUG] y_train class distribution: {0: 1750, 1: 250}
  [DEBUG] Creating probe for architecture: sklearn_linear
  [DEBUG] Creating sklearn linear probe
  [DEBUG] Created sklearn linear probe
  [DEBUG] Fitting probe normally...
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8658346333853354, 'auc': 0.9366775295036762, 'precision': 0.8752, 'recall': 0.8533541341653667, 'fpr': 0.12168486739469579}
  🚀 [BATCH] Training probe: sklearn_linear
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: linear_last
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded linear_max aggregated activations: (1800, 3584)
[DEBUG] Returning (1800, 3584) activations
[DEBUG] train activations shape: (1800, 3584), dtype: float16
[DEBUG] train activations range: [-162.500000, 236.875000]
Train activations: (1800, 3584)
[DEBUG] Pre-fit sample counts — X: 1800, y: 1800
[DEBUG] y_train class distribution: {0: 1750, 1: 50}
  [DEBUG] Creating probe for architecture: sklearn_linear
  [DEBUG] Creating sklearn linear probe
  [DEBUG] Created sklearn linear probe
  [DEBUG] Fitting probe normally...
  [DEBUG] Fitted probe normally
  - 🔥 Probe state saved to train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos1_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  [DEBUG] Fitted probe normally
  - 🔥 Probe state saved to train_on_94_better_spam_sklearn_linear_max_L20_resid_post_class0_1750_class1_250_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded linear_mean aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1053.000000, 69.687500]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5070202808112324, 'auc': 0.6540823255395115, 'precision': 0.6551724137931034, 'recall': 0.029641185647425898, 'fpr': 0.015600624024960999}
  🚀 [BATCH] Training probe: sklearn_linear
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: linear_max
  [DEBUG] Fitted probe normally
  - 🔥 Probe state saved to train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_5x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded linear_max aggregated activations: (1790, 3584)
[DEBUG] Returning (1790, 3584) activations
[DEBUG] train activations shape: (1790, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.625000]
Train activations: (1790, 3584)
[DEBUG] Pre-fit sample counts — X: 1790, y: 1790
[DEBUG] y_train class distribution: {0: 1750, 1: 40}
  [DEBUG] Creating probe for architecture: sklearn_linear
  [DEBUG] Creating sklearn linear probe
  [DEBUG] Created sklearn linear probe
  [DEBUG] Fitting probe normally...
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded linear_mean aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1506.000000, 111.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8447737909516381, 'auc': 0.915593566020332, 'precision': 0.867109634551495, 'recall': 0.8143525741029641, 'fpr': 0.12480499219968799}
  🚀 [BATCH] Training probe: sklearn_linear
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: linear_max
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded linear_max aggregated activations: (1760, 3584)
[DEBUG] Returning (1760, 3584) activations
[DEBUG] train activations shape: (1760, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 236.875000]
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sklearn_linear
  [DEBUG] Creating sklearn linear probe
  [DEBUG] Created sklearn linear probe
  [DEBUG] Fitting probe normally...
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded linear_mean aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1053.000000, 69.687500]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6006240249609984, 'auc': 0.7530769249490729, 'precision': 0.8241206030150754, 'recall': 0.25585023400936036, 'fpr': 0.054602184087363496}
  🚀 [BATCH] Training probe: sklearn_linear
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: linear_max
  [DEBUG] Fitted probe normally
  - 🔥 Probe state saved to train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_4x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  [DEBUG] Fitted probe normally
  - 🔥 Probe state saved to train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_1x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded linear_max aggregated activations: (1770, 3584)
[DEBUG] Returning (1770, 3584) activations
[DEBUG] train activations shape: (1770, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.625000]
Train activations: (1770, 3584)
[DEBUG] Pre-fit sample counts — X: 1770, y: 1770
[DEBUG] y_train class distribution: {0: 1750, 1: 20}
  [DEBUG] Creating probe for architecture: sklearn_linear
  [DEBUG] Creating sklearn linear probe
  [DEBUG] Created sklearn linear probe
  [DEBUG] Fitting probe normally...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9309701492537313, 'auc': 0.9976331031410114, 'precision': 1.0, 'recall': 0.8619402985074627, 'fpr': 0.0}
  [DEBUG] Fitted probe normally
  - 🔥 Probe state saved to train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_2x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6772388059701493, 'auc': 0.9527873691245267, 'precision': 1.0, 'recall': 0.35447761194029853, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded linear_last aggregated activations: (1770, 3584)
[DEBUG] Returning (1770, 3584) activations
[DEBUG] train activations shape: (1770, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 231.625000]
Train activations: (1770, 3584)
[DEBUG] Pre-fit sample counts — X: 1770, y: 1770
[DEBUG] y_train class distribution: {0: 1750, 1: 20}
  [DEBUG] Creating probe for architecture: sklearn_linear
  [DEBUG] Creating sklearn linear probe
  [DEBUG] Created sklearn linear probe
  [DEBUG] Fitting probe normally...
  [DEBUG] Fitted probe normally
  - 🔥 Probe state saved to train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos1_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading act_sim_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded act_sim_max aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1306.000000, 236.875000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded linear_max aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-177.375000, 237.625000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
  - ❤️‍🔥 Success! Metrics: {'accuracy': 0.5, 'auc': 0.9669624538491679}
  🚀 [BATCH] Training probe: act_sim
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: act_sim_last
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8471138845553822, 'auc': 0.9608621474344153, 'precision': 0.9704016913319239, 'recall': 0.7160686427457098, 'fpr': 0.0218408736349454}
  🚀 [BATCH] Training probe: sklearn_linear
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: linear_last
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded linear_max aggregated activations: (1780, 3584)
[DEBUG] Returning (1780, 3584) activations
[DEBUG] train activations shape: (1780, 3584), dtype: float16
[DEBUG] train activations range: [-162.500000, 236.875000]
Train activations: (1780, 3584)
[DEBUG] Pre-fit sample counts — X: 1780, y: 1780
[DEBUG] y_train class distribution: {0: 1750, 1: 30}
  [DEBUG] Creating probe for architecture: sklearn_linear
  [DEBUG] Creating sklearn linear probe
  [DEBUG] Created sklearn linear probe
  [DEBUG] Fitting probe normally...
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded linear_max aggregated activations: (1770, 3584)
[DEBUG] Returning (1770, 3584) activations
[DEBUG] train activations shape: (1770, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 236.875000]
Train activations: (1770, 3584)
[DEBUG] Pre-fit sample counts — X: 1770, y: 1770
[DEBUG] y_train class distribution: {0: 1750, 1: 20}
  [DEBUG] Creating probe for architecture: sklearn_linear
  [DEBUG] Creating sklearn linear probe
  [DEBUG] Created sklearn linear probe
  [DEBUG] Fitting probe normally...
  [DEBUG] Fitted probe normally
  - 🔥 Probe state saved to train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded linear_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-1338.000000, 237.625000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8936567164179104, 'auc': 0.9955725105814212, 'precision': 1.0, 'recall': 0.7873134328358209, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded linear_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-106.125000, 237.625000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9832089552238806, 'auc': 0.9989000891067054, 'precision': 0.977859778597786, 'recall': 0.9888059701492538, 'fpr': 0.022388059701492536}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded linear_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-1338.000000, 237.625000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded linear_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-118.625000, 237.625000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded linear_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-106.125000, 236.875000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded linear_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-106.125000, 236.875000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded linear_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-162.500000, 236.875000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded linear_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-1338.000000, 237.625000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded linear_mean aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-1506.000000, 112.187500]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded linear_mean aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-676.500000, 50.625000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9402985074626866, 'auc': 0.992718311427935, 'precision': 0.9957983193277311, 'recall': 0.8843283582089553, 'fpr': 0.0037313432835820895}
    [BATCH] Evaluating on 94_better_spam test set
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9906716417910447, 'auc': 0.9995683893963021, 'precision': 0.9816849816849816, 'recall': 1.0, 'fpr': 0.018656716417910446}
    [BATCH] Evaluating on 94_better_spam test set
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6958955223880597, 'auc': 0.9948345956783248, 'precision': 1.0, 'recall': 0.3917910447761194, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
  - ❤️‍🔥 Success! Metrics: {'acc': 0.710820895522388, 'auc': 0.9762892626420138, 'precision': 1.0, 'recall': 0.4216417910447761, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
  - ❤️‍🔥 Success! Metrics: {'acc': 0.914179104477612, 'auc': 0.9950295165961238, 'precision': 1.0, 'recall': 0.8283582089552238, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
  - ❤️‍🔥 Success! Metrics: {'acc': 0.628731343283582, 'auc': 0.944795611494765, 'precision': 1.0, 'recall': 0.2574626865671642, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9160447761194029, 'auc': 0.9963939630207173, 'precision': 0.9911894273127754, 'recall': 0.8395522388059702, 'fpr': 0.007462686567164179}
    [BATCH] Evaluating on 94_better_spam test set
  - ❤️‍🔥 Success! Metrics: {'acc': 0.621268656716418, 'auc': 0.8752923813766985, 'precision': 1.0, 'recall': 0.24253731343283583, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded linear_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-106.125000, 237.625000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded linear_max aggregated activations: (1780, 3584)
[DEBUG] Returning (1780, 3584) activations
[DEBUG] train activations shape: (1780, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 236.875000]
Train activations: (1780, 3584)
[DEBUG] Pre-fit sample counts — X: 1780, y: 1780
[DEBUG] y_train class distribution: {0: 1750, 1: 30}
  [DEBUG] Creating probe for architecture: sklearn_linear
  [DEBUG] Creating sklearn linear probe
  [DEBUG] Created sklearn linear probe
  [DEBUG] Fitting probe normally...
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded linear_max aggregated activations: (1770, 3584)
[DEBUG] Returning (1770, 3584) activations
[DEBUG] train activations shape: (1770, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 236.875000]
Train activations: (1770, 3584)
[DEBUG] Pre-fit sample counts — X: 1770, y: 1770
[DEBUG] y_train class distribution: {0: 1750, 1: 20}
  [DEBUG] Creating probe for architecture: sklearn_linear
  [DEBUG] Creating sklearn linear probe
  [DEBUG] Created sklearn linear probe
  [DEBUG] Fitting probe normally...
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7145522388059702, 'auc': 0.9913399420806416, 'precision': 1.0, 'recall': 0.4291044776119403, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded linear_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-162.500000, 236.875000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded linear_last aggregated activations: (1850, 3584)
[DEBUG] Returning (1850, 3584) activations
[DEBUG] train activations shape: (1850, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 231.625000]
Train activations: (1850, 3584)
[DEBUG] Pre-fit sample counts — X: 1850, y: 1850
[DEBUG] y_train class distribution: {0: 1750, 1: 100}
  [DEBUG] Creating probe for architecture: sklearn_linear
  [DEBUG] Creating sklearn linear probe
  [DEBUG] Created sklearn linear probe
  [DEBUG] Fitting probe normally...
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7574626865671642, 'auc': 0.9945700601470261, 'precision': 1.0, 'recall': 0.5149253731343284, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded linear_max aggregated activations: (1770, 3584)
[DEBUG] Returning (1770, 3584) activations
[DEBUG] train activations shape: (1770, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 236.875000]
Train activations: (1770, 3584)
[DEBUG] Pre-fit sample counts — X: 1770, y: 1770
[DEBUG] y_train class distribution: {0: 1750, 1: 20}
  [DEBUG] Creating probe for architecture: sklearn_linear
  [DEBUG] Creating sklearn linear probe
  [DEBUG] Created sklearn linear probe
  [DEBUG] Fitting probe normally...
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded linear_mean aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-570.000000, 51.906250]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8488805970149254, 'auc': 0.9734489864112275, 'precision': 1.0, 'recall': 0.6977611940298507, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded linear_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-106.125000, 236.875000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8917910447761194, 'auc': 0.9946675206059257, 'precision': 0.9952830188679245, 'recall': 0.7873134328358209, 'fpr': 0.0037313432835820895}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded linear_max aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-164.000000, 236.875000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9321372854914196, 'auc': 0.9772829602731692, 'precision': 0.9632107023411371, 'recall': 0.8985959438377535, 'fpr': 0.0343213728549142}
  🚀 [BATCH] Training probe: sklearn_linear
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: linear_last
  [DEBUG] Fitted probe normally
  [DEBUG] Fitted probe normally
  - 🔥 Probe state saved to train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos1_20x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 🔥 Probe state saved to train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_3x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded linear_max aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-164.000000, 236.875000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
  [DEBUG] Fitted probe normally
  - 🔥 Probe state saved to train_on_94_better_spam_sklearn_linear_max_L20_resid_post_llm_neg1750_pos10_2x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8502340093603744, 'auc': 0.9495571710543929, 'precision': 0.7973509933774835, 'recall': 0.9391575663026521, 'fpr': 0.23868954758190328}
  🚀 [BATCH] Training probe: sklearn_linear
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: linear_last
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded linear_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-106.125000, 237.625000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded linear_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-1338.000000, 237.625000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9813432835820896, 'auc': 0.9985659389619068, 'precision': 0.9777777777777777, 'recall': 0.9850746268656716, 'fpr': 0.022388059701492536}
    [BATCH] Evaluating on 94_better_spam test set
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8973880597014925, 'auc': 0.9983710180441079, 'precision': 1.0, 'recall': 0.7947761194029851, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
  [DEBUG] Fitted probe normally
  - 🔥 Probe state saved to train_on_94_better_spam_sklearn_linear_last_L20_resid_post_class0_1750_class1_100_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded linear_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-119.687500, 236.875000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9328358208955224, 'auc': 0.9992063934061038, 'precision': 1.0, 'recall': 0.8656716417910447, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9402985074626866, 'auc': 0.992718311427935, 'precision': 0.9957983193277311, 'recall': 0.8843283582089553, 'fpr': 0.0037313432835820895}
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
  - ❤️‍🔥 Success! Metrics: {'acc': 0.621268656716418, 'auc': 0.8752923813766985, 'precision': 1.0, 'recall': 0.24253731343283583, 'fpr': 0.0}
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9906716417910447, 'auc': 0.9995683893963021, 'precision': 0.9816849816849816, 'recall': 1.0, 'fpr': 0.018656716417910446}
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8488805970149254, 'auc': 0.9734489864112275, 'precision': 1.0, 'recall': 0.6977611940298507, 'fpr': 0.0}
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded linear_last aggregated activations: (1790, 3584)
[DEBUG] Returning (1790, 3584) activations
[DEBUG] train activations shape: (1790, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 231.625000]
Train activations: (1790, 3584)
[DEBUG] Pre-fit sample counts — X: 1790, y: 1790
[DEBUG] y_train class distribution: {0: 1750, 1: 40}
  [DEBUG] Creating probe for architecture: sklearn_linear
  [DEBUG] Creating sklearn linear probe
  [DEBUG] Created sklearn linear probe
  [DEBUG] Fitting probe normally...
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7145522388059702, 'auc': 0.9913399420806416, 'precision': 1.0, 'recall': 0.4291044776119403, 'fpr': 0.0}
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7574626865671642, 'auc': 0.9945700601470261, 'precision': 1.0, 'recall': 0.5149253731343284, 'fpr': 0.0}
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8917910447761194, 'auc': 0.9946675206059257, 'precision': 0.9952830188679245, 'recall': 0.7873134328358209, 'fpr': 0.0037313432835820895}
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9832089552238806, 'auc': 0.9989000891067054, 'precision': 0.977859778597786, 'recall': 0.9888059701492538, 'fpr': 0.022388059701492536}
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
  - ❤️‍🔥 Success! Metrics: {'acc': 0.628731343283582, 'auc': 0.944795611494765, 'precision': 1.0, 'recall': 0.2574626865671642, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded linear_last aggregated activations: (1790, 3584)
[DEBUG] Returning (1790, 3584) activations
[DEBUG] train activations shape: (1790, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 231.625000]
Train activations: (1790, 3584)
[DEBUG] Pre-fit sample counts — X: 1790, y: 1790
[DEBUG] y_train class distribution: {0: 1750, 1: 40}
  [DEBUG] Creating probe for architecture: sklearn_linear
  [DEBUG] Creating sklearn linear probe
  [DEBUG] Created sklearn linear probe
  [DEBUG] Fitting probe normally...
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded linear_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-119.687500, 236.875000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5261194029850746, 'auc': 0.7005597014925374, 'precision': 1.0, 'recall': 0.05223880597014925, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
  [DEBUG] Fitted probe normally
  - 🔥 Probe state saved to train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos10_4x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded linear_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-109.625000, 236.875000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8880597014925373, 'auc': 0.9980925595901092, 'precision': 1.0, 'recall': 0.7761194029850746, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9813432835820896, 'auc': 0.9985659389619068, 'precision': 0.9777777777777777, 'recall': 0.9850746268656716, 'fpr': 0.022388059701492536}
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded linear_last aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-237.625000, 80.312500]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9720149253731343, 'auc': 0.9969926486968145, 'precision': 0.9884169884169884, 'recall': 0.9552238805970149, 'fpr': 0.011194029850746268}
    [BATCH] Evaluating on 94_better_spam test set
  [DEBUG] Fitted probe normally
  - 🔥 Probe state saved to train_on_94_better_spam_sklearn_linear_last_L20_resid_post_llm_neg1750_pos10_4x_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9720149253731343, 'auc': 0.9969926486968145, 'precision': 0.9884169884169884, 'recall': 0.9552238805970149, 'fpr': 0.011194029850746268}
[DEBUG] Newly added count: 0
[DEBUG] Loading linear_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded linear_last aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-256.000000, 90.687500]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8432835820895522, 'auc': 0.9598741367787926, 'precision': 0.9842105263157894, 'recall': 0.6977611940298507, 'fpr': 0.011194029850746268}
    [BATCH] Evaluating on 94_better_spam test set
Completed 0/700 groups successfully
================================================================================
😜 Run finished. Closing log file.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/riya/pivotal/pivotal-research/evaluating-probes/src/main.py", line 988, in <module>
    main()
  File "/home/riya/pivotal/pivotal-research/evaluating-probes/src/main.py", line 969, in main
    run_batched_jobs(
  File "/home/riya/pivotal/pivotal-research/evaluating-probes/src/main.py", line 746, in run_batched_jobs
    results = Parallel(
              ^^^^^^^^^
  File "/home/riya/2025_env/lib/python3.12/site-packages/joblib/parallel.py", line 2007, in __call__
    return output if self.return_generator else list(output)
                                                ^^^^^^^^^^^^
  File "/home/riya/2025_env/lib/python3.12/site-packages/joblib/parallel.py", line 1650, in _get_outputs
    yield from self._retrieve()
  File "/home/riya/2025_env/lib/python3.12/site-packages/joblib/parallel.py", line 1754, in _retrieve
    self._raise_error_fast()
  File "/home/riya/2025_env/lib/python3.12/site-packages/joblib/parallel.py", line 1789, in _raise_error_fast
    error_job.get_result(self.timeout)
  File "/home/riya/2025_env/lib/python3.12/site-packages/joblib/parallel.py", line 745, in get_result
    return self._return_or_raise()
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/riya/2025_env/lib/python3.12/site-packages/joblib/parallel.py", line 763, in _return_or_raise
    raise self._result
joblib.externals.loky.process_executor.TerminatedWorkerError: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.

The exit codes of the workers are {SIGKILL(-9)}
2025-08-17 14:30:26 Command exited with status 1. Will restart.
