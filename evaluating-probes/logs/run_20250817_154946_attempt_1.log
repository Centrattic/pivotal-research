2025-08-17 15:49:46 Logging attempt 1 to ./logs/run_20250817_154946_attempt_1.log
2025-08-17 15:49:46 Starting (attempt 1) → python -m src.main -c gemma_spam_gpu
Loaded environment variables from /lambda/nfs/riya-probing/pivotal-research/evaluating-probes/.env

=== DEBUG: Environment Variables ===
OPENAI_API_KEY=sk**************************************************************************************************************************************************************VwIA
OMP_NUM_THREADS=1
CUDA_VISIBLE_DEVICES=None
EP_BATCHED_NJOBS=2
====================================

Set CUDA_VISIBLE_DEVICES to 0, updated device to cuda:0

=== DEBUG: Environment Variables ===
OPENAI_API_KEY=sk**************************************************************************************************************************************************************VwIA
OMP_NUM_THREADS=1
CUDA_VISIBLE_DEVICES=0
EP_BATCHED_NJOBS=2
====================================

[model_check] Clearing CUDA memory at start...
Processing 10 seeds: [42, 43, 44, 45, 46, 47, 48, 49, 50, 51]

=== Skipping model_check: all runthrough directories already exist ===


=== Running LLM upsampling (external script) ===
Invoking: /lambda/nfs/riya-probing/pivotal-research/pivotal_env/bin/python -m src.llm_upsampling.llm_upsampling_script -c gemma_spam_gpu --api-key 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
Current working directory: /lambda/nfs/riya-probing/pivotal-research/evaluating-probes
Extracted from config:
  Seeds: [42, 43, 44, 45, 46, 47, 48, 49, 50, 51]
  Num real samples: [1, 2, 3, 4, 5, 10]
  Upsampling factors: [1, 2, 3, 4, 5, 10, 20]
  Train on dataset: 94_better_spam
Initializing LLM client with model: gpt-4o-mini
API key length: 164 characters
LLM client initialized successfully

================================================================================
Processing seed 42
================================================================================
Output directory: results/spam_gemma_9b/seed_42/llm_samples_94_better_spam
Extracting samples using seed 42...
Loading dataset: 94_better_spam
No model or model_name provided, not initializing activation manager.
Total positive samples in dataset: 4000
Extracted 1 samples for n_real_samples=1
Extracted 2 samples for n_real_samples=2
Extracted 3 samples for n_real_samples=3
Extracted 4 samples for n_real_samples=4
Extracted 5 samples for n_real_samples=5
Extracted 10 samples for n_real_samples=10

------------------------------------------------------------
Processing n_real_samples = 1
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_42/llm_samples_94_better_spam/samples_1.csv
   Found 20 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 1
   Starting with existing 20 samples
Maximum target samples needed: 20
📁 Using existing dataset for activation extraction
   Debug: final_df has 20 samples, original_df has 1 samples
   Debug: LLM samples (after original) should be 19
📊 Final dataset breakdown:
   - Original samples: 1
   - Total samples in final dataset: 20
   - Generated samples: 19
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 20 samples to: results/spam_gemma_9b/seed_42/llm_samples_94_better_spam/samples_1.csv
   (Includes 1 original + 19 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 2
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_42/llm_samples_94_better_spam/samples_2.csv
   Found 40 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 2
   Starting with existing 40 samples
Maximum target samples needed: 40
📁 Using existing dataset for activation extraction
   Debug: final_df has 40 samples, original_df has 2 samples
   Debug: LLM samples (after original) should be 38
📊 Final dataset breakdown:
   - Original samples: 2
   - Total samples in final dataset: 40
   - Generated samples: 38
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 40 samples to: results/spam_gemma_9b/seed_42/llm_samples_94_better_spam/samples_2.csv
   (Includes 2 original + 38 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 3
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_42/llm_samples_94_better_spam/samples_3.csv
   Found 60 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 3
   Starting with existing 60 samples
Maximum target samples needed: 60
📁 Using existing dataset for activation extraction
   Debug: final_df has 60 samples, original_df has 3 samples
   Debug: LLM samples (after original) should be 57
📊 Final dataset breakdown:
   - Original samples: 3
   - Total samples in final dataset: 60
   - Generated samples: 57
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 60 samples to: results/spam_gemma_9b/seed_42/llm_samples_94_better_spam/samples_3.csv
   (Includes 3 original + 57 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 4
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_42/llm_samples_94_better_spam/samples_4.csv
   Found 80 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 4
   Starting with existing 80 samples
Maximum target samples needed: 80
📁 Using existing dataset for activation extraction
   Debug: final_df has 80 samples, original_df has 4 samples
   Debug: LLM samples (after original) should be 76
📊 Final dataset breakdown:
   - Original samples: 4
   - Total samples in final dataset: 80
   - Generated samples: 76
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 80 samples to: results/spam_gemma_9b/seed_42/llm_samples_94_better_spam/samples_4.csv
   (Includes 4 original + 76 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 5
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_42/llm_samples_94_better_spam/samples_5.csv
   Found 100 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 5
   Starting with existing 100 samples
Maximum target samples needed: 100
📁 Using existing dataset for activation extraction
   Debug: final_df has 100 samples, original_df has 5 samples
   Debug: LLM samples (after original) should be 95
📊 Final dataset breakdown:
   - Original samples: 5
   - Total samples in final dataset: 100
   - Generated samples: 95
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 100 samples to: results/spam_gemma_9b/seed_42/llm_samples_94_better_spam/samples_5.csv
   (Includes 5 original + 95 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 10
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_42/llm_samples_94_better_spam/samples_10.csv
   Found 200 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 10
   Starting with existing 200 samples
Maximum target samples needed: 200
📁 Using existing dataset for activation extraction
   Debug: final_df has 200 samples, original_df has 10 samples
   Debug: LLM samples (after original) should be 190
📊 Final dataset breakdown:
   - Original samples: 10
   - Total samples in final dataset: 200
   - Generated samples: 190
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 200 samples to: results/spam_gemma_9b/seed_42/llm_samples_94_better_spam/samples_10.csv
   (Includes 10 original + 190 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

Seed 42 complete! Files saved to: results/spam_gemma_9b/seed_42/llm_samples_94_better_spam

================================================================================
Processing seed 43
================================================================================
Output directory: results/spam_gemma_9b/seed_43/llm_samples_94_better_spam
Extracting samples using seed 43...
Loading dataset: 94_better_spam
No model or model_name provided, not initializing activation manager.
Total positive samples in dataset: 4000
Extracted 1 samples for n_real_samples=1
Extracted 2 samples for n_real_samples=2
Extracted 3 samples for n_real_samples=3
Extracted 4 samples for n_real_samples=4
Extracted 5 samples for n_real_samples=5
Extracted 10 samples for n_real_samples=10

------------------------------------------------------------
Processing n_real_samples = 1
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_43/llm_samples_94_better_spam/samples_1.csv
   Found 20 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 1
   Starting with existing 20 samples
Maximum target samples needed: 20
📁 Using existing dataset for activation extraction
   Debug: final_df has 20 samples, original_df has 1 samples
   Debug: LLM samples (after original) should be 19
📊 Final dataset breakdown:
   - Original samples: 1
   - Total samples in final dataset: 20
   - Generated samples: 19
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 20 samples to: results/spam_gemma_9b/seed_43/llm_samples_94_better_spam/samples_1.csv
   (Includes 1 original + 19 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 2
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_43/llm_samples_94_better_spam/samples_2.csv
   Found 40 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 2
   Starting with existing 40 samples
Maximum target samples needed: 40
📁 Using existing dataset for activation extraction
   Debug: final_df has 40 samples, original_df has 2 samples
   Debug: LLM samples (after original) should be 38
📊 Final dataset breakdown:
   - Original samples: 2
   - Total samples in final dataset: 40
   - Generated samples: 38
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 40 samples to: results/spam_gemma_9b/seed_43/llm_samples_94_better_spam/samples_2.csv
   (Includes 2 original + 38 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 3
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_43/llm_samples_94_better_spam/samples_3.csv
   Found 60 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 3
   Starting with existing 60 samples
Maximum target samples needed: 60
📁 Using existing dataset for activation extraction
   Debug: final_df has 60 samples, original_df has 3 samples
   Debug: LLM samples (after original) should be 57
📊 Final dataset breakdown:
   - Original samples: 3
   - Total samples in final dataset: 60
   - Generated samples: 57
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 60 samples to: results/spam_gemma_9b/seed_43/llm_samples_94_better_spam/samples_3.csv
   (Includes 3 original + 57 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 4
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_43/llm_samples_94_better_spam/samples_4.csv
   Found 80 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 4
   Starting with existing 80 samples
Maximum target samples needed: 80
📁 Using existing dataset for activation extraction
   Debug: final_df has 80 samples, original_df has 4 samples
   Debug: LLM samples (after original) should be 76
📊 Final dataset breakdown:
   - Original samples: 4
   - Total samples in final dataset: 80
   - Generated samples: 76
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 80 samples to: results/spam_gemma_9b/seed_43/llm_samples_94_better_spam/samples_4.csv
   (Includes 4 original + 76 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 5
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_43/llm_samples_94_better_spam/samples_5.csv
   Found 100 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 5
   Starting with existing 100 samples
Maximum target samples needed: 100
📁 Using existing dataset for activation extraction
   Debug: final_df has 100 samples, original_df has 5 samples
   Debug: LLM samples (after original) should be 95
📊 Final dataset breakdown:
   - Original samples: 5
   - Total samples in final dataset: 100
   - Generated samples: 95
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 100 samples to: results/spam_gemma_9b/seed_43/llm_samples_94_better_spam/samples_5.csv
   (Includes 5 original + 95 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 10
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_43/llm_samples_94_better_spam/samples_10.csv
   Found 200 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 10
   Starting with existing 200 samples
Maximum target samples needed: 200
📁 Using existing dataset for activation extraction
   Debug: final_df has 200 samples, original_df has 10 samples
   Debug: LLM samples (after original) should be 190
📊 Final dataset breakdown:
   - Original samples: 10
   - Total samples in final dataset: 200
   - Generated samples: 190
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 200 samples to: results/spam_gemma_9b/seed_43/llm_samples_94_better_spam/samples_10.csv
   (Includes 10 original + 190 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

Seed 43 complete! Files saved to: results/spam_gemma_9b/seed_43/llm_samples_94_better_spam

================================================================================
Processing seed 44
================================================================================
Output directory: results/spam_gemma_9b/seed_44/llm_samples_94_better_spam
Extracting samples using seed 44...
Loading dataset: 94_better_spam
No model or model_name provided, not initializing activation manager.
Total positive samples in dataset: 4000
Extracted 1 samples for n_real_samples=1
Extracted 2 samples for n_real_samples=2
Extracted 3 samples for n_real_samples=3
Extracted 4 samples for n_real_samples=4
Extracted 5 samples for n_real_samples=5
Extracted 10 samples for n_real_samples=10

------------------------------------------------------------
Processing n_real_samples = 1
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_44/llm_samples_94_better_spam/samples_1.csv
   Found 20 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 1
   Starting with existing 20 samples
Maximum target samples needed: 20
📁 Using existing dataset for activation extraction
   Debug: final_df has 20 samples, original_df has 1 samples
   Debug: LLM samples (after original) should be 19
📊 Final dataset breakdown:
   - Original samples: 1
   - Total samples in final dataset: 20
   - Generated samples: 19
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 20 samples to: results/spam_gemma_9b/seed_44/llm_samples_94_better_spam/samples_1.csv
   (Includes 1 original + 19 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 2
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_44/llm_samples_94_better_spam/samples_2.csv
   Found 40 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 2
   Starting with existing 40 samples
Maximum target samples needed: 40
📁 Using existing dataset for activation extraction
   Debug: final_df has 40 samples, original_df has 2 samples
   Debug: LLM samples (after original) should be 38
📊 Final dataset breakdown:
   - Original samples: 2
   - Total samples in final dataset: 40
   - Generated samples: 38
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 40 samples to: results/spam_gemma_9b/seed_44/llm_samples_94_better_spam/samples_2.csv
   (Includes 2 original + 38 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 3
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_44/llm_samples_94_better_spam/samples_3.csv
   Found 60 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 3
   Starting with existing 60 samples
Maximum target samples needed: 60
📁 Using existing dataset for activation extraction
   Debug: final_df has 60 samples, original_df has 3 samples
   Debug: LLM samples (after original) should be 57
📊 Final dataset breakdown:
   - Original samples: 3
   - Total samples in final dataset: 60
   - Generated samples: 57
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 60 samples to: results/spam_gemma_9b/seed_44/llm_samples_94_better_spam/samples_3.csv
   (Includes 3 original + 57 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 4
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_44/llm_samples_94_better_spam/samples_4.csv
   Found 80 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 4
   Starting with existing 80 samples
Maximum target samples needed: 80
📁 Using existing dataset for activation extraction
   Debug: final_df has 80 samples, original_df has 4 samples
   Debug: LLM samples (after original) should be 76
📊 Final dataset breakdown:
   - Original samples: 4
   - Total samples in final dataset: 80
   - Generated samples: 76
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 80 samples to: results/spam_gemma_9b/seed_44/llm_samples_94_better_spam/samples_4.csv
   (Includes 4 original + 76 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 5
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_44/llm_samples_94_better_spam/samples_5.csv
   Found 100 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 5
   Starting with existing 100 samples
Maximum target samples needed: 100
📁 Using existing dataset for activation extraction
   Debug: final_df has 100 samples, original_df has 5 samples
   Debug: LLM samples (after original) should be 95
📊 Final dataset breakdown:
   - Original samples: 5
   - Total samples in final dataset: 100
   - Generated samples: 95
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 100 samples to: results/spam_gemma_9b/seed_44/llm_samples_94_better_spam/samples_5.csv
   (Includes 5 original + 95 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 10
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_44/llm_samples_94_better_spam/samples_10.csv
   Found 200 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 10
   Starting with existing 200 samples
Maximum target samples needed: 200
📁 Using existing dataset for activation extraction
   Debug: final_df has 200 samples, original_df has 10 samples
   Debug: LLM samples (after original) should be 190
📊 Final dataset breakdown:
   - Original samples: 10
   - Total samples in final dataset: 200
   - Generated samples: 190
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 200 samples to: results/spam_gemma_9b/seed_44/llm_samples_94_better_spam/samples_10.csv
   (Includes 10 original + 190 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

Seed 44 complete! Files saved to: results/spam_gemma_9b/seed_44/llm_samples_94_better_spam

================================================================================
Processing seed 45
================================================================================
Output directory: results/spam_gemma_9b/seed_45/llm_samples_94_better_spam
Extracting samples using seed 45...
Loading dataset: 94_better_spam
No model or model_name provided, not initializing activation manager.
Total positive samples in dataset: 4000
Extracted 1 samples for n_real_samples=1
Extracted 2 samples for n_real_samples=2
Extracted 3 samples for n_real_samples=3
Extracted 4 samples for n_real_samples=4
Extracted 5 samples for n_real_samples=5
Extracted 10 samples for n_real_samples=10

------------------------------------------------------------
Processing n_real_samples = 1
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_45/llm_samples_94_better_spam/samples_1.csv
   Found 20 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 1
   Starting with existing 20 samples
Maximum target samples needed: 20
📁 Using existing dataset for activation extraction
   Debug: final_df has 20 samples, original_df has 1 samples
   Debug: LLM samples (after original) should be 19
📊 Final dataset breakdown:
   - Original samples: 1
   - Total samples in final dataset: 20
   - Generated samples: 19
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 20 samples to: results/spam_gemma_9b/seed_45/llm_samples_94_better_spam/samples_1.csv
   (Includes 1 original + 19 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 2
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_45/llm_samples_94_better_spam/samples_2.csv
   Found 40 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 2
   Starting with existing 40 samples
Maximum target samples needed: 40
📁 Using existing dataset for activation extraction
   Debug: final_df has 40 samples, original_df has 2 samples
   Debug: LLM samples (after original) should be 38
📊 Final dataset breakdown:
   - Original samples: 2
   - Total samples in final dataset: 40
   - Generated samples: 38
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 40 samples to: results/spam_gemma_9b/seed_45/llm_samples_94_better_spam/samples_2.csv
   (Includes 2 original + 38 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 3
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_45/llm_samples_94_better_spam/samples_3.csv
   Found 60 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 3
   Starting with existing 60 samples
Maximum target samples needed: 60
📁 Using existing dataset for activation extraction
   Debug: final_df has 60 samples, original_df has 3 samples
   Debug: LLM samples (after original) should be 57
📊 Final dataset breakdown:
   - Original samples: 3
   - Total samples in final dataset: 60
   - Generated samples: 57
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 60 samples to: results/spam_gemma_9b/seed_45/llm_samples_94_better_spam/samples_3.csv
   (Includes 3 original + 57 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 4
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_45/llm_samples_94_better_spam/samples_4.csv
   Found 80 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 4
   Starting with existing 80 samples
Maximum target samples needed: 80
📁 Using existing dataset for activation extraction
   Debug: final_df has 80 samples, original_df has 4 samples
   Debug: LLM samples (after original) should be 76
📊 Final dataset breakdown:
   - Original samples: 4
   - Total samples in final dataset: 80
   - Generated samples: 76
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 80 samples to: results/spam_gemma_9b/seed_45/llm_samples_94_better_spam/samples_4.csv
   (Includes 4 original + 76 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 5
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_45/llm_samples_94_better_spam/samples_5.csv
   Found 100 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 5
   Starting with existing 100 samples
Maximum target samples needed: 100
📁 Using existing dataset for activation extraction
   Debug: final_df has 100 samples, original_df has 5 samples
   Debug: LLM samples (after original) should be 95
📊 Final dataset breakdown:
   - Original samples: 5
   - Total samples in final dataset: 100
   - Generated samples: 95
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 100 samples to: results/spam_gemma_9b/seed_45/llm_samples_94_better_spam/samples_5.csv
   (Includes 5 original + 95 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 10
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_45/llm_samples_94_better_spam/samples_10.csv
   Found 200 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 10
   Starting with existing 200 samples
Maximum target samples needed: 200
📁 Using existing dataset for activation extraction
   Debug: final_df has 200 samples, original_df has 10 samples
   Debug: LLM samples (after original) should be 190
📊 Final dataset breakdown:
   - Original samples: 10
   - Total samples in final dataset: 200
   - Generated samples: 190
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 200 samples to: results/spam_gemma_9b/seed_45/llm_samples_94_better_spam/samples_10.csv
   (Includes 10 original + 190 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

Seed 45 complete! Files saved to: results/spam_gemma_9b/seed_45/llm_samples_94_better_spam

================================================================================
Processing seed 46
================================================================================
Output directory: results/spam_gemma_9b/seed_46/llm_samples_94_better_spam
Extracting samples using seed 46...
Loading dataset: 94_better_spam
No model or model_name provided, not initializing activation manager.
Total positive samples in dataset: 4000
Extracted 1 samples for n_real_samples=1
Extracted 2 samples for n_real_samples=2
Extracted 3 samples for n_real_samples=3
Extracted 4 samples for n_real_samples=4
Extracted 5 samples for n_real_samples=5
Extracted 10 samples for n_real_samples=10

------------------------------------------------------------
Processing n_real_samples = 1
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_46/llm_samples_94_better_spam/samples_1.csv
   Found 20 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 1
   Starting with existing 20 samples
Maximum target samples needed: 20
📁 Using existing dataset for activation extraction
   Debug: final_df has 20 samples, original_df has 1 samples
   Debug: LLM samples (after original) should be 19
📊 Final dataset breakdown:
   - Original samples: 1
   - Total samples in final dataset: 20
   - Generated samples: 19
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 20 samples to: results/spam_gemma_9b/seed_46/llm_samples_94_better_spam/samples_1.csv
   (Includes 1 original + 19 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 2
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_46/llm_samples_94_better_spam/samples_2.csv
   Found 40 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 2
   Starting with existing 40 samples
Maximum target samples needed: 40
📁 Using existing dataset for activation extraction
   Debug: final_df has 40 samples, original_df has 2 samples
   Debug: LLM samples (after original) should be 38
📊 Final dataset breakdown:
   - Original samples: 2
   - Total samples in final dataset: 40
   - Generated samples: 38
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 40 samples to: results/spam_gemma_9b/seed_46/llm_samples_94_better_spam/samples_2.csv
   (Includes 2 original + 38 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 3
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_46/llm_samples_94_better_spam/samples_3.csv
   Found 60 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 3
   Starting with existing 60 samples
Maximum target samples needed: 60
📁 Using existing dataset for activation extraction
   Debug: final_df has 60 samples, original_df has 3 samples
   Debug: LLM samples (after original) should be 57
📊 Final dataset breakdown:
   - Original samples: 3
   - Total samples in final dataset: 60
   - Generated samples: 57
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 60 samples to: results/spam_gemma_9b/seed_46/llm_samples_94_better_spam/samples_3.csv
   (Includes 3 original + 57 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 4
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_46/llm_samples_94_better_spam/samples_4.csv
   Found 80 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 4
   Starting with existing 80 samples
Maximum target samples needed: 80
📁 Using existing dataset for activation extraction
   Debug: final_df has 80 samples, original_df has 4 samples
   Debug: LLM samples (after original) should be 76
📊 Final dataset breakdown:
   - Original samples: 4
   - Total samples in final dataset: 80
   - Generated samples: 76
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 80 samples to: results/spam_gemma_9b/seed_46/llm_samples_94_better_spam/samples_4.csv
   (Includes 4 original + 76 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 5
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_46/llm_samples_94_better_spam/samples_5.csv
   Found 100 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 5
   Starting with existing 100 samples
Maximum target samples needed: 100
📁 Using existing dataset for activation extraction
   Debug: final_df has 100 samples, original_df has 5 samples
   Debug: LLM samples (after original) should be 95
📊 Final dataset breakdown:
   - Original samples: 5
   - Total samples in final dataset: 100
   - Generated samples: 95
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 100 samples to: results/spam_gemma_9b/seed_46/llm_samples_94_better_spam/samples_5.csv
   (Includes 5 original + 95 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 10
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_46/llm_samples_94_better_spam/samples_10.csv
   Found 200 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 10
   Starting with existing 200 samples
Maximum target samples needed: 200
📁 Using existing dataset for activation extraction
   Debug: final_df has 200 samples, original_df has 10 samples
   Debug: LLM samples (after original) should be 190
📊 Final dataset breakdown:
   - Original samples: 10
   - Total samples in final dataset: 200
   - Generated samples: 190
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 200 samples to: results/spam_gemma_9b/seed_46/llm_samples_94_better_spam/samples_10.csv
   (Includes 10 original + 190 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

Seed 46 complete! Files saved to: results/spam_gemma_9b/seed_46/llm_samples_94_better_spam

================================================================================
Processing seed 47
================================================================================
Output directory: results/spam_gemma_9b/seed_47/llm_samples_94_better_spam
Extracting samples using seed 47...
Loading dataset: 94_better_spam
No model or model_name provided, not initializing activation manager.
Total positive samples in dataset: 4000
Extracted 1 samples for n_real_samples=1
Extracted 2 samples for n_real_samples=2
Extracted 3 samples for n_real_samples=3
Extracted 4 samples for n_real_samples=4
Extracted 5 samples for n_real_samples=5
Extracted 10 samples for n_real_samples=10

------------------------------------------------------------
Processing n_real_samples = 1
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_47/llm_samples_94_better_spam/samples_1.csv
   Found 20 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 1
   Starting with existing 20 samples
Maximum target samples needed: 20
📁 Using existing dataset for activation extraction
   Debug: final_df has 20 samples, original_df has 1 samples
   Debug: LLM samples (after original) should be 19
📊 Final dataset breakdown:
   - Original samples: 1
   - Total samples in final dataset: 20
   - Generated samples: 19
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 20 samples to: results/spam_gemma_9b/seed_47/llm_samples_94_better_spam/samples_1.csv
   (Includes 1 original + 19 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 2
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_47/llm_samples_94_better_spam/samples_2.csv
   Found 40 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 2
   Starting with existing 40 samples
Maximum target samples needed: 40
📁 Using existing dataset for activation extraction
   Debug: final_df has 40 samples, original_df has 2 samples
   Debug: LLM samples (after original) should be 38
📊 Final dataset breakdown:
   - Original samples: 2
   - Total samples in final dataset: 40
   - Generated samples: 38
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 40 samples to: results/spam_gemma_9b/seed_47/llm_samples_94_better_spam/samples_2.csv
   (Includes 2 original + 38 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 3
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_47/llm_samples_94_better_spam/samples_3.csv
   Found 60 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 3
   Starting with existing 60 samples
Maximum target samples needed: 60
📁 Using existing dataset for activation extraction
   Debug: final_df has 60 samples, original_df has 3 samples
   Debug: LLM samples (after original) should be 57
📊 Final dataset breakdown:
   - Original samples: 3
   - Total samples in final dataset: 60
   - Generated samples: 57
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 60 samples to: results/spam_gemma_9b/seed_47/llm_samples_94_better_spam/samples_3.csv
   (Includes 3 original + 57 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 4
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_47/llm_samples_94_better_spam/samples_4.csv
   Found 80 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 4
   Starting with existing 80 samples
Maximum target samples needed: 80
📁 Using existing dataset for activation extraction
   Debug: final_df has 80 samples, original_df has 4 samples
   Debug: LLM samples (after original) should be 76
📊 Final dataset breakdown:
   - Original samples: 4
   - Total samples in final dataset: 80
   - Generated samples: 76
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 80 samples to: results/spam_gemma_9b/seed_47/llm_samples_94_better_spam/samples_4.csv
   (Includes 4 original + 76 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 5
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_47/llm_samples_94_better_spam/samples_5.csv
   Found 100 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 5
   Starting with existing 100 samples
Maximum target samples needed: 100
📁 Using existing dataset for activation extraction
   Debug: final_df has 100 samples, original_df has 5 samples
   Debug: LLM samples (after original) should be 95
📊 Final dataset breakdown:
   - Original samples: 5
   - Total samples in final dataset: 100
   - Generated samples: 95
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 100 samples to: results/spam_gemma_9b/seed_47/llm_samples_94_better_spam/samples_5.csv
   (Includes 5 original + 95 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 10
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_47/llm_samples_94_better_spam/samples_10.csv
   Found 200 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 10
   Starting with existing 200 samples
Maximum target samples needed: 200
📁 Using existing dataset for activation extraction
   Debug: final_df has 200 samples, original_df has 10 samples
   Debug: LLM samples (after original) should be 190
📊 Final dataset breakdown:
   - Original samples: 10
   - Total samples in final dataset: 200
   - Generated samples: 190
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 200 samples to: results/spam_gemma_9b/seed_47/llm_samples_94_better_spam/samples_10.csv
   (Includes 10 original + 190 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

Seed 47 complete! Files saved to: results/spam_gemma_9b/seed_47/llm_samples_94_better_spam

================================================================================
Processing seed 48
================================================================================
Output directory: results/spam_gemma_9b/seed_48/llm_samples_94_better_spam
Extracting samples using seed 48...
Loading dataset: 94_better_spam
No model or model_name provided, not initializing activation manager.
Total positive samples in dataset: 4000
Extracted 1 samples for n_real_samples=1
Extracted 2 samples for n_real_samples=2
Extracted 3 samples for n_real_samples=3
Extracted 4 samples for n_real_samples=4
Extracted 5 samples for n_real_samples=5
Extracted 10 samples for n_real_samples=10

------------------------------------------------------------
Processing n_real_samples = 1
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_48/llm_samples_94_better_spam/samples_1.csv
   Found 20 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 1
   Starting with existing 20 samples
Maximum target samples needed: 20
📁 Using existing dataset for activation extraction
   Debug: final_df has 20 samples, original_df has 1 samples
   Debug: LLM samples (after original) should be 19
📊 Final dataset breakdown:
   - Original samples: 1
   - Total samples in final dataset: 20
   - Generated samples: 19
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 20 samples to: results/spam_gemma_9b/seed_48/llm_samples_94_better_spam/samples_1.csv
   (Includes 1 original + 19 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 2
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_48/llm_samples_94_better_spam/samples_2.csv
   Found 40 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 2
   Starting with existing 40 samples
Maximum target samples needed: 40
📁 Using existing dataset for activation extraction
   Debug: final_df has 40 samples, original_df has 2 samples
   Debug: LLM samples (after original) should be 38
📊 Final dataset breakdown:
   - Original samples: 2
   - Total samples in final dataset: 40
   - Generated samples: 38
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 40 samples to: results/spam_gemma_9b/seed_48/llm_samples_94_better_spam/samples_2.csv
   (Includes 2 original + 38 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 3
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_48/llm_samples_94_better_spam/samples_3.csv
   Found 60 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 3
   Starting with existing 60 samples
Maximum target samples needed: 60
📁 Using existing dataset for activation extraction
   Debug: final_df has 60 samples, original_df has 3 samples
   Debug: LLM samples (after original) should be 57
📊 Final dataset breakdown:
   - Original samples: 3
   - Total samples in final dataset: 60
   - Generated samples: 57
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 60 samples to: results/spam_gemma_9b/seed_48/llm_samples_94_better_spam/samples_3.csv
   (Includes 3 original + 57 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 4
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_48/llm_samples_94_better_spam/samples_4.csv
   Found 80 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 4
   Starting with existing 80 samples
Maximum target samples needed: 80
📁 Using existing dataset for activation extraction
   Debug: final_df has 80 samples, original_df has 4 samples
   Debug: LLM samples (after original) should be 76
📊 Final dataset breakdown:
   - Original samples: 4
   - Total samples in final dataset: 80
   - Generated samples: 76
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 80 samples to: results/spam_gemma_9b/seed_48/llm_samples_94_better_spam/samples_4.csv
   (Includes 4 original + 76 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 5
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_48/llm_samples_94_better_spam/samples_5.csv
   Found 100 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 5
   Starting with existing 100 samples
Maximum target samples needed: 100
📁 Using existing dataset for activation extraction
   Debug: final_df has 100 samples, original_df has 5 samples
   Debug: LLM samples (after original) should be 95
📊 Final dataset breakdown:
   - Original samples: 5
   - Total samples in final dataset: 100
   - Generated samples: 95
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 100 samples to: results/spam_gemma_9b/seed_48/llm_samples_94_better_spam/samples_5.csv
   (Includes 5 original + 95 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 10
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_48/llm_samples_94_better_spam/samples_10.csv
   Found 200 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 10
   Starting with existing 200 samples
Maximum target samples needed: 200
📁 Using existing dataset for activation extraction
   Debug: final_df has 200 samples, original_df has 10 samples
   Debug: LLM samples (after original) should be 190
📊 Final dataset breakdown:
   - Original samples: 10
   - Total samples in final dataset: 200
   - Generated samples: 190
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 200 samples to: results/spam_gemma_9b/seed_48/llm_samples_94_better_spam/samples_10.csv
   (Includes 10 original + 190 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

Seed 48 complete! Files saved to: results/spam_gemma_9b/seed_48/llm_samples_94_better_spam

================================================================================
Processing seed 49
================================================================================
Output directory: results/spam_gemma_9b/seed_49/llm_samples_94_better_spam
Extracting samples using seed 49...
Loading dataset: 94_better_spam
No model or model_name provided, not initializing activation manager.
Total positive samples in dataset: 4000
Extracted 1 samples for n_real_samples=1
Extracted 2 samples for n_real_samples=2
Extracted 3 samples for n_real_samples=3
Extracted 4 samples for n_real_samples=4
Extracted 5 samples for n_real_samples=5
Extracted 10 samples for n_real_samples=10

------------------------------------------------------------
Processing n_real_samples = 1
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_49/llm_samples_94_better_spam/samples_1.csv
   Found 20 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 1
   Starting with existing 20 samples
Maximum target samples needed: 20
📁 Using existing dataset for activation extraction
   Debug: final_df has 20 samples, original_df has 1 samples
   Debug: LLM samples (after original) should be 19
📊 Final dataset breakdown:
   - Original samples: 1
   - Total samples in final dataset: 20
   - Generated samples: 19
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 20 samples to: results/spam_gemma_9b/seed_49/llm_samples_94_better_spam/samples_1.csv
   (Includes 1 original + 19 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 2
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_49/llm_samples_94_better_spam/samples_2.csv
   Found 40 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 2
   Starting with existing 40 samples
Maximum target samples needed: 40
📁 Using existing dataset for activation extraction
   Debug: final_df has 40 samples, original_df has 2 samples
   Debug: LLM samples (after original) should be 38
📊 Final dataset breakdown:
   - Original samples: 2
   - Total samples in final dataset: 40
   - Generated samples: 38
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 40 samples to: results/spam_gemma_9b/seed_49/llm_samples_94_better_spam/samples_2.csv
   (Includes 2 original + 38 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 3
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_49/llm_samples_94_better_spam/samples_3.csv
   Found 60 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 3
   Starting with existing 60 samples
Maximum target samples needed: 60
📁 Using existing dataset for activation extraction
   Debug: final_df has 60 samples, original_df has 3 samples
   Debug: LLM samples (after original) should be 57
📊 Final dataset breakdown:
   - Original samples: 3
   - Total samples in final dataset: 60
   - Generated samples: 57
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 60 samples to: results/spam_gemma_9b/seed_49/llm_samples_94_better_spam/samples_3.csv
   (Includes 3 original + 57 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 4
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_49/llm_samples_94_better_spam/samples_4.csv
   Found 80 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 4
   Starting with existing 80 samples
Maximum target samples needed: 80
📁 Using existing dataset for activation extraction
   Debug: final_df has 80 samples, original_df has 4 samples
   Debug: LLM samples (after original) should be 76
📊 Final dataset breakdown:
   - Original samples: 4
   - Total samples in final dataset: 80
   - Generated samples: 76
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 80 samples to: results/spam_gemma_9b/seed_49/llm_samples_94_better_spam/samples_4.csv
   (Includes 4 original + 76 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 5
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_49/llm_samples_94_better_spam/samples_5.csv
   Found 100 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 5
   Starting with existing 100 samples
Maximum target samples needed: 100
📁 Using existing dataset for activation extraction
   Debug: final_df has 100 samples, original_df has 5 samples
   Debug: LLM samples (after original) should be 95
📊 Final dataset breakdown:
   - Original samples: 5
   - Total samples in final dataset: 100
   - Generated samples: 95
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 100 samples to: results/spam_gemma_9b/seed_49/llm_samples_94_better_spam/samples_5.csv
   (Includes 5 original + 95 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 10
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_49/llm_samples_94_better_spam/samples_10.csv
   Found 200 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 10
   Starting with existing 200 samples
Maximum target samples needed: 200
📁 Using existing dataset for activation extraction
   Debug: final_df has 200 samples, original_df has 10 samples
   Debug: LLM samples (after original) should be 190
📊 Final dataset breakdown:
   - Original samples: 10
   - Total samples in final dataset: 200
   - Generated samples: 190
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 200 samples to: results/spam_gemma_9b/seed_49/llm_samples_94_better_spam/samples_10.csv
   (Includes 10 original + 190 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

Seed 49 complete! Files saved to: results/spam_gemma_9b/seed_49/llm_samples_94_better_spam

================================================================================
Processing seed 50
================================================================================
Output directory: results/spam_gemma_9b/seed_50/llm_samples_94_better_spam
Extracting samples using seed 50...
Loading dataset: 94_better_spam
No model or model_name provided, not initializing activation manager.
Total positive samples in dataset: 4000
Extracted 1 samples for n_real_samples=1
Extracted 2 samples for n_real_samples=2
Extracted 3 samples for n_real_samples=3
Extracted 4 samples for n_real_samples=4
Extracted 5 samples for n_real_samples=5
Extracted 10 samples for n_real_samples=10

------------------------------------------------------------
Processing n_real_samples = 1
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_50/llm_samples_94_better_spam/samples_1.csv
   Found 20 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 1
   Starting with existing 20 samples
Maximum target samples needed: 20
📁 Using existing dataset for activation extraction
   Debug: final_df has 20 samples, original_df has 1 samples
   Debug: LLM samples (after original) should be 19
📊 Final dataset breakdown:
   - Original samples: 1
   - Total samples in final dataset: 20
   - Generated samples: 19
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 20 samples to: results/spam_gemma_9b/seed_50/llm_samples_94_better_spam/samples_1.csv
   (Includes 1 original + 19 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 2
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_50/llm_samples_94_better_spam/samples_2.csv
   Found 40 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 2
   Starting with existing 40 samples
Maximum target samples needed: 40
📁 Using existing dataset for activation extraction
   Debug: final_df has 40 samples, original_df has 2 samples
   Debug: LLM samples (after original) should be 38
📊 Final dataset breakdown:
   - Original samples: 2
   - Total samples in final dataset: 40
   - Generated samples: 38
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 40 samples to: results/spam_gemma_9b/seed_50/llm_samples_94_better_spam/samples_2.csv
   (Includes 2 original + 38 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 3
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_50/llm_samples_94_better_spam/samples_3.csv
   Found 60 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 3
   Starting with existing 60 samples
Maximum target samples needed: 60
📁 Using existing dataset for activation extraction
   Debug: final_df has 60 samples, original_df has 3 samples
   Debug: LLM samples (after original) should be 57
📊 Final dataset breakdown:
   - Original samples: 3
   - Total samples in final dataset: 60
   - Generated samples: 57
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 60 samples to: results/spam_gemma_9b/seed_50/llm_samples_94_better_spam/samples_3.csv
   (Includes 3 original + 57 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 4
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_50/llm_samples_94_better_spam/samples_4.csv
   Found 80 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 4
   Starting with existing 80 samples
Maximum target samples needed: 80
📁 Using existing dataset for activation extraction
   Debug: final_df has 80 samples, original_df has 4 samples
   Debug: LLM samples (after original) should be 76
📊 Final dataset breakdown:
   - Original samples: 4
   - Total samples in final dataset: 80
   - Generated samples: 76
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 80 samples to: results/spam_gemma_9b/seed_50/llm_samples_94_better_spam/samples_4.csv
   (Includes 4 original + 76 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 5
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_50/llm_samples_94_better_spam/samples_5.csv
   Found 100 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 5
   Starting with existing 100 samples
Maximum target samples needed: 100
📁 Using existing dataset for activation extraction
   Debug: final_df has 100 samples, original_df has 5 samples
   Debug: LLM samples (after original) should be 95
📊 Final dataset breakdown:
   - Original samples: 5
   - Total samples in final dataset: 100
   - Generated samples: 95
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 100 samples to: results/spam_gemma_9b/seed_50/llm_samples_94_better_spam/samples_5.csv
   (Includes 5 original + 95 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 10
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_50/llm_samples_94_better_spam/samples_10.csv
   Found 200 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 10
   Starting with existing 200 samples
Maximum target samples needed: 200
📁 Using existing dataset for activation extraction
   Debug: final_df has 200 samples, original_df has 10 samples
   Debug: LLM samples (after original) should be 190
📊 Final dataset breakdown:
   - Original samples: 10
   - Total samples in final dataset: 200
   - Generated samples: 190
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 200 samples to: results/spam_gemma_9b/seed_50/llm_samples_94_better_spam/samples_10.csv
   (Includes 10 original + 190 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

Seed 50 complete! Files saved to: results/spam_gemma_9b/seed_50/llm_samples_94_better_spam

================================================================================
Processing seed 51
================================================================================
Output directory: results/spam_gemma_9b/seed_51/llm_samples_94_better_spam
Extracting samples using seed 51...
Loading dataset: 94_better_spam
No model or model_name provided, not initializing activation manager.
Total positive samples in dataset: 4000
Extracted 1 samples for n_real_samples=1
Extracted 2 samples for n_real_samples=2
Extracted 3 samples for n_real_samples=3
Extracted 4 samples for n_real_samples=4
Extracted 5 samples for n_real_samples=5
Extracted 10 samples for n_real_samples=10

------------------------------------------------------------
Processing n_real_samples = 1
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_51/llm_samples_94_better_spam/samples_1.csv
   Found 20 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 1
   Starting with existing 20 samples
Maximum target samples needed: 20
📁 Using existing dataset for activation extraction
   Debug: final_df has 20 samples, original_df has 1 samples
   Debug: LLM samples (after original) should be 19
📊 Final dataset breakdown:
   - Original samples: 1
   - Total samples in final dataset: 20
   - Generated samples: 19
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 20 samples to: results/spam_gemma_9b/seed_51/llm_samples_94_better_spam/samples_1.csv
   (Includes 1 original + 19 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 2
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_51/llm_samples_94_better_spam/samples_2.csv
   Found 40 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 2
   Starting with existing 40 samples
Maximum target samples needed: 40
📁 Using existing dataset for activation extraction
   Debug: final_df has 40 samples, original_df has 2 samples
   Debug: LLM samples (after original) should be 38
📊 Final dataset breakdown:
   - Original samples: 2
   - Total samples in final dataset: 40
   - Generated samples: 38
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 40 samples to: results/spam_gemma_9b/seed_51/llm_samples_94_better_spam/samples_2.csv
   (Includes 2 original + 38 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 3
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_51/llm_samples_94_better_spam/samples_3.csv
   Found 60 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 3
   Starting with existing 60 samples
Maximum target samples needed: 60
📁 Using existing dataset for activation extraction
   Debug: final_df has 60 samples, original_df has 3 samples
   Debug: LLM samples (after original) should be 57
📊 Final dataset breakdown:
   - Original samples: 3
   - Total samples in final dataset: 60
   - Generated samples: 57
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 60 samples to: results/spam_gemma_9b/seed_51/llm_samples_94_better_spam/samples_3.csv
   (Includes 3 original + 57 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 4
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_51/llm_samples_94_better_spam/samples_4.csv
   Found 80 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 4
   Starting with existing 80 samples
Maximum target samples needed: 80
📁 Using existing dataset for activation extraction
   Debug: final_df has 80 samples, original_df has 4 samples
   Debug: LLM samples (after original) should be 76
📊 Final dataset breakdown:
   - Original samples: 4
   - Total samples in final dataset: 80
   - Generated samples: 76
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 80 samples to: results/spam_gemma_9b/seed_51/llm_samples_94_better_spam/samples_4.csv
   (Includes 4 original + 76 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 5
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_51/llm_samples_94_better_spam/samples_5.csv
   Found 100 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 5
   Starting with existing 100 samples
Maximum target samples needed: 100
📁 Using existing dataset for activation extraction
   Debug: final_df has 100 samples, original_df has 5 samples
   Debug: LLM samples (after original) should be 95
📊 Final dataset breakdown:
   - Original samples: 5
   - Total samples in final dataset: 100
   - Generated samples: 95
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 100 samples to: results/spam_gemma_9b/seed_51/llm_samples_94_better_spam/samples_5.csv
   (Includes 5 original + 95 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 10
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_51/llm_samples_94_better_spam/samples_10.csv
   Found 200 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 10
   Starting with existing 200 samples
Maximum target samples needed: 200
📁 Using existing dataset for activation extraction
   Debug: final_df has 200 samples, original_df has 10 samples
   Debug: LLM samples (after original) should be 190
📊 Final dataset breakdown:
   - Original samples: 10
   - Total samples in final dataset: 200
   - Generated samples: 190
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 200 samples to: results/spam_gemma_9b/seed_51/llm_samples_94_better_spam/samples_10.csv
   (Includes 10 original + 190 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

Seed 51 complete! Files saved to: results/spam_gemma_9b/seed_51/llm_samples_94_better_spam

================================================================================
LLM upsampling complete for all seeds!
Results saved to: results/spam_gemma_9b/seed_*/llm_samples/
Upsampling factors processed: [1, 2, 3, 4, 5, 10, 20]
================================================================================
=== LLM upsampling complete ===
Loading model 'google/gemma-2-9b' for activation extraction...
Loading checkpoint shards:   0%|                                                                                  | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█████████▎                                                                | 1/8 [00:00<00:03,  1.78it/s]Loading checkpoint shards:  25%|██████████████████▌                                                       | 2/8 [00:01<00:03,  1.79it/s]Loading checkpoint shards:  38%|███████████████████████████▊                                              | 3/8 [00:01<00:02,  1.78it/s]Loading checkpoint shards:  50%|█████████████████████████████████████                                     | 4/8 [00:02<00:02,  1.78it/s]Loading checkpoint shards:  62%|██████████████████████████████████████████████▎                           | 5/8 [00:02<00:01,  1.78it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████▌                  | 6/8 [00:03<00:01,  1.78it/s]Loading checkpoint shards:  88%|████████████████████████████████████████████████████████████████▊         | 7/8 [00:03<00:00,  1.77it/s]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 8/8 [00:04<00:00,  2.13it/s]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 8/8 [00:04<00:00,  1.90it/s]

========================= ACTIVATION EXTRACTION PHASE =========================
  - Extracting activations for 87_is_spam, L20, resid_post (on_policy=False)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: Gemma2ForCausalLM(
  (model): Gemma2Model(
    (embed_tokens): Embedding(256000, 3584, padding_idx=0)
    (layers): ModuleList(
      (0-41): 42 x Gemma2DecoderLayer(
        (self_attn): Gemma2Attention(
          (q_proj): Linear(in_features=3584, out_features=4096, bias=False)
          (k_proj): Linear(in_features=3584, out_features=2048, bias=False)
          (v_proj): Linear(in_features=3584, out_features=2048, bias=False)
          (o_proj): Linear(in_features=4096, out_features=3584, bias=False)
        )
        (mlp): Gemma2MLP(
          (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)
          (up_proj): Linear(in_features=3584, out_features=14336, bias=False)
          (down_proj): Linear(in_features=14336, out_features=3584, bias=False)
          (act_fn): PytorchGELUTanh()
        )
        (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)
        (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)
        (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)
        (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)
      )
    )
    (norm): Gemma2RMSNorm((3584,), eps=1e-06)
    (rotary_emb): Gemma2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3584, out_features=256000, bias=False)
)
[DEBUG] model type: <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'>
Successfully initialized ActivationManager
    - Off-policy: extracting activations from prompts...
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 5157
[DEBUG] Processing 5157 full activations
[DEBUG] Estimated memory requirement: 1.63 GB
[WARNING] Large memory requirement (1.63 GB).
[DEBUG] Max sequence length: 237
[DEBUG] Shape of first activation: (1, 27, 3584)
[DEBUG] Final array shape: (5157, 237, 3584)
[DEBUG] Returning (5157, 237, 3584) activations
    - Cached 0 new activations (dataset)
    - Aggregated activations are up-to-date
  - Extracting activations for 94_better_spam, L20, resid_post (on_policy=False)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: Gemma2ForCausalLM(
  (model): Gemma2Model(
    (embed_tokens): Embedding(256000, 3584, padding_idx=0)
    (layers): ModuleList(
      (0-41): 42 x Gemma2DecoderLayer(
        (self_attn): Gemma2Attention(
          (q_proj): Linear(in_features=3584, out_features=4096, bias=False)
          (k_proj): Linear(in_features=3584, out_features=2048, bias=False)
          (v_proj): Linear(in_features=3584, out_features=2048, bias=False)
          (o_proj): Linear(in_features=4096, out_features=3584, bias=False)
        )
        (mlp): Gemma2MLP(
          (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)
          (up_proj): Linear(in_features=3584, out_features=14336, bias=False)
          (down_proj): Linear(in_features=14336, out_features=3584, bias=False)
          (act_fn): PytorchGELUTanh()
        )
        (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)
        (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)
        (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)
        (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)
      )
    )
    (norm): Gemma2RMSNorm((3584,), eps=1e-06)
    (rotary_emb): Gemma2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3584, out_features=256000, bias=False)
)
[DEBUG] model type: <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'>
Successfully initialized ActivationManager
    - Off-policy: extracting activations from prompts...
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 8000 full activations
[DEBUG] Estimated memory requirement: 19.66 GB
[WARNING] Large memory requirement (19.66 GB).
[DEBUG] Max sequence length: 1167
[DEBUG] Shape of first activation: (1, 47, 3584)
[DEBUG] Final array shape: (8000, 1167, 3584)
[DEBUG] Returning (8000, 1167, 3584) activations
    - Cached 0 new activations (dataset)
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 5000 full activations
[DEBUG] Estimated memory requirement: 5.41 GB
[WARNING] Large memory requirement (5.41 GB).
[DEBUG] Max sequence length: 935
[DEBUG] Shape of first activation: (1, 252, 3584)
[DEBUG] Final array shape: (5000, 935, 3584)
[DEBUG] Returning (5000, 935, 3584) activations
    - Cached 0 new activations (LLM)
    - Aggregated activations are up-to-date

========================= MODEL UNLOADING PHASE =========================
Unloading model to free GPU memory for parallel processing...
Model unloaded successfully

========================= JOB CREATION PHASE =========================
Created 6300 probe jobs

========================= PROBE PROCESSING PHASE (BATCHED) =========================
Processing 6300 probe jobs in batched groups...
Running 6300 jobs in 700 groups (batched)

=== Group 1/700 — ('94_better_spam', 20, 'resid_post', 42, '__none__', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
Split data: 2682 train, 358 val, 537 test
[DEBUG] Class distribution — train: {0: 2146, 1: 536}, val: {0: 287, 1: 71}, test: {0: 430, 1: 107}
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Split data (only_test): 0 train, 0 val, 5157 test (non-balanced)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: attention
  - [SKIP] Probe already trained: train_on_94_better_spam_attention_L20_resid_post_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set

=== Group 2/700 — ('94_better_spam', 20, 'resid_post', 43, '__none__', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
Split data: 2682 train, 358 val, 537 test
[DEBUG] Class distribution — train: {0: 2146, 1: 536}, val: {0: 287, 1: 71}, test: {0: 430, 1: 107}
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Split data (only_test): 0 train, 0 val, 5157 test (non-balanced)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: attention
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: full
Loaded probe from results/spam_gemma_9b/seed_42/1-spam-pred-auc/trained/train_on_94_better_spam_attention_L20_resid_post_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 537 full activations
[DEBUG] Estimated memory requirement: 1.26 GB
[WARNING] Large memory requirement (1.26 GB).
[DEBUG] Max sequence length: 772
[DEBUG] Shape of first activation: (1, 27, 3584)
[DEBUG] Final array shape: (537, 772, 3584)
[DEBUG] Returning (537, 772, 3584) activations
[DEBUG] test activations shape: (537, 772, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (537, 772, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating validation on '94_better_spam': Got unsupported ScalarType BFloat16
    [BATCH] Evaluating on 94_better_spam test set
Loaded probe from results/spam_gemma_9b/seed_42/1-spam-pred-auc/trained/train_on_94_better_spam_attention_L20_resid_post_state.npz
Converted attention probe back to bfloat16 for training
Test activations: (537, 772, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating test on '94_better_spam': Got unsupported ScalarType BFloat16
Loaded probe from results/spam_gemma_9b/seed_42/1-spam-pred-auc/trained/train_on_94_better_spam_attention_L20_resid_post_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 5157
[DEBUG] Processing 5157 full activations
[DEBUG] Estimated memory requirement: 1.63 GB
[WARNING] Large memory requirement (1.63 GB).
[DEBUG] Max sequence length: 237
[DEBUG] Shape of first activation: (1, 27, 3584)
[DEBUG] Final array shape: (5157, 237, 3584)
[DEBUG] Returning (5157, 237, 3584) activations
[DEBUG] test activations shape: (5157, 237, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.500000]
Test activations: (5157, 237, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating on '87_is_spam': Got unsupported ScalarType BFloat16

=== Group 3/700 — ('94_better_spam', 20, 'resid_post', 44, '__none__', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
Split data: 2682 train, 358 val, 537 test
[DEBUG] Class distribution — train: {0: 2146, 1: 536}, val: {0: 287, 1: 71}, test: {0: 430, 1: 107}
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Split data (only_test): 0 train, 0 val, 5157 test (non-balanced)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (2682, 3584)
[DEBUG] Returning (2682, 3584) activations
[DEBUG] train activations shape: (2682, 3584), dtype: float16
[DEBUG] train activations range: [-1337.000000, 237.500000]
Train activations: (2682, 3584)
[DEBUG] Pre-fit sample counts — X: 2682, y: 2682
[DEBUG] y_train class distribution: {0: 2146, 1: 536}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_softmax
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 2682 full activations
[DEBUG] Estimated memory requirement: 6.07 GB
[WARNING] Large memory requirement (6.07 GB).
[DEBUG] Max sequence length: 1167
[DEBUG] Shape of first activation: (1, 18, 3584)
[DEBUG] Final array shape: (2682, 1167, 3584)
[DEBUG] Returning (2682, 1167, 3584) activations
[DEBUG] train activations shape: (2682, 1167, 3584), dtype: float16
[DEBUG] train activations range: [-1674.000000, 237.500000]
Train activations: (2682, 1167, 3584)
[DEBUG] Pre-fit sample counts — X: 2682, y: 2682
[DEBUG] y_train class distribution: {0: 2146, 1: 536}
  [DEBUG] Creating probe for architecture: attention
  [DEBUG] Creating attention probe

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (2682, 3584)
Input y shape: (2682,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.74GB, GPU_allocated: 0.03GB, GPU_reserved: 38.32GB
[DEBUG] Input activations shape: (2682, 3584)
[DEBUG] Using pre-aggregated inputs: (2682, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 3 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/3 [00:00<?, ?it/s]Encoding activations:  67%|████████████████████████████████████████████████████▋                          | 2/3 [00:00<00:00, 13.39it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 19.23it/s]
[DEBUG] Final encoded shape: (2682, 262144)
Encoded feature matrix shape: (2682, 262144)
[DEBUG] After encoding. Memory: RAM: 12.40GB, GPU_allocated: 7.03GB, GPU_reserved: 38.32GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(2682, 262144), y_train=(2682,)
[DEBUG] Memory at start of feature selection: RAM: 12.40GB, GPU_allocated: 7.03GB, GPU_reserved: 38.32GB
[DEBUG] Feature matrix for selection shape: (2682, 262144)
[DEBUG] Memory after feature matrix build: RAM: 12.40GB, GPU_allocated: 7.03GB, GPU_reserved: 38.32GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 12.40GB, GPU_allocated: 7.03GB, GPU_reserved: 38.32GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 12.40GB, GPU_allocated: 7.03GB, GPU_reserved: 38.32GB
Selected 3584 features
[DEBUG] X_selected shape: (2682, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  [DEBUG] Fitting attention probe normally...
Epoch 10/100, Loss: 0.2785
Epoch 20/100, Loss: 0.1113
Epoch 30/100, Loss: 0.0562
Epoch 40/100, Loss: 0.0336
Epoch 50/100, Loss: 0.0219
Epoch 60/100, Loss: 0.0178
Epoch 70/100, Loss: 0.0136
Early stopping at epoch 78
Saved probe to results/spam_gemma_9b/seed_43/1-spam-pred-auc/trained/train_on_94_better_spam_attention_L20_resid_post_state.npz
Saved training log to results/spam_gemma_9b/seed_43/1-spam-pred-auc/trained/train_on_94_better_spam_attention_L20_resid_post_state_train_log.json
  - 🔥 Probe state saved to train_on_94_better_spam_attention_L20_resid_post_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (537, 3584)
[DEBUG] Returning (537, 3584) activations
[DEBUG] test activations shape: (537, 3584), dtype: float16
[DEBUG] test activations range: [-1377.000000, 237.000000]
Test activations: (537, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.01it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.95it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.73it/s]
[DEBUG] Final encoded shape: (537, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9906890130353817, 'auc': 0.9998261247554879, 'precision': 0.9553571428571429, 'recall': 1.0, 'fpr': 0.011627906976744186}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (537, 3584)
  - 🤗 Calculating metrics...
Loaded probe from results/spam_gemma_9b/seed_43/1-spam-pred-auc/trained/train_on_94_better_spam_attention_L20_resid_post_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 537 full activations
[DEBUG] Estimated memory requirement: 1.23 GB
[WARNING] Large memory requirement (1.23 GB).
[DEBUG] Max sequence length: 708
[DEBUG] Shape of first activation: (1, 286, 3584)
[DEBUG] Final array shape: (537, 708, 3584)
[DEBUG] Returning (537, 708, 3584) activations
[DEBUG] test activations shape: (537, 708, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (537, 708, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating validation on '94_better_spam': Got unsupported ScalarType BFloat16
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.04it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.89it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.78it/s]
[DEBUG] Final encoded shape: (537, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9906890130353817, 'auc': 0.9998261247554879, 'precision': 0.9553571428571429, 'recall': 1.0, 'fpr': 0.011627906976744186}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (5157, 3584)
[DEBUG] Returning (5157, 3584) activations
[DEBUG] test activations shape: (5157, 3584), dtype: float16
[DEBUG] test activations range: [-1371.000000, 237.500000]
Test activations: (5157, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 16.06it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.09it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 20.82it/s]
Loaded probe from results/spam_gemma_9b/seed_43/1-spam-pred-auc/trained/train_on_94_better_spam_attention_L20_resid_post_state.npz
Converted attention probe back to bfloat16 for training
Test activations: (537, 708, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating test on '94_better_spam': Got unsupported ScalarType BFloat16
[DEBUG] Final encoded shape: (5157, 262144)
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 17.97it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.92it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 22.04it/s]
[DEBUG] Final encoded shape: (5157, 262144)
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 17.97it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.92it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 22.04it/s]
[DEBUG] Final encoded shape: (5157, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8927671126624006, 'auc': 0.9520197902690244, 'precision': 0.5433070866141733, 'recall': 0.8611544461778471, 'fpr': 0.10274579273693533}
  🚀 [BATCH] Training probe: attention
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: full
Loaded probe from results/spam_gemma_9b/seed_43/1-spam-pred-auc/trained/train_on_94_better_spam_attention_L20_resid_post_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 5157
[DEBUG] Processing 5157 full activations
[DEBUG] Estimated memory requirement: 1.63 GB
[WARNING] Large memory requirement (1.63 GB).
[DEBUG] Max sequence length: 237
[DEBUG] Shape of first activation: (1, 27, 3584)
[DEBUG] Final array shape: (5157, 237, 3584)
[DEBUG] Returning (5157, 237, 3584) activations
[DEBUG] test activations shape: (5157, 237, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.500000]
Test activations: (5157, 237, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating on '87_is_spam': Got unsupported ScalarType BFloat16

=== Group 4/700 — ('94_better_spam', 20, 'resid_post', 45, '__none__', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
Split data: 2682 train, 358 val, 537 test
[DEBUG] Class distribution — train: {0: 2146, 1: 536}, val: {0: 287, 1: 71}, test: {0: 430, 1: 107}
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Split data (only_test): 0 train, 0 val, 5157 test (non-balanced)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (537, 3584)
[DEBUG] Returning (537, 3584) activations
[DEBUG] test activations shape: (537, 3584), dtype: float16
[DEBUG] test activations range: [-124.375000, 235.875000]
Test activations: (537, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 16.65it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.93it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.13it/s]
[DEBUG] Final encoded shape: (537, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9813780260707635, 'auc': 0.9993262334275157, 'precision': 0.9145299145299145, 'recall': 1.0, 'fpr': 0.023255813953488372}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (537, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.71it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.19it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.06it/s]
[DEBUG] Final encoded shape: (537, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9813780260707635, 'auc': 0.9993262334275157, 'precision': 0.9145299145299145, 'recall': 1.0, 'fpr': 0.023255813953488372}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (5157, 3584)
[DEBUG] Returning (5157, 3584) activations
[DEBUG] test activations shape: (5157, 3584), dtype: float16
[DEBUG] test activations range: [-1371.000000, 237.500000]
Test activations: (5157, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 15.96it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.04it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 20.77it/s]
[DEBUG] Final encoded shape: (5157, 262144)
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 17.90it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.92it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 22.03it/s]
[DEBUG] Final encoded shape: (5157, 262144)
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 17.87it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.90it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 22.01it/s]
[DEBUG] Final encoded shape: (5157, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8952879581151832, 'auc': 0.9517582829088186, 'precision': 0.5499505440158259, 'recall': 0.8673946957878315, 'fpr': 0.10075287865367583}
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 2682 full activations
[DEBUG] Estimated memory requirement: 6.05 GB
[WARNING] Large memory requirement (6.05 GB).
[DEBUG] Max sequence length: 1167
[DEBUG] Shape of first activation: (1, 163, 3584)
[DEBUG] Final array shape: (2682, 1167, 3584)
[DEBUG] Returning (2682, 1167, 3584) activations
[DEBUG] train activations shape: (2682, 1167, 3584), dtype: float16
[DEBUG] train activations range: [-1674.000000, 237.500000]
Train activations: (2682, 1167, 3584)
[DEBUG] Pre-fit sample counts — X: 2682, y: 2682
[DEBUG] y_train class distribution: {0: 2146, 1: 536}
  [DEBUG] Creating probe for architecture: attention
  [DEBUG] Creating attention probe
  [DEBUG] Fitting attention probe normally...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (2682, 3584)
[DEBUG] Returning (2682, 3584) activations
[DEBUG] train activations shape: (2682, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 231.875000]
Train activations: (2682, 3584)
[DEBUG] Pre-fit sample counts — X: 2682, y: 2682
[DEBUG] y_train class distribution: {0: 2146, 1: 536}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (2682, 3584)
Input y shape: (2682,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 7.04GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (2682, 3584)
[DEBUG] Using pre-aggregated inputs: (2682, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 3 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/3 [00:00<?, ?it/s]Encoding activations:  33%|██████████████████████████▎                                                    | 1/3 [00:00<00:00,  6.98it/s]Encoding activations:  67%|████████████████████████████████████████████████████▋                          | 2/3 [00:00<00:00,  8.06it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 11.11it/s]
Epoch 10/100, Loss: 0.2961
Epoch 20/100, Loss: 0.1104
Epoch 30/100, Loss: 0.0491
Epoch 40/100, Loss: 0.0375
Epoch 50/100, Loss: 0.0268
Epoch 60/100, Loss: 0.0161
Early stopping at epoch 63
Saved probe to results/spam_gemma_9b/seed_44/1-spam-pred-auc/trained/train_on_94_better_spam_attention_L20_resid_post_state.npz
Saved training log to results/spam_gemma_9b/seed_44/1-spam-pred-auc/trained/train_on_94_better_spam_attention_L20_resid_post_state_train_log.json
  - 🔥 Probe state saved to train_on_94_better_spam_attention_L20_resid_post_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Final encoded shape: (2682, 262144)
Encoded feature matrix shape: (2682, 262144)
[DEBUG] After encoding. Memory: RAM: 9.66GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(2682, 262144), y_train=(2682,)
[DEBUG] Memory at start of feature selection: RAM: 9.66GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (2682, 262144)
[DEBUG] Memory after feature matrix build: RAM: 9.66GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 9.66GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 9.66GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (2682, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (537, 3584)
[DEBUG] Returning (537, 3584) activations
[DEBUG] test activations shape: (537, 3584), dtype: float16
[DEBUG] test activations range: [-237.375000, 92.687500]
Test activations: (537, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.69it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.97it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.03it/s]
[DEBUG] Final encoded shape: (537, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9553072625698324, 'auc': 0.9945011953923061, 'precision': 0.8373983739837398, 'recall': 0.9626168224299065, 'fpr': 0.046511627906976744}
    [BATCH] Evaluating on 94_better_spam test set
Loaded probe from results/spam_gemma_9b/seed_44/1-spam-pred-auc/trained/train_on_94_better_spam_attention_L20_resid_post_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 537 full activations
[DEBUG] Estimated memory requirement: 1.29 GB
[WARNING] Large memory requirement (1.29 GB).
[DEBUG] Max sequence length: 935
[DEBUG] Shape of first activation: (1, 77, 3584)
[DEBUG] Final array shape: (537, 935, 3584)
[DEBUG] Returning (537, 935, 3584) activations
[DEBUG] test activations shape: (537, 935, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (537, 935, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating validation on '94_better_spam': Got unsupported ScalarType BFloat16
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (537, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.62it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.85it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.14it/s]
Loaded probe from results/spam_gemma_9b/seed_44/1-spam-pred-auc/trained/train_on_94_better_spam_attention_L20_resid_post_state.npz
Converted attention probe back to bfloat16 for training
Test activations: (537, 935, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Final encoded shape: (537, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9553072625698324, 'auc': 0.9945011953923061, 'precision': 0.8373983739837398, 'recall': 0.9626168224299065, 'fpr': 0.046511627906976744}
    💀💀💀 ERROR evaluating test on '94_better_spam': Got unsupported ScalarType BFloat16
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (5157, 3584)
[DEBUG] Returning (5157, 3584) activations
[DEBUG] test activations shape: (5157, 3584), dtype: float16
[DEBUG] test activations range: [-1371.000000, 229.375000]
Test activations: (5157, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 15.98it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.06it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 20.79it/s]
[DEBUG] Final encoded shape: (5157, 262144)
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 17.86it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.88it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 21.98it/s]
[DEBUG] Final encoded shape: (5157, 262144)
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 17.90it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.91it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 22.02it/s]
[DEBUG] Final encoded shape: (5157, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8524335854178786, 'auc': 0.8661510676547522, 'precision': 0.4353448275862069, 'recall': 0.6302652106084243, 'fpr': 0.11603188662533215}
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (2682, 3584)
[DEBUG] Returning (2682, 3584) activations
[DEBUG] train activations shape: (2682, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.500000]
Train activations: (2682, 3584)
[DEBUG] Pre-fit sample counts — X: 2682, y: 2682
[DEBUG] y_train class distribution: {0: 2146, 1: 536}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (2682, 3584)
Input y shape: (2682,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 7.04GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (2682, 3584)
[DEBUG] Using pre-aggregated inputs: (2682, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 3 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/3 [00:00<?, ?it/s]Encoding activations:  67%|████████████████████████████████████████████████████▋                          | 2/3 [00:00<00:00, 15.98it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 22.75it/s]
Loaded probe from results/spam_gemma_9b/seed_44/1-spam-pred-auc/trained/train_on_94_better_spam_attention_L20_resid_post_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 5157
[DEBUG] Processing 5157 full activations
[DEBUG] Estimated memory requirement: 1.63 GB
[WARNING] Large memory requirement (1.63 GB).
[DEBUG] Max sequence length: 237
[DEBUG] Shape of first activation: (1, 27, 3584)
[DEBUG] Final array shape: (5157, 237, 3584)
[DEBUG] Returning (5157, 237, 3584) activations
[DEBUG] test activations shape: (5157, 237, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.500000]
Test activations: (5157, 237, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Final encoded shape: (2682, 262144)
Encoded feature matrix shape: (2682, 262144)
[DEBUG] After encoding. Memory: RAM: 9.66GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(2682, 262144), y_train=(2682,)
[DEBUG] Memory at start of feature selection: RAM: 9.66GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (2682, 262144)
[DEBUG] Memory after feature matrix build: RAM: 9.66GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 9.66GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 9.66GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (2682, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
    💀💀💀 ERROR evaluating on '87_is_spam': Got unsupported ScalarType BFloat16

=== Group 5/700 — ('94_better_spam', 20, 'resid_post', 46, '__none__', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
Split data: 2682 train, 358 val, 537 test
[DEBUG] Class distribution — train: {0: 2146, 1: 536}, val: {0: 287, 1: 71}, test: {0: 430, 1: 107}
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Split data (only_test): 0 train, 0 val, 5157 test (non-balanced)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (537, 3584)
[DEBUG] Returning (537, 3584) activations
[DEBUG] test activations shape: (537, 3584), dtype: float16
[DEBUG] test activations range: [-124.375000, 235.875000]
Test activations: (537, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (537, 3584)
[DEBUG] Returning (537, 3584) activations
[DEBUG] test activations shape: (537, 3584), dtype: float16
[DEBUG] test activations range: [-115.000000, 237.000000]
Test activations: (537, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.66it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.92it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.17it/s]
[DEBUG] Final encoded shape: (537, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9832402234636871, 'auc': 0.9993262334275158, 'precision': 0.9224137931034483, 'recall': 1.0, 'fpr': 0.020930232558139535}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 26.82it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.94it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.74it/s]
[DEBUG] Final encoded shape: (537, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9851024208566108, 'auc': 0.9993914366442078, 'precision': 0.9380530973451328, 'recall': 0.9906542056074766, 'fpr': 0.01627906976744186}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (537, 3584)
  - 🤗 Calculating metrics...
Test activations: (537, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.63it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.26it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.05it/s]
[DEBUG] Final encoded shape: (537, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9832402234636871, 'auc': 0.9993262334275158, 'precision': 0.9224137931034483, 'recall': 1.0, 'fpr': 0.020930232558139535}
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.94it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.76it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.66it/s]
[DEBUG] Final encoded shape: (537, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9851024208566108, 'auc': 0.9993914366442078, 'precision': 0.9380530973451328, 'recall': 0.9906542056074766, 'fpr': 0.01627906976744186}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (5157, 3584)
[DEBUG] Returning (5157, 3584) activations
[DEBUG] test activations shape: (5157, 3584), dtype: float16
[DEBUG] test activations range: [-1371.000000, 237.500000]
Test activations: (5157, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (5157, 3584)
[DEBUG] Returning (5157, 3584) activations
[DEBUG] test activations shape: (5157, 3584), dtype: float16
[DEBUG] test activations range: [-1371.000000, 237.500000]
Test activations: (5157, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 15.97it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.05it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 20.78it/s]
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 16.02it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.07it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 20.80it/s]
[DEBUG] Final encoded shape: (5157, 262144)
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 17.86it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.88it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 21.98it/s]
[DEBUG] Final encoded shape: (5157, 262144)
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 17.98it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.93it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 22.06it/s]
[DEBUG] Final encoded shape: (5157, 262144)
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 17.88it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.89it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 21.99it/s]
[DEBUG] Final encoded shape: (5157, 262144)
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 18.01it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.96it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 22.09it/s]
[DEBUG] Final encoded shape: (5157, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8819080860965678, 'auc': 0.9546307184439726, 'precision': 0.5145454545454545, 'recall': 0.8829953198127926, 'fpr': 0.11824623560673161}
  🚀 [BATCH] Training probe: attention
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: full
[DEBUG] Final encoded shape: (5157, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9065348070583673, 'auc': 0.9553299138165703, 'precision': 0.5844845908607864, 'recall': 0.858034321372855, 'fpr': 0.08658104517271922}
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (2682, 3584)
[DEBUG] Returning (2682, 3584) activations
[DEBUG] train activations shape: (2682, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 231.875000]
Train activations: (2682, 3584)
[DEBUG] Pre-fit sample counts — X: 2682, y: 2682
[DEBUG] y_train class distribution: {0: 2146, 1: 536}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (2682, 3584)
Input y shape: (2682,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.75GB, GPU_allocated: 0.06GB, GPU_reserved: 34.60GB
[DEBUG] Input activations shape: (2682, 3584)
[DEBUG] Using pre-aggregated inputs: (2682, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 3 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/3 [00:00<?, ?it/s]Encoding activations:  67%|████████████████████████████████████████████████████▋                          | 2/3 [00:00<00:00, 16.11it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 22.96it/s]
[DEBUG] Final encoded shape: (2682, 262144)
Encoded feature matrix shape: (2682, 262144)
[DEBUG] After encoding. Memory: RAM: 12.37GB, GPU_allocated: 7.06GB, GPU_reserved: 34.60GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(2682, 262144), y_train=(2682,)
[DEBUG] Memory at start of feature selection: RAM: 12.37GB, GPU_allocated: 7.06GB, GPU_reserved: 34.60GB
[DEBUG] Feature matrix for selection shape: (2682, 262144)
[DEBUG] Memory after feature matrix build: RAM: 12.37GB, GPU_allocated: 7.06GB, GPU_reserved: 34.60GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 12.37GB, GPU_allocated: 7.06GB, GPU_reserved: 34.60GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 12.37GB, GPU_allocated: 7.06GB, GPU_reserved: 34.60GB
Selected 3584 features
[DEBUG] X_selected shape: (2682, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (537, 3584)
[DEBUG] Returning (537, 3584) activations
[DEBUG] test activations shape: (537, 3584), dtype: float16
[DEBUG] test activations range: [-238.250000, 80.812500]
Test activations: (537, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.00it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.86it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.82it/s]
[DEBUG] Final encoded shape: (537, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9832402234636871, 'auc': 0.9985872636383394, 'precision': 0.9298245614035088, 'recall': 0.9906542056074766, 'fpr': 0.018604651162790697}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (537, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.04it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.25it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.73it/s]
[DEBUG] Final encoded shape: (537, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9832402234636871, 'auc': 0.9985872636383394, 'precision': 0.9298245614035088, 'recall': 0.9906542056074766, 'fpr': 0.018604651162790697}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (5157, 3584)
[DEBUG] Returning (5157, 3584) activations
[DEBUG] test activations shape: (5157, 3584), dtype: float16
[DEBUG] test activations range: [-1371.000000, 229.375000]
Test activations: (5157, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 16.05it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.15it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 20.89it/s]
[DEBUG] Final encoded shape: (5157, 262144)
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 18.05it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.99it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 22.13it/s]
[DEBUG] Final encoded shape: (5157, 262144)
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 18.04it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.97it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 22.12it/s]
[DEBUG] Final encoded shape: (5157, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8607717665309288, 'auc': 0.8761847285228874, 'precision': 0.45977011494252873, 'recall': 0.6864274570982839, 'fpr': 0.11448184233835253}
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (2682, 3584)
[DEBUG] Returning (2682, 3584) activations
[DEBUG] train activations shape: (2682, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.500000]
Train activations: (2682, 3584)
[DEBUG] Pre-fit sample counts — X: 2682, y: 2682
[DEBUG] y_train class distribution: {0: 2146, 1: 536}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (2682, 3584)
Input y shape: (2682,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.75GB, GPU_allocated: 0.06GB, GPU_reserved: 34.60GB
[DEBUG] Input activations shape: (2682, 3584)
[DEBUG] Using pre-aggregated inputs: (2682, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 3 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/3 [00:00<?, ?it/s]Encoding activations:  67%|████████████████████████████████████████████████████▋                          | 2/3 [00:00<00:00, 16.12it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 22.99it/s]
[DEBUG] Final encoded shape: (2682, 262144)
Encoded feature matrix shape: (2682, 262144)
[DEBUG] After encoding. Memory: RAM: 12.37GB, GPU_allocated: 7.06GB, GPU_reserved: 34.60GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(2682, 262144), y_train=(2682,)
[DEBUG] Memory at start of feature selection: RAM: 12.37GB, GPU_allocated: 7.06GB, GPU_reserved: 34.60GB
[DEBUG] Feature matrix for selection shape: (2682, 262144)
[DEBUG] Memory after feature matrix build: RAM: 12.37GB, GPU_allocated: 7.06GB, GPU_reserved: 34.60GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 12.37GB, GPU_allocated: 7.06GB, GPU_reserved: 34.60GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 12.37GB, GPU_allocated: 7.06GB, GPU_reserved: 34.60GB
Selected 3584 features
[DEBUG] X_selected shape: (2682, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 2682 full activations
[DEBUG] Estimated memory requirement: 6.19 GB
[WARNING] Large memory requirement (6.19 GB).
[DEBUG] Max sequence length: 1167
[DEBUG] Shape of first activation: (1, 188, 3584)
[DEBUG] Final array shape: (2682, 1167, 3584)
[DEBUG] Returning (2682, 1167, 3584) activations
[DEBUG] train activations shape: (2682, 1167, 3584), dtype: float16
[DEBUG] train activations range: [-1674.000000, 237.500000]
Train activations: (2682, 1167, 3584)
[DEBUG] Pre-fit sample counts — X: 2682, y: 2682
[DEBUG] y_train class distribution: {0: 2146, 1: 536}
  [DEBUG] Creating probe for architecture: attention
  [DEBUG] Creating attention probe
  [DEBUG] Fitting attention probe normally...
Epoch 10/100, Loss: 0.2758
Epoch 20/100, Loss: 0.1093
Epoch 30/100, Loss: 0.0412
Epoch 40/100, Loss: 0.0266
Epoch 50/100, Loss: 0.0195
Epoch 60/100, Loss: 0.0145
Early stopping at epoch 61
Saved probe to results/spam_gemma_9b/seed_45/1-spam-pred-auc/trained/train_on_94_better_spam_attention_L20_resid_post_state.npz
Saved training log to results/spam_gemma_9b/seed_45/1-spam-pred-auc/trained/train_on_94_better_spam_attention_L20_resid_post_state_train_log.json
  - 🔥 Probe state saved to train_on_94_better_spam_attention_L20_resid_post_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (537, 3584)
[DEBUG] Returning (537, 3584) activations
[DEBUG] test activations shape: (537, 3584), dtype: float16
[DEBUG] test activations range: [-115.250000, 237.000000]
Test activations: (537, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.00it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.81it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.72it/s]
[DEBUG] Final encoded shape: (537, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9813780260707635, 'auc': 0.999847859161052, 'precision': 0.9145299145299145, 'recall': 1.0, 'fpr': 0.023255813953488372}
    [BATCH] Evaluating on 94_better_spam test set
Loaded probe from results/spam_gemma_9b/seed_45/1-spam-pred-auc/trained/train_on_94_better_spam_attention_L20_resid_post_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 537 full activations
[DEBUG] Estimated memory requirement: 1.20 GB
[WARNING] Large memory requirement (1.20 GB).
[DEBUG] Max sequence length: 772
[DEBUG] Shape of first activation: (1, 478, 3584)
[DEBUG] Final array shape: (537, 772, 3584)
[DEBUG] Returning (537, 772, 3584) activations
[DEBUG] test activations shape: (537, 772, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 235.875000]
Test activations: (537, 772, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating validation on '94_better_spam': Got unsupported ScalarType BFloat16
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (537, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.00it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.72it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.29it/s]
[DEBUG] Final encoded shape: (537, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9813780260707635, 'auc': 0.999847859161052, 'precision': 0.9145299145299145, 'recall': 1.0, 'fpr': 0.023255813953488372}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (5157, 3584)
[DEBUG] Returning (5157, 3584) activations
[DEBUG] test activations shape: (5157, 3584), dtype: float16
[DEBUG] test activations range: [-1371.000000, 237.500000]
Test activations: (5157, 3584)
  - 🤗 Calculating metrics...
Loaded probe from results/spam_gemma_9b/seed_45/1-spam-pred-auc/trained/train_on_94_better_spam_attention_L20_resid_post_state.npz
Converted attention probe back to bfloat16 for training
Test activations: (537, 772, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating test on '94_better_spam': Got unsupported ScalarType BFloat16
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 16.03it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.11it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 20.85it/s]
[DEBUG] Final encoded shape: (5157, 262144)
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 18.04it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.95it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 22.09it/s]
[DEBUG] Final encoded shape: (5157, 262144)
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 18.03it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.94it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 22.08it/s]
[DEBUG] Final encoded shape: (5157, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.872406437851464, 'auc': 0.9517620828836697, 'precision': 0.4925764192139738, 'recall': 0.8798751950078003, 'fpr': 0.12865367581930912}
  🚀 [BATCH] Training probe: attention
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: full
Loaded probe from results/spam_gemma_9b/seed_45/1-spam-pred-auc/trained/train_on_94_better_spam_attention_L20_resid_post_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 5157
[DEBUG] Processing 5157 full activations
[DEBUG] Estimated memory requirement: 1.63 GB
[WARNING] Large memory requirement (1.63 GB).
[DEBUG] Max sequence length: 237
[DEBUG] Shape of first activation: (1, 27, 3584)
[DEBUG] Final array shape: (5157, 237, 3584)
[DEBUG] Returning (5157, 237, 3584) activations
[DEBUG] test activations shape: (5157, 237, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.500000]
Test activations: (5157, 237, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating on '87_is_spam': Got unsupported ScalarType BFloat16

=== Group 6/700 — ('94_better_spam', 20, 'resid_post', 47, '__none__', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
Split data: 2682 train, 358 val, 537 test
[DEBUG] Class distribution — train: {0: 2146, 1: 536}, val: {0: 287, 1: 71}, test: {0: 430, 1: 107}
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Split data (only_test): 0 train, 0 val, 5157 test (non-balanced)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (537, 3584)
[DEBUG] Returning (537, 3584) activations
[DEBUG] test activations shape: (537, 3584), dtype: float16
[DEBUG] test activations range: [-1053.000000, 68.750000]
Test activations: (537, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.41it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.05it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.37it/s]
[DEBUG] Final encoded shape: (537, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9702048417132216, 'auc': 0.9826994131710498, 'precision': 0.9333333333333333, 'recall': 0.9158878504672897, 'fpr': 0.01627906976744186}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (5157, 3584)
[DEBUG] Returning (5157, 3584) activations
[DEBUG] test activations shape: (5157, 3584), dtype: float16
[DEBUG] test activations range: [-1522.000000, 113.625000]
Test activations: (5157, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 15.97it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.07it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 20.80it/s]
[DEBUG] Final encoded shape: (5157, 262144)
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 17.88it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.90it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 22.02it/s]
[DEBUG] Final encoded shape: (5157, 262144)
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 17.90it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.90it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 22.02it/s]
[DEBUG] Final encoded shape: (5157, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.847585805700989, 'auc': 0.7928319347122865, 'precision': 0.37213403880070545, 'recall': 0.3291731669266771, 'fpr': 0.07883082373782108}
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (2682, 3584)
[DEBUG] Returning (2682, 3584) activations
[DEBUG] train activations shape: (2682, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.500000]
Train activations: (2682, 3584)
[DEBUG] Pre-fit sample counts — X: 2682, y: 2682
[DEBUG] y_train class distribution: {0: 2146, 1: 536}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_max
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (2682, 3584)
Input y shape: (2682,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.97GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (2682, 3584)
[DEBUG] Using pre-aggregated inputs: (2682, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 3 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/3 [00:00<?, ?it/s]Encoding activations:  67%|████████████████████████████████████████████████████▋                          | 2/3 [00:00<00:00, 15.98it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 22.76it/s]
[DEBUG] Final encoded shape: (2682, 262144)
Encoded feature matrix shape: (2682, 262144)
[DEBUG] After encoding. Memory: RAM: 9.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(2682, 262144), y_train=(2682,)
[DEBUG] Memory at start of feature selection: RAM: 9.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (2682, 262144)
[DEBUG] Memory after feature matrix build: RAM: 9.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 9.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 9.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (2682, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 2682 full activations
[DEBUG] Estimated memory requirement: 6.15 GB
[WARNING] Large memory requirement (6.15 GB).
[DEBUG] Max sequence length: 939
[DEBUG] Shape of first activation: (1, 133, 3584)
[DEBUG] Final array shape: (2682, 939, 3584)
[DEBUG] Returning (2682, 939, 3584) activations
[DEBUG] train activations shape: (2682, 939, 3584), dtype: float16
[DEBUG] train activations range: [-1674.000000, 237.500000]
Train activations: (2682, 939, 3584)
[DEBUG] Pre-fit sample counts — X: 2682, y: 2682
[DEBUG] y_train class distribution: {0: 2146, 1: 536}
  [DEBUG] Creating probe for architecture: attention
  [DEBUG] Creating attention probe
  [DEBUG] Fitting attention probe normally...
Epoch 10/100, Loss: 0.2220
Epoch 20/100, Loss: 0.0866
Epoch 30/100, Loss: 0.0406
Epoch 40/100, Loss: 0.0271
Epoch 50/100, Loss: 0.0186
Early stopping at epoch 53
Saved probe to results/spam_gemma_9b/seed_46/1-spam-pred-auc/trained/train_on_94_better_spam_attention_L20_resid_post_state.npz
Saved training log to results/spam_gemma_9b/seed_46/1-spam-pred-auc/trained/train_on_94_better_spam_attention_L20_resid_post_state_train_log.json
  - 🔥 Probe state saved to train_on_94_better_spam_attention_L20_resid_post_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (537, 3584)
[DEBUG] Returning (537, 3584) activations
[DEBUG] test activations shape: (537, 3584), dtype: float16
[DEBUG] test activations range: [-162.375000, 237.000000]
Test activations: (537, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.64it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.17it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.17it/s]
[DEBUG] Final encoded shape: (537, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9776536312849162, 'auc': 0.9993044990219517, 'precision': 0.905982905982906, 'recall': 0.9906542056074766, 'fpr': 0.02558139534883721}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (537, 3584)
  - 🤗 Calculating metrics...
Loaded probe from results/spam_gemma_9b/seed_46/1-spam-pred-auc/trained/train_on_94_better_spam_attention_L20_resid_post_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 537 full activations
[DEBUG] Estimated memory requirement: 1.24 GB
[WARNING] Large memory requirement (1.24 GB).
[DEBUG] Max sequence length: 1167
[DEBUG] Shape of first activation: (1, 208, 3584)
[DEBUG] Final array shape: (537, 1167, 3584)
[DEBUG] Returning (537, 1167, 3584) activations
[DEBUG] test activations shape: (537, 1167, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (537, 1167, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating validation on '94_better_spam': Got unsupported ScalarType BFloat16
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.61it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.83it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.85it/s]
[DEBUG] Final encoded shape: (537, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9776536312849162, 'auc': 0.9993044990219517, 'precision': 0.905982905982906, 'recall': 0.9906542056074766, 'fpr': 0.02558139534883721}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (5157, 3584)
[DEBUG] Returning (5157, 3584) activations
[DEBUG] test activations shape: (5157, 3584), dtype: float16
[DEBUG] test activations range: [-1371.000000, 237.500000]
Test activations: (5157, 3584)
  - 🤗 Calculating metrics...
Loaded probe from results/spam_gemma_9b/seed_46/1-spam-pred-auc/trained/train_on_94_better_spam_attention_L20_resid_post_state.npz
Converted attention probe back to bfloat16 for training
Test activations: (537, 1167, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]    💀💀💀 ERROR evaluating test on '94_better_spam': Got unsupported ScalarType BFloat16
Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 14.16it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 16.14it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 19.47it/s]
[DEBUG] Final encoded shape: (5157, 262144)
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 17.82it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.84it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 21.94it/s]
[DEBUG] Final encoded shape: (5157, 262144)
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 17.84it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.85it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 21.94it/s]
[DEBUG] Final encoded shape: (5157, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8966453364359124, 'auc': 0.9522730067750097, 'precision': 0.5543259557344065, 'recall': 0.859594383775351, 'fpr': 0.09809565987599646}
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (2682, 3584)
[DEBUG] Returning (2682, 3584) activations
[DEBUG] train activations shape: (2682, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 231.875000]
Train activations: (2682, 3584)
[DEBUG] Pre-fit sample counts — X: 2682, y: 2682
[DEBUG] y_train class distribution: {0: 2146, 1: 536}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (2682, 3584)
Input y shape: (2682,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.97GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (2682, 3584)
[DEBUG] Using pre-aggregated inputs: (2682, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 3 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/3 [00:00<?, ?it/s]Encoding activations:  67%|████████████████████████████████████████████████████▋                          | 2/3 [00:00<00:00, 15.98it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 22.75it/s]
[DEBUG] Final encoded shape: (2682, 262144)
Encoded feature matrix shape: (2682, 262144)
[DEBUG] After encoding. Memory: RAM: 9.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(2682, 262144), y_train=(2682,)
[DEBUG] Memory at start of feature selection: RAM: 9.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (2682, 262144)
[DEBUG] Memory after feature matrix build: RAM: 9.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 9.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 9.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (2682, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Loaded probe from results/spam_gemma_9b/seed_46/1-spam-pred-auc/trained/train_on_94_better_spam_attention_L20_resid_post_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 5157
[DEBUG] Processing 5157 full activations
[DEBUG] Estimated memory requirement: 1.63 GB
[WARNING] Large memory requirement (1.63 GB).
[DEBUG] Max sequence length: 237
[DEBUG] Shape of first activation: (1, 27, 3584)
[DEBUG] Final array shape: (5157, 237, 3584)
[DEBUG] Returning (5157, 237, 3584) activations
[DEBUG] test activations shape: (5157, 237, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.500000]
Test activations: (5157, 237, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating on '87_is_spam': Got unsupported ScalarType BFloat16

=== Group 7/700 — ('94_better_spam', 20, 'resid_post', 48, '__none__', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
Split data: 2682 train, 358 val, 537 test
[DEBUG] Class distribution — train: {0: 2146, 1: 536}, val: {0: 287, 1: 71}, test: {0: 430, 1: 107}
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Split data (only_test): 0 train, 0 val, 5157 test (non-balanced)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (5157, 3584)
[DEBUG] Returning (5157, 3584) activations
[DEBUG] test activations shape: (5157, 3584), dtype: float16
[DEBUG] test activations range: [-1371.000000, 237.500000]
Test activations: (5157, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (537, 3584)
[DEBUG] Returning (537, 3584) activations
[DEBUG] test activations shape: (537, 3584), dtype: float16
[DEBUG] test activations range: [-237.500000, 92.687500]
Test activations: (537, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 16.12it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.10it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 20.85it/s]
[DEBUG] Final encoded shape: (5157, 262144)
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 18.03it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.95it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 22.09it/s]
[DEBUG] Final encoded shape: (5157, 262144)
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 18.02it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.93it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 22.07it/s]
[DEBUG] Final encoded shape: (5157, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.903820050416909, 'auc': 0.9441331151917467, 'precision': 0.5782092772384034, 'recall': 0.8361934477379095, 'fpr': 0.08658104517271922}
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.65it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.75it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.66it/s]
[DEBUG] Final encoded shape: (537, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9869646182495344, 'auc': 0.9976092153879591, 'precision': 0.9629629629629629, 'recall': 0.9719626168224299, 'fpr': 0.009302325581395349}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (2682, 3584)
[DEBUG] Returning (2682, 3584) activations
[DEBUG] train activations shape: (2682, 3584), dtype: float16
[DEBUG] train activations range: [-1337.000000, 219.750000]
Train activations: (2682, 3584)
[DEBUG] Pre-fit sample counts — X: 2682, y: 2682
[DEBUG] y_train class distribution: {0: 2146, 1: 536}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_last
  [DEBUG] Fitting SAE probe normally...
Test activations: (537, 3584)
  - 🤗 Calculating metrics...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (2682, 3584)
Input y shape: (2682,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.75GB, GPU_allocated: 0.06GB, GPU_reserved: 41.02GB
[DEBUG] Input activations shape: (2682, 3584)
[DEBUG] Using pre-aggregated inputs: (2682, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 3 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/3 [00:00<?, ?it/s]Encoding activations:  67%|████████████████████████████████████████████████████▋                          | 2/3 [00:00<00:00, 16.11it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 22.96it/s]
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.73it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.33it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.88it/s]
[DEBUG] Final encoded shape: (537, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9869646182495344, 'auc': 0.9976092153879591, 'precision': 0.9629629629629629, 'recall': 0.9719626168224299, 'fpr': 0.009302325581395349}
[DEBUG] Final encoded shape: (2682, 262144)
Encoded feature matrix shape: (2682, 262144)
[DEBUG] After encoding. Memory: RAM: 12.36GB, GPU_allocated: 7.06GB, GPU_reserved: 41.02GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(2682, 262144), y_train=(2682,)
[DEBUG] Memory at start of feature selection: RAM: 12.36GB, GPU_allocated: 7.06GB, GPU_reserved: 41.02GB
[DEBUG] Feature matrix for selection shape: (2682, 262144)
[DEBUG] Memory after feature matrix build: RAM: 12.36GB, GPU_allocated: 7.06GB, GPU_reserved: 41.02GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 12.36GB, GPU_allocated: 7.06GB, GPU_reserved: 41.02GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 12.36GB, GPU_allocated: 7.06GB, GPU_reserved: 41.02GB
Selected 3584 features
[DEBUG] X_selected shape: (2682, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (5157, 3584)
[DEBUG] Returning (5157, 3584) activations
[DEBUG] test activations shape: (5157, 3584), dtype: float16
[DEBUG] test activations range: [-1371.000000, 229.375000]
Test activations: (5157, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 15.93it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.02it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 20.74it/s]
[DEBUG] Final encoded shape: (5157, 262144)
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 17.88it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.88it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 21.98it/s]
[DEBUG] Final encoded shape: (5157, 262144)
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 17.84it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.86it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 21.96it/s]
[DEBUG] Final encoded shape: (5157, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8811324413418654, 'auc': 0.8964980122677008, 'precision': 0.5171149144254279, 'recall': 0.6599063962558502, 'fpr': 0.08746678476527901}
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (537, 3584)
[DEBUG] Returning (537, 3584) activations
[DEBUG] test activations shape: (537, 3584), dtype: float16
[DEBUG] test activations range: [-1377.000000, 231.875000]
Test activations: (537, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.01it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.68it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.57it/s]
[DEBUG] Final encoded shape: (537, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9795158286778398, 'auc': 0.9969354488154749, 'precision': 0.9285714285714286, 'recall': 0.9719626168224299, 'fpr': 0.018604651162790697}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (2682, 3584)
[DEBUG] Returning (2682, 3584) activations
[DEBUG] train activations shape: (2682, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.500000]
Train activations: (2682, 3584)
[DEBUG] Pre-fit sample counts — X: 2682, y: 2682
[DEBUG] y_train class distribution: {0: 2146, 1: 536}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (2682, 3584)
Input y shape: (2682,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.98GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (2682, 3584)
[DEBUG] Using pre-aggregated inputs: (2682, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 3 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/3 [00:00<?, ?it/s]Encoding activations:  67%|████████████████████████████████████████████████████▋                          | 2/3 [00:00<00:00, 15.95it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 22.69it/s]
[DEBUG] Final encoded shape: (2682, 262144)
Encoded feature matrix shape: (2682, 262144)
[DEBUG] After encoding. Memory: RAM: 9.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(2682, 262144), y_train=(2682,)
[DEBUG] Memory at start of feature selection: RAM: 9.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (2682, 262144)
[DEBUG] Memory after feature matrix build: RAM: 9.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 9.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 9.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (2682, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (537, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.04it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.72it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.63it/s]
[DEBUG] Final encoded shape: (537, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9795158286778398, 'auc': 0.9969354488154749, 'precision': 0.9285714285714286, 'recall': 0.9719626168224299, 'fpr': 0.018604651162790697}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (5157, 3584)
[DEBUG] Returning (5157, 3584) activations
[DEBUG] test activations shape: (5157, 3584), dtype: float16
[DEBUG] test activations range: [-1371.000000, 229.375000]
Test activations: (5157, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (537, 3584)
[DEBUG] Returning (537, 3584) activations
[DEBUG] test activations shape: (537, 3584), dtype: float16
[DEBUG] test activations range: [-162.375000, 237.000000]
Test activations: (537, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 16.04it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.06it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 20.79it/s]
[DEBUG] Final encoded shape: (5157, 262144)
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 18.03it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.94it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 22.08it/s]
[DEBUG] Final encoded shape: (5157, 262144)
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 18.01it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.93it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 22.06it/s]
[DEBUG] Final encoded shape: (5157, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8793872406437852, 'auc': 0.9059216044461087, 'precision': 0.5104281009879253, 'recall': 0.7254290171606864, 'fpr': 0.0987599645704163}
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.64it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.81it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.77it/s]
[DEBUG] Final encoded shape: (537, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9795158286778398, 'auc': 0.9990002173440558, 'precision': 0.9067796610169492, 'recall': 1.0, 'fpr': 0.02558139534883721}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (2682, 3584)
[DEBUG] Returning (2682, 3584) activations
[DEBUG] train activations shape: (2682, 3584), dtype: float16
[DEBUG] train activations range: [-1337.000000, 237.500000]
Train activations: (2682, 3584)
[DEBUG] Pre-fit sample counts — X: 2682, y: 2682
[DEBUG] y_train class distribution: {0: 2146, 1: 536}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_softmax
  [DEBUG] Fitting SAE probe normally...
Test activations: (537, 3584)
  - 🤗 Calculating metrics...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (2682, 3584)
Input y shape: (2682,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.74GB, GPU_allocated: 0.06GB, GPU_reserved: 41.02GB
[DEBUG] Input activations shape: (2682, 3584)
[DEBUG] Using pre-aggregated inputs: (2682, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 3 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/3 [00:00<?, ?it/s]Encoding activations:  67%|████████████████████████████████████████████████████▋                          | 2/3 [00:00<00:00, 16.11it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 22.96it/s]
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.71it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.95it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.20it/s]
[DEBUG] Final encoded shape: (537, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9795158286778398, 'auc': 0.9990002173440558, 'precision': 0.9067796610169492, 'recall': 1.0, 'fpr': 0.02558139534883721}
[DEBUG] Final encoded shape: (2682, 262144)
Encoded feature matrix shape: (2682, 262144)
[DEBUG] After encoding. Memory: RAM: 12.36GB, GPU_allocated: 7.06GB, GPU_reserved: 41.02GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(2682, 262144), y_train=(2682,)
[DEBUG] Memory at start of feature selection: RAM: 12.36GB, GPU_allocated: 7.06GB, GPU_reserved: 41.02GB
[DEBUG] Feature matrix for selection shape: (2682, 262144)
[DEBUG] Memory after feature matrix build: RAM: 12.36GB, GPU_allocated: 7.06GB, GPU_reserved: 41.02GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 12.36GB, GPU_allocated: 7.06GB, GPU_reserved: 41.02GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 12.36GB, GPU_allocated: 7.06GB, GPU_reserved: 41.02GB
Selected 3584 features
[DEBUG] X_selected shape: (2682, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (5157, 3584)
[DEBUG] Returning (5157, 3584) activations
[DEBUG] test activations shape: (5157, 3584), dtype: float16
[DEBUG] test activations range: [-1371.000000, 237.500000]
Test activations: (5157, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 15.94it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.02it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 20.73it/s]
[DEBUG] Final encoded shape: (5157, 262144)
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 17.84it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.85it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 21.94it/s]
[DEBUG] Final encoded shape: (5157, 262144)
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 17.84it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.85it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 21.94it/s]
[DEBUG] Final encoded shape: (5157, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8819080860965678, 'auc': 0.9532575457136976, 'precision': 0.5143626570915619, 'recall': 0.8939157566302652, 'fpr': 0.11979627989371125}
  🚀 [BATCH] Training probe: attention
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: full
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (537, 3584)
[DEBUG] Returning (537, 3584) activations
[DEBUG] test activations shape: (537, 3584), dtype: float16
[DEBUG] test activations range: [-1377.000000, 237.000000]
Test activations: (537, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.01it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.71it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.99it/s]
[DEBUG] Final encoded shape: (537, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9776536312849162, 'auc': 0.9998913279721799, 'precision': 0.8991596638655462, 'recall': 1.0, 'fpr': 0.027906976744186046}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (537, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.03it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.78it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.03it/s]
[DEBUG] Final encoded shape: (537, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9776536312849162, 'auc': 0.9998913279721799, 'precision': 0.8991596638655462, 'recall': 1.0, 'fpr': 0.027906976744186046}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (5157, 3584)
[DEBUG] Returning (5157, 3584) activations
[DEBUG] test activations shape: (5157, 3584), dtype: float16
[DEBUG] test activations range: [-1371.000000, 237.500000]
Test activations: (5157, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 16.05it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.08it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 20.82it/s]
[DEBUG] Final encoded shape: (5157, 262144)
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 18.04it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.97it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 22.11it/s]
[DEBUG] Final encoded shape: (5157, 262144)
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 17.97it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.94it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 22.05it/s]
[DEBUG] Final encoded shape: (5157, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.890246267209618, 'auc': 0.9458952671658682, 'precision': 0.5374625374625375, 'recall': 0.8393135725429017, 'fpr': 0.10252435783879539}
  🚀 [BATCH] Training probe: attention
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: full
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 2682 full activations
[DEBUG] Estimated memory requirement: 6.17 GB
[WARNING] Large memory requirement (6.17 GB).
[DEBUG] Max sequence length: 1167
[DEBUG] Shape of first activation: (1, 190, 3584)
[DEBUG] Final array shape: (2682, 1167, 3584)
[DEBUG] Returning (2682, 1167, 3584) activations
[DEBUG] train activations shape: (2682, 1167, 3584), dtype: float16
[DEBUG] train activations range: [-1674.000000, 237.500000]
Train activations: (2682, 1167, 3584)
[DEBUG] Pre-fit sample counts — X: 2682, y: 2682
[DEBUG] y_train class distribution: {0: 2146, 1: 536}
  [DEBUG] Creating probe for architecture: attention
  [DEBUG] Creating attention probe
  [DEBUG] Fitting attention probe normally...
Epoch 10/100, Loss: 0.2746
Epoch 20/100, Loss: 0.1037
Epoch 30/100, Loss: 0.0425
Epoch 40/100, Loss: 0.0259
Epoch 50/100, Loss: 0.0198
Early stopping at epoch 54
Saved probe to results/spam_gemma_9b/seed_47/1-spam-pred-auc/trained/train_on_94_better_spam_attention_L20_resid_post_state.npz
Saved training log to results/spam_gemma_9b/seed_47/1-spam-pred-auc/trained/train_on_94_better_spam_attention_L20_resid_post_state_train_log.json
  - 🔥 Probe state saved to train_on_94_better_spam_attention_L20_resid_post_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Loaded probe from results/spam_gemma_9b/seed_47/1-spam-pred-auc/trained/train_on_94_better_spam_attention_L20_resid_post_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 537 full activations
[DEBUG] Estimated memory requirement: 1.19 GB
[WARNING] Large memory requirement (1.19 GB).
[DEBUG] Max sequence length: 710
[DEBUG] Shape of first activation: (1, 320, 3584)
[DEBUG] Final array shape: (537, 710, 3584)
[DEBUG] Returning (537, 710, 3584) activations
[DEBUG] test activations shape: (537, 710, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (537, 710, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating validation on '94_better_spam': Got unsupported ScalarType BFloat16
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 2682 full activations
[DEBUG] Estimated memory requirement: 6.15 GB
[WARNING] Large memory requirement (6.15 GB).
[DEBUG] Max sequence length: 1167
[DEBUG] Shape of first activation: (1, 63, 3584)
[DEBUG] Final array shape: (2682, 1167, 3584)
[DEBUG] Returning (2682, 1167, 3584) activations
[DEBUG] train activations shape: (2682, 1167, 3584), dtype: float16
[DEBUG] train activations range: [-1674.000000, 237.500000]
Train activations: (2682, 1167, 3584)
[DEBUG] Pre-fit sample counts — X: 2682, y: 2682
[DEBUG] y_train class distribution: {0: 2146, 1: 536}
  [DEBUG] Creating probe for architecture: attention
  [DEBUG] Creating attention probe
  [DEBUG] Fitting attention probe normally...
Loaded probe from results/spam_gemma_9b/seed_47/1-spam-pred-auc/trained/train_on_94_better_spam_attention_L20_resid_post_state.npz
Converted attention probe back to bfloat16 for training
Test activations: (537, 710, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating test on '94_better_spam': Got unsupported ScalarType BFloat16
Epoch 10/100, Loss: 0.3047
Epoch 20/100, Loss: 0.1271
Epoch 30/100, Loss: 0.0613
Epoch 40/100, Loss: 0.0260
Epoch 50/100, Loss: 0.0233
Early stopping at epoch 50
Saved probe to results/spam_gemma_9b/seed_48/1-spam-pred-auc/trained/train_on_94_better_spam_attention_L20_resid_post_state.npz
Saved training log to results/spam_gemma_9b/seed_48/1-spam-pred-auc/trained/train_on_94_better_spam_attention_L20_resid_post_state_train_log.json
  - 🔥 Probe state saved to train_on_94_better_spam_attention_L20_resid_post_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Loaded probe from results/spam_gemma_9b/seed_48/1-spam-pred-auc/trained/train_on_94_better_spam_attention_L20_resid_post_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 537 full activations
[DEBUG] Estimated memory requirement: 1.25 GB
[WARNING] Large memory requirement (1.25 GB).
[DEBUG] Max sequence length: 798
[DEBUG] Shape of first activation: (1, 204, 3584)
[DEBUG] Final array shape: (537, 798, 3584)
[DEBUG] Returning (537, 798, 3584) activations
[DEBUG] test activations shape: (537, 798, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (537, 798, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating validation on '94_better_spam': Got unsupported ScalarType BFloat16
    [BATCH] Evaluating on 94_better_spam test set
Loaded probe from results/spam_gemma_9b/seed_47/1-spam-pred-auc/trained/train_on_94_better_spam_attention_L20_resid_post_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 5157
[DEBUG] Processing 5157 full activations
[DEBUG] Estimated memory requirement: 1.63 GB
[WARNING] Large memory requirement (1.63 GB).
[DEBUG] Max sequence length: 237
[DEBUG] Shape of first activation: (1, 27, 3584)
[DEBUG] Final array shape: (5157, 237, 3584)
[DEBUG] Returning (5157, 237, 3584) activations
[DEBUG] test activations shape: (5157, 237, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.500000]
Test activations: (5157, 237, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating on '87_is_spam': Got unsupported ScalarType BFloat16

=== Group 8/700 — ('94_better_spam', 20, 'resid_post', 49, '__none__', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
Split data: 2682 train, 358 val, 537 test
[DEBUG] Class distribution — train: {0: 2146, 1: 536}, val: {0: 287, 1: 71}, test: {0: 430, 1: 107}
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Split data (only_test): 0 train, 0 val, 5157 test (non-balanced)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Loaded probe from results/spam_gemma_9b/seed_48/1-spam-pred-auc/trained/train_on_94_better_spam_attention_L20_resid_post_state.npz
Converted attention probe back to bfloat16 for training
Test activations: (537, 798, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating test on '94_better_spam': Got unsupported ScalarType BFloat16
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (537, 3584)
[DEBUG] Returning (537, 3584) activations
[DEBUG] test activations shape: (537, 3584), dtype: float16
[DEBUG] test activations range: [-663.000000, 51.000000]
Test activations: (537, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.70it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.36it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.36it/s]
[DEBUG] Final encoded shape: (537, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9851024208566108, 'auc': 0.9986742012605955, 'precision': 0.9459459459459459, 'recall': 0.9813084112149533, 'fpr': 0.013953488372093023}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (537, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.70it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.20it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.47it/s]
[DEBUG] Final encoded shape: (537, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9851024208566108, 'auc': 0.9986742012605955, 'precision': 0.9459459459459459, 'recall': 0.9813084112149533, 'fpr': 0.013953488372093023}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (5157, 3584)
[DEBUG] Returning (5157, 3584) activations
[DEBUG] test activations shape: (5157, 3584), dtype: float16
[DEBUG] test activations range: [-1522.000000, 113.625000]
Test activations: (5157, 3584)
  - 🤗 Calculating metrics...
Loaded probe from results/spam_gemma_9b/seed_48/1-spam-pred-auc/trained/train_on_94_better_spam_attention_L20_resid_post_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 5157
[DEBUG] Processing 5157 full activations
[DEBUG] Estimated memory requirement: 1.63 GB
[WARNING] Large memory requirement (1.63 GB).
[DEBUG] Max sequence length: 237
[DEBUG] Shape of first activation: (1, 27, 3584)
[DEBUG] Final array shape: (5157, 237, 3584)
[DEBUG] Returning (5157, 237, 3584) activations
[DEBUG] test activations shape: (5157, 237, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.500000]
Test activations: (5157, 237, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating on '87_is_spam': Got unsupported ScalarType BFloat16

=== Group 9/700 — ('94_better_spam', 20, 'resid_post', 50, '__none__', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
Split data: 2682 train, 358 val, 537 test
[DEBUG] Class distribution — train: {0: 2146, 1: 536}, val: {0: 287, 1: 71}, test: {0: 430, 1: 107}
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Split data (only_test): 0 train, 0 val, 5157 test (non-balanced)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (5157, 3584)
[DEBUG] Returning (5157, 3584) activations
[DEBUG] test activations shape: (5157, 3584), dtype: float16
[DEBUG] test activations range: [-1522.000000, 113.625000]
Test activations: (5157, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 15.95it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.02it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 20.75it/s]
[DEBUG] Final encoded shape: (5157, 262144)
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 17.83it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.85it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 21.94it/s]
[DEBUG] Final encoded shape: (5157, 262144)
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 17.81it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.85it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 21.94it/s]
[DEBUG] Final encoded shape: (5157, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8859802210587551, 'auc': 0.8618204781335629, 'precision': 0.5726027397260274, 'recall': 0.32605304212168484, 'fpr': 0.03454384410983171}
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 16.55it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.30it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 21.14it/s]
[DEBUG] Final encoded shape: (5157, 262144)
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 17.99it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.92it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 22.05it/s]
[DEBUG] Final encoded shape: (5157, 262144)
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 18.00it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.92it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 22.05it/s]
[DEBUG] Final encoded shape: (5157, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8590265658328485, 'auc': 0.8049289819245561, 'precision': 0.4234875444839858, 'recall': 0.3712948517940718, 'fpr': 0.07174490699734277}
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (2682, 3584)
[DEBUG] Returning (2682, 3584) activations
[DEBUG] train activations shape: (2682, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.500000]
Train activations: (2682, 3584)
[DEBUG] Pre-fit sample counts — X: 2682, y: 2682
[DEBUG] y_train class distribution: {0: 2146, 1: 536}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_max
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (2682, 3584)
[DEBUG] Returning (2682, 3584) activations
[DEBUG] train activations shape: (2682, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.500000]
Train activations: (2682, 3584)
[DEBUG] Pre-fit sample counts — X: 2682, y: 2682
[DEBUG] y_train class distribution: {0: 2146, 1: 536}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_max
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (2682, 3584)
Input y shape: (2682,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.92GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (2682, 3584)
[DEBUG] Using pre-aggregated inputs: (2682, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 3 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/3 [00:00<?, ?it/s]Encoding activations:  67%|████████████████████████████████████████████████████▋                          | 2/3 [00:00<00:00, 16.24it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 23.09it/s]
[DEBUG] Final encoded shape: (2682, 262144)
Encoded feature matrix shape: (2682, 262144)
[DEBUG] After encoding. Memory: RAM: 9.54GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(2682, 262144), y_train=(2682,)
[DEBUG] Memory at start of feature selection: RAM: 9.54GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (2682, 262144)
[DEBUG] Memory after feature matrix build: RAM: 9.54GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 9.54GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 9.54GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (2682, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (2682, 3584)
Input y shape: (2682,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.76GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (2682, 3584)
[DEBUG] Using pre-aggregated inputs: (2682, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 3 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/3 [00:00<?, ?it/s]Encoding activations:  67%|████████████████████████████████████████████████████▋                          | 2/3 [00:00<00:00, 16.11it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 22.96it/s]
[DEBUG] Final encoded shape: (2682, 262144)
Encoded feature matrix shape: (2682, 262144)
[DEBUG] After encoding. Memory: RAM: 12.38GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(2682, 262144), y_train=(2682,)
[DEBUG] Memory at start of feature selection: RAM: 12.38GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (2682, 262144)
[DEBUG] Memory after feature matrix build: RAM: 12.38GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 12.38GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 12.38GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (2682, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (537, 3584)
[DEBUG] Returning (537, 3584) activations
[DEBUG] test activations shape: (537, 3584), dtype: float16
[DEBUG] test activations range: [-124.375000, 237.000000]
Test activations: (537, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (537, 3584)
[DEBUG] Returning (537, 3584) activations
[DEBUG] test activations shape: (537, 3584), dtype: float16
[DEBUG] test activations range: [-118.875000, 237.000000]
Test activations: (537, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.66it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.93it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.15it/s]
[DEBUG] Final encoded shape: (537, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9813780260707635, 'auc': 0.9993697022386437, 'precision': 0.9217391304347826, 'recall': 0.9906542056074766, 'fpr': 0.020930232558139535}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.01it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.96it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.63it/s]
[DEBUG] Final encoded shape: (537, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9813780260707635, 'auc': 0.9996087806998478, 'precision': 0.9217391304347826, 'recall': 0.9906542056074766, 'fpr': 0.020930232558139535}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (537, 3584)
  - 🤗 Calculating metrics...
Test activations: (537, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.64it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.24it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.25it/s]
[DEBUG] Final encoded shape: (537, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9813780260707635, 'auc': 0.9993697022386437, 'precision': 0.9217391304347826, 'recall': 0.9906542056074766, 'fpr': 0.020930232558139535}
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.01it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.72it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.92it/s]
[DEBUG] Final encoded shape: (537, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9813780260707635, 'auc': 0.9996087806998478, 'precision': 0.9217391304347826, 'recall': 0.9906542056074766, 'fpr': 0.020930232558139535}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (5157, 3584)
[DEBUG] Returning (5157, 3584) activations
[DEBUG] test activations shape: (5157, 3584), dtype: float16
[DEBUG] test activations range: [-1371.000000, 237.500000]
Test activations: (5157, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (5157, 3584)
[DEBUG] Returning (5157, 3584) activations
[DEBUG] test activations shape: (5157, 3584), dtype: float16
[DEBUG] test activations range: [-1371.000000, 237.500000]
Test activations: (5157, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 15.92it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.01it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 20.72it/s]
[DEBUG] Final encoded shape: (5157, 262144)
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 17.86it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.87it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 21.97it/s]
[DEBUG] Final encoded shape: (5157, 262144)
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s][DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  20%|███████████████▊                                                               | 1/5 [00:00<00:00,  6.99it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00,  9.96it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00,  8.06it/s]Encoding activations:  60%|███████████████████████████████████████████████▍                               | 3/5 [00:00<00:00,  9.54it/s]Encoding activations:  60%|███████████████████████████████████████████████▍                               | 3/5 [00:00<00:00,  8.46it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00,  9.35it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 11.64it/s]
Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 11.30it/s]
[DEBUG] Final encoded shape: (5157, 262144)
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s][DEBUG] Final encoded shape: (5157, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9001357378320729, 'auc': 0.9434619014521431, 'precision': 0.5665961945031712, 'recall': 0.8361934477379095, 'fpr': 0.09078830823737821}
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 17.98it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.92it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 22.05it/s]
[DEBUG] Final encoded shape: (5157, 262144)
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 18.06it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.96it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 22.09it/s]
[DEBUG] Final encoded shape: (5157, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9044017839829358, 'auc': 0.9556577480105404, 'precision': 0.5770833333333333, 'recall': 0.8642745709828393, 'fpr': 0.08990256864481842}
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (2682, 3584)
[DEBUG] Returning (2682, 3584) activations
[DEBUG] train activations shape: (2682, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 231.875000]
Train activations: (2682, 3584)
[DEBUG] Pre-fit sample counts — X: 2682, y: 2682
[DEBUG] y_train class distribution: {0: 2146, 1: 536}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_last
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (2682, 3584)
[DEBUG] Returning (2682, 3584) activations
[DEBUG] train activations shape: (2682, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 231.875000]
Train activations: (2682, 3584)
[DEBUG] Pre-fit sample counts — X: 2682, y: 2682
[DEBUG] y_train class distribution: {0: 2146, 1: 536}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (2682, 3584)
Input y shape: (2682,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.93GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (2682, 3584)
[DEBUG] Using pre-aggregated inputs: (2682, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 3 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/3 [00:00<?, ?it/s]Encoding activations:  67%|████████████████████████████████████████████████████▋                          | 2/3 [00:00<00:00, 15.89it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 22.63it/s]

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (2682, 3584)
Input y shape: (2682,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.77GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (2682, 3584)
[DEBUG] Using pre-aggregated inputs: (2682, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 3 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/3 [00:00<?, ?it/s]Encoding activations:  67%|████████████████████████████████████████████████████▋                          | 2/3 [00:00<00:00, 16.08it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 22.89it/s]
[DEBUG] Final encoded shape: (2682, 262144)
Encoded feature matrix shape: (2682, 262144)
[DEBUG] After encoding. Memory: RAM: 9.55GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(2682, 262144), y_train=(2682,)
[DEBUG] Memory at start of feature selection: RAM: 9.55GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (2682, 262144)
[DEBUG] Memory after feature matrix build: RAM: 9.55GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 9.55GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 9.55GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (2682, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Final encoded shape: (2682, 262144)
Encoded feature matrix shape: (2682, 262144)
[DEBUG] After encoding. Memory: RAM: 12.38GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(2682, 262144), y_train=(2682,)
[DEBUG] Memory at start of feature selection: RAM: 12.38GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (2682, 262144)
[DEBUG] Memory after feature matrix build: RAM: 12.38GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 12.38GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 12.38GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (2682, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (537, 3584)
[DEBUG] Returning (537, 3584) activations
[DEBUG] test activations shape: (537, 3584), dtype: float16
[DEBUG] test activations range: [-237.500000, 86.000000]
Test activations: (537, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (537, 3584)
[DEBUG] Returning (537, 3584) activations
[DEBUG] test activations shape: (537, 3584), dtype: float16
[DEBUG] test activations range: [-237.500000, 92.687500]
Test activations: (537, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.65it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.83it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.09it/s]
[DEBUG] Final encoded shape: (537, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9813780260707635, 'auc': 0.9983481851771353, 'precision': 0.9532710280373832, 'recall': 0.9532710280373832, 'fpr': 0.011627906976744186}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.02it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.70it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.72it/s]
[DEBUG] Final encoded shape: (537, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9739292364990689, 'auc': 0.9877852640730276, 'precision': 0.9345794392523364, 'recall': 0.9345794392523364, 'fpr': 0.01627906976744186}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (537, 3584)
  - 🤗 Calculating metrics...
Test activations: (537, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.68it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.91it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.85it/s]
[DEBUG] Final encoded shape: (537, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9813780260707635, 'auc': 0.9983481851771353, 'precision': 0.9532710280373832, 'recall': 0.9532710280373832, 'fpr': 0.011627906976744186}
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.99it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.74it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.51it/s]
[DEBUG] Final encoded shape: (537, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9739292364990689, 'auc': 0.9877852640730276, 'precision': 0.9345794392523364, 'recall': 0.9345794392523364, 'fpr': 0.01627906976744186}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (5157, 3584)
[DEBUG] Returning (5157, 3584) activations
[DEBUG] test activations shape: (5157, 3584), dtype: float16
[DEBUG] test activations range: [-1371.000000, 229.375000]
Test activations: (5157, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (5157, 3584)
[DEBUG] Returning (5157, 3584) activations
[DEBUG] test activations shape: (5157, 3584), dtype: float16
[DEBUG] test activations range: [-1371.000000, 229.375000]
Test activations: (5157, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 15.97it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.04it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 20.77it/s]
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 16.06it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.07it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 20.82it/s]
[DEBUG] Final encoded shape: (5157, 262144)
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 17.90it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.91it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 22.02it/s]
[DEBUG] Final encoded shape: (5157, 262144)
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 17.94it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.89it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 22.01it/s]
[DEBUG] Final encoded shape: (5157, 262144)
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 17.89it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.91it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 22.01it/s]
[DEBUG] Final encoded shape: (5157, 262144)
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 17.99it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.92it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 22.04it/s]
[DEBUG] Final encoded shape: (5157, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.887919332945511, 'auc': 0.9018856857020072, 'precision': 0.5379975874547648, 'recall': 0.6957878315132605, 'fpr': 0.08480956598759964}
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
[DEBUG] Final encoded shape: (5157, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8766724840023269, 'auc': 0.8780159709488469, 'precision': 0.5033025099075297, 'recall': 0.594383775351014, 'fpr': 0.08325952170062002}
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (2682, 3584)
[DEBUG] Returning (2682, 3584) activations
[DEBUG] train activations shape: (2682, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.500000]
Train activations: (2682, 3584)
[DEBUG] Pre-fit sample counts — X: 2682, y: 2682
[DEBUG] y_train class distribution: {0: 2146, 1: 536}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_softmax
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (2682, 3584)
[DEBUG] Returning (2682, 3584) activations
[DEBUG] train activations shape: (2682, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.500000]
Train activations: (2682, 3584)
[DEBUG] Pre-fit sample counts — X: 2682, y: 2682
[DEBUG] y_train class distribution: {0: 2146, 1: 536}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (2682, 3584)
Input y shape: (2682,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.93GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (2682, 3584)
[DEBUG] Using pre-aggregated inputs: (2682, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 3 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/3 [00:00<?, ?it/s]Encoding activations:  67%|████████████████████████████████████████████████████▋                          | 2/3 [00:00<00:00, 15.98it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 22.74it/s]

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (2682, 3584)
Input y shape: (2682,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.75GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (2682, 3584)
[DEBUG] Using pre-aggregated inputs: (2682, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 3 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/3 [00:00<?, ?it/s]Encoding activations:  67%|████████████████████████████████████████████████████▋                          | 2/3 [00:00<00:00, 16.06it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 22.86it/s]
[DEBUG] Final encoded shape: (2682, 262144)
Encoded feature matrix shape: (2682, 262144)
[DEBUG] After encoding. Memory: RAM: 9.55GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(2682, 262144), y_train=(2682,)
[DEBUG] Memory at start of feature selection: RAM: 9.55GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (2682, 262144)
[DEBUG] Memory after feature matrix build: RAM: 9.55GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 9.55GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 9.55GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (2682, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Final encoded shape: (2682, 262144)
Encoded feature matrix shape: (2682, 262144)
[DEBUG] After encoding. Memory: RAM: 12.37GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(2682, 262144), y_train=(2682,)
[DEBUG] Memory at start of feature selection: RAM: 12.37GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (2682, 262144)
[DEBUG] Memory after feature matrix build: RAM: 12.37GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 12.37GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 12.37GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (2682, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (537, 3584)
[DEBUG] Returning (537, 3584) activations
[DEBUG] test activations shape: (537, 3584), dtype: float16
[DEBUG] test activations range: [-119.187500, 237.000000]
Test activations: (537, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (537, 3584)
[DEBUG] Returning (537, 3584) activations
[DEBUG] test activations shape: (537, 3584), dtype: float16
[DEBUG] test activations range: [-124.375000, 237.000000]
Test activations: (537, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 23.33it/s]
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s][DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 17.25it/s]
Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 22.81it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s][DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 16.94it/s]
Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 16.94it/s]
[DEBUG] Final encoded shape: (537, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9795158286778398, 'auc': 0.9996305151054119, 'precision': 0.9067796610169492, 'recall': 1.0, 'fpr': 0.02558139534883721}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 34.54it/s]
[DEBUG] Final encoded shape: (537, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9776536312849162, 'auc': 0.9995870462942839, 'precision': 0.8991596638655462, 'recall': 1.0, 'fpr': 0.027906976744186046}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (537, 3584)
  - 🤗 Calculating metrics...
Test activations: (537, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.02it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.70it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s][DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 18.52it/s]
Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 14.50it/s]
[DEBUG] Final encoded shape: (537, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9795158286778398, 'auc': 0.9996305151054119, 'precision': 0.9067796610169492, 'recall': 1.0, 'fpr': 0.02558139534883721}
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 32.73it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.70it/s]
[DEBUG] Final encoded shape: (537, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9776536312849162, 'auc': 0.9995870462942839, 'precision': 0.8991596638655462, 'recall': 1.0, 'fpr': 0.027906976744186046}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (5157, 3584)
[DEBUG] Returning (5157, 3584) activations
[DEBUG] test activations shape: (5157, 3584), dtype: float16
[DEBUG] test activations range: [-1371.000000, 237.500000]
Test activations: (5157, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (5157, 3584)
[DEBUG] Returning (5157, 3584) activations
[DEBUG] test activations shape: (5157, 3584), dtype: float16
[DEBUG] test activations range: [-1371.000000, 237.500000]
Test activations: (5157, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 16.11it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.10it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 20.86it/s]
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 15.97it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.06it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 20.78it/s]
[DEBUG] Final encoded shape: (5157, 262144)
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 18.00it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.91it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 22.02it/s]
[DEBUG] Final encoded shape: (5157, 262144)
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 17.87it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.89it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 21.99it/s]
[DEBUG] Final encoded shape: (5157, 262144)
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 17.97it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.91it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 22.03it/s]
[DEBUG] Final encoded shape: (5157, 262144)
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 17.88it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.89it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 22.00it/s]
[DEBUG] Final encoded shape: (5157, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8855923986814039, 'auc': 0.9570291934795196, 'precision': 0.5230769230769231, 'recall': 0.9017160686427457, 'fpr': 0.116696191319752}
  🚀 [BATCH] Training probe: attention
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: full
[DEBUG] Final encoded shape: (5157, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8873375993794842, 'auc': 0.9484605956426033, 'precision': 0.5283018867924528, 'recall': 0.8736349453978159, 'fpr': 0.11071744906997343}
  🚀 [BATCH] Training probe: attention
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: full
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 2682 full activations
[DEBUG] Estimated memory requirement: 6.14 GB
[WARNING] Large memory requirement (6.14 GB).
[DEBUG] Max sequence length: 1167
[DEBUG] Shape of first activation: (1, 69, 3584)
[DEBUG] Final array shape: (2682, 1167, 3584)
[DEBUG] Returning (2682, 1167, 3584) activations
[DEBUG] train activations shape: (2682, 1167, 3584), dtype: float16
[DEBUG] train activations range: [-1674.000000, 237.500000]
Train activations: (2682, 1167, 3584)
[DEBUG] Pre-fit sample counts — X: 2682, y: 2682
[DEBUG] y_train class distribution: {0: 2146, 1: 536}
  [DEBUG] Creating probe for architecture: attention
  [DEBUG] Creating attention probe
  [DEBUG] Fitting attention probe normally...
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 2682 full activations
[DEBUG] Estimated memory requirement: 6.14 GB
[WARNING] Large memory requirement (6.14 GB).
[DEBUG] Max sequence length: 1167
[DEBUG] Shape of first activation: (1, 255, 3584)
[DEBUG] Final array shape: (2682, 1167, 3584)
[DEBUG] Returning (2682, 1167, 3584) activations
[DEBUG] train activations shape: (2682, 1167, 3584), dtype: float16
[DEBUG] train activations range: [-1674.000000, 237.500000]
Train activations: (2682, 1167, 3584)
[DEBUG] Pre-fit sample counts — X: 2682, y: 2682
[DEBUG] y_train class distribution: {0: 2146, 1: 536}
  [DEBUG] Creating probe for architecture: attention
  [DEBUG] Creating attention probe
  [DEBUG] Fitting attention probe normally...
Epoch 10/100, Loss: 0.2850
Epoch 20/100, Loss: 0.1138
Epoch 30/100, Loss: 0.0611
Epoch 40/100, Loss: 0.0342
Epoch 50/100, Loss: 0.0224
Epoch 60/100, Loss: 0.0175
Early stopping at epoch 66
Saved probe to results/spam_gemma_9b/seed_50/1-spam-pred-auc/trained/train_on_94_better_spam_attention_L20_resid_post_state.npz
Saved training log to results/spam_gemma_9b/seed_50/1-spam-pred-auc/trained/train_on_94_better_spam_attention_L20_resid_post_state_train_log.json
  - 🔥 Probe state saved to train_on_94_better_spam_attention_L20_resid_post_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Epoch 10/100, Loss: 0.2765
Epoch 20/100, Loss: 0.1187
Epoch 30/100, Loss: 0.0623
Epoch 40/100, Loss: 0.0312
Epoch 50/100, Loss: 0.0246
Epoch 60/100, Loss: 0.0132
Epoch 70/100, Loss: 0.0164
Early stopping at epoch 70
Saved probe to results/spam_gemma_9b/seed_49/1-spam-pred-auc/trained/train_on_94_better_spam_attention_L20_resid_post_state.npz
Saved training log to results/spam_gemma_9b/seed_49/1-spam-pred-auc/trained/train_on_94_better_spam_attention_L20_resid_post_state_train_log.json
  - 🔥 Probe state saved to train_on_94_better_spam_attention_L20_resid_post_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Loaded probe from results/spam_gemma_9b/seed_50/1-spam-pred-auc/trained/train_on_94_better_spam_attention_L20_resid_post_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 537 full activations
[DEBUG] Estimated memory requirement: 1.27 GB
[WARNING] Large memory requirement (1.27 GB).
[DEBUG] Max sequence length: 939
[DEBUG] Shape of first activation: (1, 207, 3584)
[DEBUG] Final array shape: (537, 939, 3584)
[DEBUG] Returning (537, 939, 3584) activations
[DEBUG] test activations shape: (537, 939, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (537, 939, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating validation on '94_better_spam': Got unsupported ScalarType BFloat16
    [BATCH] Evaluating on 94_better_spam test set
Loaded probe from results/spam_gemma_9b/seed_49/1-spam-pred-auc/trained/train_on_94_better_spam_attention_L20_resid_post_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 537 full activations
[DEBUG] Estimated memory requirement: 1.21 GB
[WARNING] Large memory requirement (1.21 GB).
[DEBUG] Max sequence length: 939
[DEBUG] Shape of first activation: (1, 190, 3584)
[DEBUG] Final array shape: (537, 939, 3584)
[DEBUG] Returning (537, 939, 3584) activations
[DEBUG] test activations shape: (537, 939, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (537, 939, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating validation on '94_better_spam': Got unsupported ScalarType BFloat16
    [BATCH] Evaluating on 94_better_spam test set
Loaded probe from results/spam_gemma_9b/seed_50/1-spam-pred-auc/trained/train_on_94_better_spam_attention_L20_resid_post_state.npz
Converted attention probe back to bfloat16 for training
Test activations: (537, 939, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating test on '94_better_spam': Got unsupported ScalarType BFloat16
Loaded probe from results/spam_gemma_9b/seed_49/1-spam-pred-auc/trained/train_on_94_better_spam_attention_L20_resid_post_state.npz
Converted attention probe back to bfloat16 for training
Test activations: (537, 939, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating test on '94_better_spam': Got unsupported ScalarType BFloat16
Loaded probe from results/spam_gemma_9b/seed_50/1-spam-pred-auc/trained/train_on_94_better_spam_attention_L20_resid_post_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 5157
[DEBUG] Processing 5157 full activations
[DEBUG] Estimated memory requirement: 1.63 GB
[WARNING] Large memory requirement (1.63 GB).
[DEBUG] Max sequence length: 237
[DEBUG] Shape of first activation: (1, 27, 3584)
[DEBUG] Final array shape: (5157, 237, 3584)
[DEBUG] Returning (5157, 237, 3584) activations
[DEBUG] test activations shape: (5157, 237, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.500000]
Test activations: (5157, 237, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating on '87_is_spam': Got unsupported ScalarType BFloat16

=== Group 10/700 — ('94_better_spam', 20, 'resid_post', 51, '__none__', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
Split data: 2682 train, 358 val, 537 test
[DEBUG] Class distribution — train: {0: 2146, 1: 536}, val: {0: 287, 1: 71}, test: {0: 430, 1: 107}
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Split data (only_test): 0 train, 0 val, 5157 test (non-balanced)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
Loaded probe from results/spam_gemma_9b/seed_49/1-spam-pred-auc/trained/train_on_94_better_spam_attention_L20_resid_post_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 5157
[DEBUG] Processing 5157 full activations
[DEBUG] Estimated memory requirement: 1.63 GB
[WARNING] Large memory requirement (1.63 GB).
[DEBUG] Max sequence length: 237
[DEBUG] Shape of first activation: (1, 27, 3584)
[DEBUG] Final array shape: (5157, 237, 3584)
[DEBUG] Returning (5157, 237, 3584) activations
[DEBUG] test activations shape: (5157, 237, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.500000]
Test activations: (5157, 237, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating on '87_is_spam': Got unsupported ScalarType BFloat16

=== Group 11/700 — ('94_better_spam', 20, 'resid_post', 42, '{"class_counts": {"0": 1750, "1": 1}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (537, 3584)
[DEBUG] Returning (537, 3584) activations
[DEBUG] test activations shape: (537, 3584), dtype: float16
[DEBUG] test activations range: [-1377.000000, 237.000000]
Test activations: (537, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1751, 3584)
[DEBUG] Returning (1751, 3584) activations
[DEBUG] train activations shape: (1751, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1751, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_max
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.04it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.86it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.47it/s]
[DEBUG] Final encoded shape: (537, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9851024208566108, 'auc': 0.9999347967833079, 'precision': 0.9304347826086956, 'recall': 1.0, 'fpr': 0.018604651162790697}

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1751, 3584)
Input y shape: (1751,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.91GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1751, 3584)
[DEBUG] Using pre-aggregated inputs: (1751, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.71it/s]
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (5157, 3584)
[DEBUG] Returning (5157, 3584) activations
[DEBUG] test activations shape: (5157, 3584), dtype: float16
[DEBUG] test activations range: [-1371.000000, 237.500000]
Test activations: (5157, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Final encoded shape: (1751, 262144)
Encoded feature matrix shape: (1751, 262144)
[DEBUG] After encoding. Memory: RAM: 8.62GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1751, 262144), y_train=(1751,)
[DEBUG] Memory at start of feature selection: RAM: 8.62GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1751, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.62GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.62GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.62GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1751, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 16.11it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.09it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 20.84it/s]
[DEBUG] Final encoded shape: (5157, 262144)
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 18.03it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.91it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 22.05it/s]
[DEBUG] Final encoded shape: (5157, 262144)
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 18.02it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.92it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 22.05it/s]
[DEBUG] Final encoded shape: (5157, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.881326352530541, 'auc': 0.9478418906463965, 'precision': 0.5131938125568699, 'recall': 0.8798751950078003, 'fpr': 0.11846767050487157}
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-1337.000000, 237.500000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.97it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.60it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.62it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.503731343283582, 'auc': 0.8598797059478726, 'precision': 1.0, 'recall': 0.007462686567164179, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (2682, 3584)
[DEBUG] Returning (2682, 3584) activations
[DEBUG] train activations shape: (2682, 3584), dtype: float16
[DEBUG] train activations range: [-1337.000000, 219.750000]
Train activations: (2682, 3584)
[DEBUG] Pre-fit sample counts — X: 2682, y: 2682
[DEBUG] y_train class distribution: {0: 2146, 1: 536}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (2682, 3584)
Input y shape: (2682,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.72GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (2682, 3584)
[DEBUG] Using pre-aggregated inputs: (2682, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 3 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/3 [00:00<?, ?it/s]Encoding activations:  67%|████████████████████████████████████████████████████▋                          | 2/3 [00:00<00:00, 16.11it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 22.97it/s]
[DEBUG] Final encoded shape: (2682, 262144)
Encoded feature matrix shape: (2682, 262144)
[DEBUG] After encoding. Memory: RAM: 12.34GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(2682, 262144), y_train=(2682,)
[DEBUG] Memory at start of feature selection: RAM: 12.34GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (2682, 262144)
[DEBUG] Memory after feature matrix build: RAM: 12.34GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 12.34GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 12.34GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (2682, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.60it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.66it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.76it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.503731343283582, 'auc': 0.8598797059478726, 'precision': 1.0, 'recall': 0.007462686567164179, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-150.125000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (537, 3584)
[DEBUG] Returning (537, 3584) activations
[DEBUG] test activations shape: (537, 3584), dtype: float16
[DEBUG] test activations range: [-1377.000000, 231.875000]
Test activations: (537, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 27.69it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.60it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.58it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.8641869543736509, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.05it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.66it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.96it/s]
[DEBUG] Final encoded shape: (537, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9795158286778398, 'auc': 0.9983699195826994, 'precision': 0.9137931034482759, 'recall': 0.9906542056074766, 'fpr': 0.023255813953488372}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1751, 3584)
[DEBUG] Returning (1751, 3584) activations
[DEBUG] train activations shape: (1751, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 231.875000]
Train activations: (1751, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_last
  [DEBUG] Fitting SAE probe normally...
Test activations: (537, 3584)
  - 🤗 Calculating metrics...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1751, 3584)
Input y shape: (1751,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.91GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1751, 3584)
[DEBUG] Using pre-aggregated inputs: (1751, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.71it/s]
[DEBUG] Final encoded shape: (1751, 262144)
Encoded feature matrix shape: (1751, 262144)
[DEBUG] After encoding. Memory: RAM: 8.62GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1751, 262144), y_train=(1751,)
[DEBUG] Memory at start of feature selection: RAM: 8.62GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1751, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.62GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.62GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.62GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1751, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.02it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.95it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.87it/s]
[DEBUG] Final encoded shape: (537, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9795158286778398, 'auc': 0.9983699195826994, 'precision': 0.9137931034482759, 'recall': 0.9906542056074766, 'fpr': 0.023255813953488372}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (5157, 3584)
[DEBUG] Returning (5157, 3584) activations
[DEBUG] test activations shape: (5157, 3584), dtype: float16
[DEBUG] test activations range: [-1371.000000, 229.375000]
Test activations: (5157, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 16.11it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.10it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 20.85it/s]
[DEBUG] Final encoded shape: (5157, 262144)
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 18.03it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.92it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 22.06it/s]
[DEBUG] Final encoded shape: (5157, 262144)
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 18.03it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.92it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 22.05it/s]
[DEBUG] Final encoded shape: (5157, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8658134574364941, 'auc': 0.8756762227973618, 'precision': 0.47357512953367875, 'recall': 0.7129485179407177, 'fpr': 0.112488928255093}
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-1337.000000, 219.750000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.57it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.93it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.73it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5914179104477612, 'auc': 0.7204834038761417, 'precision': 0.9622641509433962, 'recall': 0.19029850746268656, 'fpr': 0.007462686567164179}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (2682, 3584)
[DEBUG] Returning (2682, 3584) activations
[DEBUG] train activations shape: (2682, 3584), dtype: float16
[DEBUG] train activations range: [-1337.000000, 237.500000]
Train activations: (2682, 3584)
[DEBUG] Pre-fit sample counts — X: 2682, y: 2682
[DEBUG] y_train class distribution: {0: 2146, 1: 536}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (2682, 3584)
Input y shape: (2682,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.72GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (2682, 3584)
[DEBUG] Using pre-aggregated inputs: (2682, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 3 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/3 [00:00<?, ?it/s]Encoding activations:  67%|████████████████████████████████████████████████████▋                          | 2/3 [00:00<00:00, 16.11it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 22.96it/s]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Final encoded shape: (2682, 262144)
Encoded feature matrix shape: (2682, 262144)
[DEBUG] After encoding. Memory: RAM: 12.34GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(2682, 262144), y_train=(2682,)
[DEBUG] Memory at start of feature selection: RAM: 12.34GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (2682, 262144)
[DEBUG] Memory after feature matrix build: RAM: 12.34GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 12.34GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 12.34GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (2682, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.02it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 35.06it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.64it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5914179104477612, 'auc': 0.7204834038761417, 'precision': 0.9622641509433962, 'recall': 0.19029850746268656, 'fpr': 0.007462686567164179}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-259.000000, 76.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.04it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s][DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (537, 3584)
[DEBUG] Returning (537, 3584) activations
[DEBUG] test activations shape: (537, 3584), dtype: float16
[DEBUG] test activations range: [-1377.000000, 237.000000]
Test activations: (537, 3584)
  - 🤗 Calculating metrics...
Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.62it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.56it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5842433697347894, 'auc': 0.6694517390680026, 'precision': 0.990909090909091, 'recall': 0.17004680187207488, 'fpr': 0.0015600624024961}
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.03it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.69it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.68it/s]
[DEBUG] Final encoded shape: (537, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9776536312849162, 'auc': 0.999956531188872, 'precision': 0.8991596638655462, 'recall': 1.0, 'fpr': 0.027906976744186046}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1751, 3584)
[DEBUG] Returning (1751, 3584) activations
[DEBUG] train activations shape: (1751, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1751, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_softmax
  [DEBUG] Fitting SAE probe normally...
Test activations: (537, 3584)
  - 🤗 Calculating metrics...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1751, 3584)
Input y shape: (1751,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.91GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1751, 3584)
[DEBUG] Using pre-aggregated inputs: (1751, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.74it/s]
[DEBUG] Final encoded shape: (1751, 262144)
Encoded feature matrix shape: (1751, 262144)
[DEBUG] After encoding. Memory: RAM: 8.62GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1751, 262144), y_train=(1751,)
[DEBUG] Memory at start of feature selection: RAM: 8.62GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1751, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.62GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.62GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.62GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1751, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.04it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.98it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.59it/s]
[DEBUG] Final encoded shape: (537, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9776536312849162, 'auc': 0.999956531188872, 'precision': 0.8991596638655462, 'recall': 1.0, 'fpr': 0.027906976744186046}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (5157, 3584)
[DEBUG] Returning (5157, 3584) activations
[DEBUG] test activations shape: (5157, 3584), dtype: float16
[DEBUG] test activations range: [-1371.000000, 237.500000]
Test activations: (5157, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 16.03it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.04it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 20.78it/s]
[DEBUG] Final encoded shape: (5157, 262144)
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 18.04it/s]Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.95it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 22.08it/s]
[DEBUG] Final encoded shape: (5157, 262144)
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|███████████████████████████████▌                                               | 2/5 [00:00<00:00, 18.02it/s][DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-1337.000000, 237.500000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Encoding activations:  80%|███████████████████████████████████████████████████████████████▏               | 4/5 [00:00<00:00, 17.91it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 22.04it/s]
[DEBUG] Final encoded shape: (5157, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8681403917006011, 'auc': 0.9470421686663747, 'precision': 0.4835164835164835, 'recall': 0.8923556942277691, 'fpr': 0.13529672276350752}
  🚀 [BATCH] Training probe: attention
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: full
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.64it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.86it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.78it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.503731343283582, 'auc': 0.891025284027623, 'precision': 1.0, 'recall': 0.007462686567164179, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.70it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.30it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.50it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.503731343283582, 'auc': 0.891025284027623, 'precision': 1.0, 'recall': 0.007462686567164179, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-150.125000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.14it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.69it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.66it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.8228830245253493, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: attention
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: full
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 1751 full activations
[DEBUG] Estimated memory requirement: 3.28 GB
[WARNING] Large memory requirement (3.28 GB).
[DEBUG] Max sequence length: 520
[DEBUG] Shape of first activation: (1, 70, 3584)
[DEBUG] Final array shape: (1751, 520, 3584)
[DEBUG] Returning (1751, 520, 3584) activations
[DEBUG] train activations shape: (1751, 520, 3584), dtype: float16
[DEBUG] train activations range: [-1674.000000, 237.000000]
Train activations: (1751, 520, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: attention
  [DEBUG] Creating attention probe
  [DEBUG] Fitting attention probe normally...
Epoch 10/100, Loss: 0.1179
Epoch 20/100, Loss: 0.0350
Epoch 30/100, Loss: 0.0033
Epoch 40/100, Loss: 0.0008
Early stopping at epoch 43
Saved probe to results/spam_gemma_9b/seed_42/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state.npz
Saved training log to results/spam_gemma_9b/seed_42/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state_train_log.json
  - 🔥 Probe state saved to train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 2682 full activations
[DEBUG] Estimated memory requirement: 6.07 GB
[WARNING] Large memory requirement (6.07 GB).
[DEBUG] Max sequence length: 939
[DEBUG] Shape of first activation: (1, 122, 3584)
[DEBUG] Final array shape: (2682, 939, 3584)
[DEBUG] Returning (2682, 939, 3584) activations
[DEBUG] train activations shape: (2682, 939, 3584), dtype: float16
[DEBUG] train activations range: [-1674.000000, 237.500000]
Train activations: (2682, 939, 3584)
[DEBUG] Pre-fit sample counts — X: 2682, y: 2682
[DEBUG] y_train class distribution: {0: 2146, 1: 536}
  [DEBUG] Creating probe for architecture: attention
  [DEBUG] Creating attention probe
  [DEBUG] Fitting attention probe normally...
Epoch 10/100, Loss: 0.2349
Epoch 20/100, Loss: 0.0915
Epoch 30/100, Loss: 0.0464
Epoch 40/100, Loss: 0.0309
Epoch 50/100, Loss: 0.0178
Epoch 60/100, Loss: 0.0134
Early stopping at epoch 65
Saved probe to results/spam_gemma_9b/seed_51/1-spam-pred-auc/trained/train_on_94_better_spam_attention_L20_resid_post_state.npz
Saved training log to results/spam_gemma_9b/seed_51/1-spam-pred-auc/trained/train_on_94_better_spam_attention_L20_resid_post_state_train_log.json
  - 🔥 Probe state saved to train_on_94_better_spam_attention_L20_resid_post_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Loaded probe from results/spam_gemma_9b/seed_42/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 536 full activations
[DEBUG] Estimated memory requirement: 1.58 GB
[WARNING] Large memory requirement (1.58 GB).
[DEBUG] Max sequence length: 939
[DEBUG] Shape of first activation: (1, 151, 3584)
[DEBUG] Final array shape: (536, 939, 3584)
[DEBUG] Returning (536, 939, 3584) activations
[DEBUG] test activations shape: (536, 939, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.500000]
Test activations: (536, 939, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating validation on '94_better_spam': Got unsupported ScalarType BFloat16
    [BATCH] Evaluating on 94_better_spam test set
Loaded probe from results/spam_gemma_9b/seed_51/1-spam-pred-auc/trained/train_on_94_better_spam_attention_L20_resid_post_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 537 full activations
[DEBUG] Estimated memory requirement: 1.30 GB
[WARNING] Large memory requirement (1.30 GB).
[DEBUG] Max sequence length: 1167
[DEBUG] Shape of first activation: (1, 237, 3584)
[DEBUG] Final array shape: (537, 1167, 3584)
[DEBUG] Returning (537, 1167, 3584) activations
[DEBUG] test activations shape: (537, 1167, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (537, 1167, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating validation on '94_better_spam': Got unsupported ScalarType BFloat16
    [BATCH] Evaluating on 94_better_spam test set
Loaded probe from results/spam_gemma_9b/seed_42/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state.npz
Converted attention probe back to bfloat16 for training
Test activations: (536, 939, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating test on '94_better_spam': Got unsupported ScalarType BFloat16
Loaded probe from results/spam_gemma_9b/seed_42/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 5157
[DEBUG] Processing 1282 full activations
[DEBUG] Estimated memory requirement: 0.60 GB
[WARNING] Large memory requirement (0.60 GB).
[DEBUG] Max sequence length: 210
[DEBUG] Shape of first activation: (1, 54, 3584)
[DEBUG] Final array shape: (1282, 210, 3584)
[DEBUG] Returning (1282, 210, 3584) activations
[DEBUG] test activations shape: (1282, 210, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (1282, 210, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating on '87_is_spam': Got unsupported ScalarType BFloat16

=== Group 12/700 — ('94_better_spam', 20, 'resid_post', 43, '{"class_counts": {"0": 1750, "1": 1}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
Loaded probe from results/spam_gemma_9b/seed_51/1-spam-pred-auc/trained/train_on_94_better_spam_attention_L20_resid_post_state.npz
Converted attention probe back to bfloat16 for training
Test activations: (537, 1167, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating test on '94_better_spam': Got unsupported ScalarType BFloat16
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1751, 3584)
[DEBUG] Returning (1751, 3584) activations
[DEBUG] train activations shape: (1751, 3584), dtype: float16
[DEBUG] train activations range: [-162.375000, 237.000000]
Train activations: (1751, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1751, 3584)
Input y shape: (1751,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.87GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1751, 3584)
[DEBUG] Using pre-aggregated inputs: (1751, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 334.37it/s]
[DEBUG] Final encoded shape: (1751, 16384)
Encoded feature matrix shape: (1751, 16384)
[DEBUG] After encoding. Memory: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1751, 16384), y_train=(1751,)
[DEBUG] Memory at start of feature selection: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1751, 16384)
[DEBUG] Memory after feature matrix build: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1751, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-1337.000000, 237.500000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 408.13it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 425.13it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 419.98it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.503731343283582, 'auc': 0.8190855424370682, 'precision': 1.0, 'recall': 0.007462686567164179, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Loaded probe from results/spam_gemma_9b/seed_51/1-spam-pred-auc/trained/train_on_94_better_spam_attention_L20_resid_post_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 5157
[DEBUG] Processing 5157 full activations
[DEBUG] Estimated memory requirement: 1.63 GB
[WARNING] Large memory requirement (1.63 GB).
[DEBUG] Max sequence length: 237
[DEBUG] Shape of first activation: (1, 27, 3584)
[DEBUG] Final array shape: (5157, 237, 3584)
[DEBUG] Returning (5157, 237, 3584) activations
[DEBUG] test activations shape: (5157, 237, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.500000]
Test activations: (5157, 237, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating on '87_is_spam': Got unsupported ScalarType BFloat16

=== Group 13/700 — ('94_better_spam', 20, 'resid_post', 44, '{"class_counts": {"0": 1750, "1": 1}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 411.53it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 387.14it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 376.10it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.503731343283582, 'auc': 0.8190855424370682, 'precision': 1.0, 'recall': 0.007462686567164179, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-147.875000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 435.48it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 433.36it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 405.91it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.8210893178316836, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-238.250000, 80.812500]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 411.00it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 376.17it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 383.67it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5055970149253731, 'auc': 0.7319141234127868, 'precision': 0.8, 'recall': 0.014925373134328358, 'fpr': 0.0037313432835820895}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-274.000000, 76.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 361.92it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 404.21it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 430.10it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.6908082875577113, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1751, 3584)
[DEBUG] Returning (1751, 3584) activations
[DEBUG] train activations shape: (1751, 3584), dtype: float16
[DEBUG] train activations range: [-1067.000000, 73.875000]
Train activations: (1751, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_mean
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1751, 3584)
Input y shape: (1751,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.90GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1751, 3584)
[DEBUG] Using pre-aggregated inputs: (1751, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s][DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1751, 3584)
[DEBUG] Returning (1751, 3584) activations
[DEBUG] train activations shape: (1751, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1751, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_softmax
  [DEBUG] Fitting SAE probe normally...
Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.60it/s]

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1751, 3584)
Input y shape: (1751,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.73GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1751, 3584)
[DEBUG] Using pre-aggregated inputs: (1751, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 331.28it/s]
[DEBUG] Final encoded shape: (1751, 16384)
Encoded feature matrix shape: (1751, 16384)
[DEBUG] After encoding. Memory: RAM: 9.73GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1751, 16384), y_train=(1751,)
[DEBUG] Memory at start of feature selection: RAM: 9.73GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1751, 16384)
[DEBUG] Memory after feature matrix build: RAM: 9.73GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 9.73GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 9.73GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1751, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Final encoded shape: (1751, 262144)
Encoded feature matrix shape: (1751, 262144)
[DEBUG] After encoding. Memory: RAM: 8.61GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1751, 262144), y_train=(1751,)
[DEBUG] Memory at start of feature selection: RAM: 8.61GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1751, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.61GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.61GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.61GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1751, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-109.625000, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 414.54it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 409.00it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 426.86it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5018656716417911, 'auc': 0.8871547115170416, 'precision': 1.0, 'recall': 0.0037313432835820895, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-1506.000000, 112.125000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 384.66it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 414.05it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 376.58it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5018656716417911, 'auc': 0.8871547115170416, 'precision': 1.0, 'recall': 0.0037313432835820895, 'fpr': 0.0}
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.62it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.64it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.80it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.539179104477612, 'auc': 0.5988388282468255, 'precision': 1.0, 'recall': 0.07835820895522388, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-143.875000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 445.11it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 399.17it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 408.86it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.7813089434653829, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1751, 3584)
[DEBUG] Returning (1751, 3584) activations
[DEBUG] train activations shape: (1751, 3584), dtype: float16
[DEBUG] train activations range: [-1526.000000, 117.875000]
Train activations: (1751, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_mean
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.02it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.92it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.73it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.539179104477612, 'auc': 0.5988388282468255, 'precision': 1.0, 'recall': 0.07835820895522388, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1063.000000, 70.687500]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1751, 3584)
Input y shape: (1751,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.73GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1751, 3584)
[DEBUG] Using pre-aggregated inputs: (1751, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.92it/s]
[DEBUG] Final encoded shape: (1751, 262144)
Encoded feature matrix shape: (1751, 262144)
[DEBUG] After encoding. Memory: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1751, 262144), y_train=(1751,)
[DEBUG] Memory at start of feature selection: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1751, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1751, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.22it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.79it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.65it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.6598528527724572, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-593.000000, 51.843750]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1751, 3584)
[DEBUG] Returning (1751, 3584) activations
[DEBUG] train activations shape: (1751, 3584), dtype: float16
[DEBUG] train activations range: [-162.375000, 237.000000]
Train activations: (1751, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_max
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.01it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.94it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.05it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5186567164179104, 'auc': 0.6467336823345957, 'precision': 1.0, 'recall': 0.03731343283582089, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1751, 3584)
Input y shape: (1751,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.90GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1751, 3584)
[DEBUG] Using pre-aggregated inputs: (1751, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.73it/s]
[DEBUG] Final encoded shape: (1751, 262144)
Encoded feature matrix shape: (1751, 262144)
[DEBUG] After encoding. Memory: RAM: 8.61GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1751, 262144), y_train=(1751,)
[DEBUG] Memory at start of feature selection: RAM: 8.61GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1751, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.61GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.61GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.61GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1751, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-1337.000000, 237.500000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.01it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.66it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.59it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5186567164179104, 'auc': 0.6467336823345957, 'precision': 1.0, 'recall': 0.03731343283582089, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1053.000000, 69.625000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.63it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.94it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.98it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.503731343283582, 'auc': 0.8546586099353977, 'precision': 1.0, 'recall': 0.007462686567164179, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.36it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.17it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.04it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5226209048361935, 'auc': 0.49374879831386703, 'precision': 0.8085106382978723, 'recall': 0.059282371294851796, 'fpr': 0.014040561622464899}
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1751, 3584)
[DEBUG] Returning (1751, 3584) activations
[DEBUG] train activations shape: (1751, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1751, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_max
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.65it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.26it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.95it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.503731343283582, 'auc': 0.8546586099353977, 'precision': 1.0, 'recall': 0.007462686567164179, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-147.875000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1751, 3584)
Input y shape: (1751,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.73GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1751, 3584)
[DEBUG] Using pre-aggregated inputs: (1751, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.92it/s]
[DEBUG] Final encoded shape: (1751, 262144)
Encoded feature matrix shape: (1751, 262144)
[DEBUG] After encoding. Memory: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1751, 262144), y_train=(1751,)
[DEBUG] Memory at start of feature selection: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1751, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1751, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.14it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.67it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.73it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.7442349488051284, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-109.562500, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1751, 3584)
[DEBUG] Returning (1751, 3584) activations
[DEBUG] train activations shape: (1751, 3584), dtype: float16
[DEBUG] train activations range: [-256.250000, 92.687500]
Train activations: (1751, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_last
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.02it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.66it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.75it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5018656716417911, 'auc': 0.802698262419247, 'precision': 1.0, 'recall': 0.0037313432835820895, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1751, 3584)
Input y shape: (1751,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.90GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1751, 3584)
[DEBUG] Using pre-aggregated inputs: (1751, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.72it/s]
[DEBUG] Final encoded shape: (1751, 262144)
Encoded feature matrix shape: (1751, 262144)
[DEBUG] After encoding. Memory: RAM: 8.61GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1751, 262144), y_train=(1751,)
[DEBUG] Memory at start of feature selection: RAM: 8.61GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1751, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.61GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.61GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.61GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1751, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-1337.000000, 219.750000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.02it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.49it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.61it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5018656716417911, 'auc': 0.802698262419247, 'precision': 1.0, 'recall': 0.0037313432835820895, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-143.875000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.66it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.90it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.07it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5298507462686567, 'auc': 0.6625640454444197, 'precision': 0.9444444444444444, 'recall': 0.06343283582089553, 'fpr': 0.0037313432835820895}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.40it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.03it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.03it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.6397107678378898, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1751, 3584)
[DEBUG] Returning (1751, 3584) activations
[DEBUG] train activations shape: (1751, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 231.875000]
Train activations: (1751, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_last
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.63it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.00it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.79it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5298507462686567, 'auc': 0.6625640454444197, 'precision': 0.9444444444444444, 'recall': 0.06343283582089553, 'fpr': 0.0037313432835820895}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-252.250000, 76.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1751, 3584)
Input y shape: (1751,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.72GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1751, 3584)
[DEBUG] Using pre-aggregated inputs: (1751, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.91it/s]
[DEBUG] Final encoded shape: (1751, 262144)
Encoded feature matrix shape: (1751, 262144)
[DEBUG] After encoding. Memory: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1751, 262144), y_train=(1751,)
[DEBUG] Memory at start of feature selection: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1751, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1751, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.11it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.63it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.61it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.500780031201248, 'auc': 0.6130388117240758, 'precision': 0.5384615384615384, 'recall': 0.0109204368174727, 'fpr': 0.0093603744149766}
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Train activations: (1751, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_softmax
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.05it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.96it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.70it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5597014925373134, 'auc': 0.6158804856315439, 'precision': 0.9, 'recall': 0.13432835820895522, 'fpr': 0.014925373134328358}
    [BATCH] Evaluating on 94_better_spam test set

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1751, 3584)
Input y shape: (1751,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.91GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1751, 3584)
[DEBUG] Using pre-aggregated inputs: (1751, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.72it/s]
[DEBUG] Final encoded shape: (1751, 262144)
Encoded feature matrix shape: (1751, 262144)
[DEBUG] After encoding. Memory: RAM: 8.62GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1751, 262144), y_train=(1751,)
[DEBUG] Memory at start of feature selection: RAM: 8.62GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1751, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.62GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.62GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.62GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1751, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.04it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.65it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.83it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5597014925373134, 'auc': 0.6158804856315439, 'precision': 0.9, 'recall': 0.13432835820895522, 'fpr': 0.014925373134328358}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.69it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.13it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.97it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.503731343283582, 'auc': 0.8166768768099799, 'precision': 1.0, 'recall': 0.007462686567164179, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.40it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.96it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.02it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5327613104524181, 'auc': 0.5959024632436156, 'precision': 0.7625, 'recall': 0.09516380655226209, 'fpr': 0.029641185647425898}
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Train activations: (1751, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_softmax
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.64it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.91it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.97it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.503731343283582, 'auc': 0.8166768768099799, 'precision': 1.0, 'recall': 0.007462686567164179, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1751, 3584)
Input y shape: (1751,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.71GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1751, 3584)
[DEBUG] Using pre-aggregated inputs: (1751, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.86it/s]
[DEBUG] Final encoded shape: (1751, 262144)
Encoded feature matrix shape: (1751, 262144)
[DEBUG] After encoding. Memory: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1751, 262144), y_train=(1751,)
[DEBUG] Memory at start of feature selection: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1751, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1751, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.11it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.62it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.69it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.6822705357512273, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: attention
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: full
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.04it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.71it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.90it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5018656716417911, 'auc': 0.7773863889507686, 'precision': 1.0, 'recall': 0.0037313432835820895, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.08it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.04it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.09it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5018656716417911, 'auc': 0.7773863889507686, 'precision': 1.0, 'recall': 0.0037313432835820895, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 1751 full activations
[DEBUG] Estimated memory requirement: 3.19 GB
[WARNING] Large memory requirement (3.19 GB).
[DEBUG] Max sequence length: 520
[DEBUG] Shape of first activation: (1, 240, 3584)
[DEBUG] Final array shape: (1751, 520, 3584)
[DEBUG] Returning (1751, 520, 3584) activations
[DEBUG] train activations shape: (1751, 520, 3584), dtype: float16
[DEBUG] train activations range: [-1674.000000, 237.000000]
Train activations: (1751, 520, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: attention
  [DEBUG] Creating attention probe
  [DEBUG] Fitting attention probe normally...
Epoch 10/100, Loss: 0.1092
Epoch 20/100, Loss: 0.0255
Epoch 30/100, Loss: 0.0029
Early stopping at epoch 39
Saved probe to results/spam_gemma_9b/seed_43/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state.npz
Saved training log to results/spam_gemma_9b/seed_43/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state_train_log.json
  - 🔥 Probe state saved to train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.42it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.06it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.02it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.6478785828500222, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: attention
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: full
Loaded probe from results/spam_gemma_9b/seed_43/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 536 full activations
[DEBUG] Estimated memory requirement: 1.61 GB
[WARNING] Large memory requirement (1.61 GB).
[DEBUG] Max sequence length: 939
[DEBUG] Shape of first activation: (1, 62, 3584)
[DEBUG] Final array shape: (536, 939, 3584)
[DEBUG] Returning (536, 939, 3584) activations
[DEBUG] test activations shape: (536, 939, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.500000]
Test activations: (536, 939, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating validation on '94_better_spam': Got unsupported ScalarType BFloat16
    [BATCH] Evaluating on 94_better_spam test set
Loaded probe from results/spam_gemma_9b/seed_43/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state.npz
Converted attention probe back to bfloat16 for training
Test activations: (536, 939, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating test on '94_better_spam': Got unsupported ScalarType BFloat16
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 1751 full activations
[DEBUG] Estimated memory requirement: 3.33 GB
[WARNING] Large memory requirement (3.33 GB).
[DEBUG] Max sequence length: 520
[DEBUG] Shape of first activation: (1, 27, 3584)
[DEBUG] Final array shape: (1751, 520, 3584)
[DEBUG] Returning (1751, 520, 3584) activations
[DEBUG] train activations shape: (1751, 520, 3584), dtype: float16
[DEBUG] train activations range: [-1674.000000, 237.000000]
Train activations: (1751, 520, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: attention
  [DEBUG] Creating attention probe
  [DEBUG] Fitting attention probe normally...
Epoch 10/100, Loss: 0.1057
Epoch 20/100, Loss: 0.0367
Epoch 30/100, Loss: 0.0180
Epoch 40/100, Loss: 0.0093
Epoch 50/100, Loss: 0.0038
Early stopping at epoch 59
Saved probe to results/spam_gemma_9b/seed_44/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state.npz
Saved training log to results/spam_gemma_9b/seed_44/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state_train_log.json
  - 🔥 Probe state saved to train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Loaded probe from results/spam_gemma_9b/seed_43/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 5157
[DEBUG] Processing 1282 full activations
[DEBUG] Estimated memory requirement: 0.60 GB
[WARNING] Large memory requirement (0.60 GB).
[DEBUG] Max sequence length: 119
[DEBUG] Shape of first activation: (1, 8, 3584)
[DEBUG] Final array shape: (1282, 119, 3584)
[DEBUG] Returning (1282, 119, 3584) activations
[DEBUG] test activations shape: (1282, 119, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (1282, 119, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating on '87_is_spam': Got unsupported ScalarType BFloat16

=== Group 14/700 — ('94_better_spam', 20, 'resid_post', 45, '{"class_counts": {"0": 1750, "1": 1}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1751, 3584)
[DEBUG] Returning (1751, 3584) activations
[DEBUG] train activations shape: (1751, 3584), dtype: float16
[DEBUG] train activations range: [-1506.000000, 112.125000]
Train activations: (1751, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_mean
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1751, 3584)
Input y shape: (1751,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.86GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1751, 3584)
[DEBUG] Using pre-aggregated inputs: (1751, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.76it/s]
Loaded probe from results/spam_gemma_9b/seed_44/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 536 full activations
[DEBUG] Estimated memory requirement: 1.59 GB
[WARNING] Large memory requirement (1.59 GB).
[DEBUG] Max sequence length: 935
[DEBUG] Shape of first activation: (1, 56, 3584)
[DEBUG] Final array shape: (536, 935, 3584)
[DEBUG] Returning (536, 935, 3584) activations
[DEBUG] test activations shape: (536, 935, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (536, 935, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating validation on '94_better_spam': Got unsupported ScalarType BFloat16
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Final encoded shape: (1751, 262144)
Encoded feature matrix shape: (1751, 262144)
[DEBUG] After encoding. Memory: RAM: 8.57GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1751, 262144), y_train=(1751,)
[DEBUG] Memory at start of feature selection: RAM: 8.57GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1751, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.57GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.57GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.57GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1751, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Loaded probe from results/spam_gemma_9b/seed_44/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state.npz
Converted attention probe back to bfloat16 for training
Test activations: (536, 935, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating test on '94_better_spam': Got unsupported ScalarType BFloat16
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-677.000000, 50.625000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Loaded probe from results/spam_gemma_9b/seed_44/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 5157
[DEBUG] Processing 1282 full activations
[DEBUG] Estimated memory requirement: 0.60 GB
[WARNING] Large memory requirement (0.60 GB).
[DEBUG] Max sequence length: 163
[DEBUG] Shape of first activation: (1, 45, 3584)
[DEBUG] Final array shape: (1282, 163, 3584)
[DEBUG] Returning (1282, 163, 3584) activations
[DEBUG] test activations shape: (1282, 163, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (1282, 163, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating on '87_is_spam': Got unsupported ScalarType BFloat16

=== Group 15/700 — ('94_better_spam', 20, 'resid_post', 46, '{"class_counts": {"0": 1750, "1": 1}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.64it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.90it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.58it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5335820895522388, 'auc': 0.7417854756070393, 'precision': 0.95, 'recall': 0.0708955223880597, 'fpr': 0.0037313432835820895}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1751, 3584)
[DEBUG] Returning (1751, 3584) activations
[DEBUG] train activations shape: (1751, 3584), dtype: float16
[DEBUG] train activations range: [-1506.000000, 112.125000]
Train activations: (1751, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_mean
  [DEBUG] Fitting SAE probe normally...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1751, 3584)
Input y shape: (1751,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.71GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1751, 3584)
[DEBUG] Using pre-aggregated inputs: (1751, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.93it/s]
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.01it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.08it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.53it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5335820895522388, 'auc': 0.7417854756070393, 'precision': 0.95, 'recall': 0.0708955223880597, 'fpr': 0.0037313432835820895}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1505.000000, 110.875000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.22it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.66it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.72it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5023400936037441, 'auc': 0.7573409332629155, 'precision': 0.5714285714285714, 'recall': 0.0187207488299532, 'fpr': 0.014040561622464899}
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
[DEBUG] Final encoded shape: (1751, 262144)
Encoded feature matrix shape: (1751, 262144)
[DEBUG] After encoding. Memory: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1751, 262144), y_train=(1751,)
[DEBUG] Memory at start of feature selection: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1751, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1751, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1751, 3584)
[DEBUG] Returning (1751, 3584) activations
[DEBUG] train activations shape: (1751, 3584), dtype: float16
[DEBUG] train activations range: [-1337.000000, 237.000000]
Train activations: (1751, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_max
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-1053.000000, 68.750000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1751, 3584)
Input y shape: (1751,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.87GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1751, 3584)
[DEBUG] Using pre-aggregated inputs: (1751, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.74it/s]
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.98it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.33it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.50it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5130597014925373, 'auc': 0.7686845622633103, 'precision': 1.0, 'recall': 0.026119402985074626, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Final encoded shape: (1751, 262144)
Encoded feature matrix shape: (1751, 262144)
[DEBUG] After encoding. Memory: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1751, 262144), y_train=(1751,)
[DEBUG] Memory at start of feature selection: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1751, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1751, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-120.437500, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.02it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.63it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5130597014925373, 'auc': 0.7686845622633103, 'precision': 1.0, 'recall': 0.026119402985074626, 'fpr': 0.0}
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.02it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.85it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.11it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.8786617286700824, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1490.000000, 111.750000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.41it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.06it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.11it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.578003120124805, 'auc': 0.32978404939629724, 'precision': 1.0, 'recall': 0.15600624024961, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.02it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 33.64it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.66it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.8786617286700824, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1751, 3584)
[DEBUG] Returning (1751, 3584) activations
[DEBUG] train activations shape: (1751, 3584), dtype: float16
[DEBUG] train activations range: [-1337.000000, 237.000000]
Train activations: (1751, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_max
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1336.000000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1751, 3584)
Input y shape: (1751,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.71GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1751, 3584)
[DEBUG] Using pre-aggregated inputs: (1751, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.90it/s]
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.06it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.81it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.66it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.413878957654406, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
[DEBUG] Final encoded shape: (1751, 262144)
Encoded feature matrix shape: (1751, 262144)
[DEBUG] After encoding. Memory: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1751, 262144), y_train=(1751,)
[DEBUG] Memory at start of feature selection: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1751, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1751, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1751, 3584)
[DEBUG] Returning (1751, 3584) activations
[DEBUG] train activations shape: (1751, 3584), dtype: float16
[DEBUG] train activations range: [-1337.000000, 219.750000]
Train activations: (1751, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_last
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-162.375000, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1751, 3584)
Input y shape: (1751,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.87GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1751, 3584)
[DEBUG] Using pre-aggregated inputs: (1751, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.75it/s]
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.97it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.60it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.70it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5018656716417911, 'auc': 0.814797282245489, 'precision': 1.0, 'recall': 0.0037313432835820895, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Final encoded shape: (1751, 262144)
Encoded feature matrix shape: (1751, 262144)
[DEBUG] After encoding. Memory: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1751, 262144), y_train=(1751,)
[DEBUG] Memory at start of feature selection: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1751, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1751, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-227.500000, 81.187500]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.69it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.77it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5018656716417911, 'auc': 0.814797282245489, 'precision': 1.0, 'recall': 0.0037313432835820895, 'fpr': 0.0}
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.63it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.88it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.90it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5541044776119403, 'auc': 0.7200796391178436, 'precision': 1.0, 'recall': 0.10820895522388059, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1306.000000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.41it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.04it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.01it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.9104558254093034, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.61it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.89it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.11it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5541044776119403, 'auc': 0.7200796391178436, 'precision': 1.0, 'recall': 0.10820895522388059, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1336.000000, 221.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1751, 3584)
[DEBUG] Returning (1751, 3584) activations
[DEBUG] train activations shape: (1751, 3584), dtype: float16
[DEBUG] train activations range: [-1337.000000, 219.750000]
Train activations: (1751, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_last
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.13it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.70it/s]

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1751, 3584)
Input y shape: (1751,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.72GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1751, 3584)
[DEBUG] Using pre-aggregated inputs: (1751, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s][DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 16.59it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 16.55it/s]
Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 22.67it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5553822152886115, 'auc': 0.6962672890690978, 'precision': 0.961038961038961, 'recall': 0.11544461778471139, 'fpr': 0.0046801872074883}
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
[DEBUG] Final encoded shape: (1751, 262144)
Encoded feature matrix shape: (1751, 262144)
[DEBUG] After encoding. Memory: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1751, 262144), y_train=(1751,)
[DEBUG] Memory at start of feature selection: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1751, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1751, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1751, 3584)
[DEBUG] Returning (1751, 3584) activations
[DEBUG] train activations shape: (1751, 3584), dtype: float16
[DEBUG] train activations range: [-1337.000000, 237.000000]
Train activations: (1751, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_softmax
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-231.750000, 90.750000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1751, 3584)
Input y shape: (1751,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.87GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1751, 3584)
[DEBUG] Using pre-aggregated inputs: (1751, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.75it/s]
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.97it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.81it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.52it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5708955223880597, 'auc': 0.6816106036979281, 'precision': 1.0, 'recall': 0.1417910447761194, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Final encoded shape: (1751, 262144)
Encoded feature matrix shape: (1751, 262144)
[DEBUG] After encoding. Memory: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1751, 262144), y_train=(1751,)
[DEBUG] Memory at start of feature selection: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1751, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1751, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-120.625000, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.72it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.98it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5708955223880597, 'auc': 0.6816106036979281, 'precision': 1.0, 'recall': 0.1417910447761194, 'fpr': 0.0}
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.00it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.01it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.8810425484517709, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1306.000000, 211.500000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.42it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.05it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.18it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5616224648985959, 'auc': 0.703220640526089, 'precision': 0.9247311827956989, 'recall': 0.13416536661466458, 'fpr': 0.0109204368174727}
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.98it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.07it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.11it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.8810425484517709, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1751, 3584)
[DEBUG] Returning (1751, 3584) activations
[DEBUG] train activations shape: (1751, 3584), dtype: float16
[DEBUG] train activations range: [-1337.000000, 237.000000]
Train activations: (1751, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_softmax
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1336.000000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]
=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1751, 3584)
Input y shape: (1751,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.72GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1751, 3584)
[DEBUG] Using pre-aggregated inputs: (1751, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations:  50%|███████████████████████████████████████▌                                       | 1/2 [00:00<00:00,  7.96it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 15.30it/s]
Encoding activations:  50%|███████████████████████████████████████▌                                       | 1/2 [00:00<00:00,  7.76it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 13.20it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.66it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.61it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.47285710461179764, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: attention
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: full
[DEBUG] Final encoded shape: (1751, 262144)
Encoded feature matrix shape: (1751, 262144)
[DEBUG] After encoding. Memory: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1751, 262144), y_train=(1751,)
[DEBUG] Memory at start of feature selection: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1751, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1751, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-162.375000, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.06it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.07it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.74it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5018656716417911, 'auc': 0.8457200935620406, 'precision': 1.0, 'recall': 0.0037313432835820895, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 1751 full activations
[DEBUG] Estimated memory requirement: 3.26 GB
[WARNING] Large memory requirement (3.26 GB).
[DEBUG] Max sequence length: 501
[DEBUG] Shape of first activation: (1, 34, 3584)
[DEBUG] Final array shape: (1751, 501, 3584)
[DEBUG] Returning (1751, 501, 3584) activations
[DEBUG] train activations shape: (1751, 501, 3584), dtype: float16
[DEBUG] train activations range: [-1674.000000, 237.000000]
Train activations: (1751, 501, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: attention
  [DEBUG] Creating attention probe
  [DEBUG] Fitting attention probe normally...
Epoch 10/100, Loss: 0.1315
Epoch 20/100, Loss: 0.0464
Epoch 30/100, Loss: 0.0195
Epoch 40/100, Loss: 0.0096
Early stopping at epoch 44
Saved probe to results/spam_gemma_9b/seed_45/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state.npz
Saved training log to results/spam_gemma_9b/seed_45/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state_train_log.json
  - 🔥 Probe state saved to train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.04it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.08it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.49it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5018656716417911, 'auc': 0.8457200935620406, 'precision': 1.0, 'recall': 0.0037313432835820895, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1306.000000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.39it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.04it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.01it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.8708750222083766, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: attention
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: full
Loaded probe from results/spam_gemma_9b/seed_45/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 536 full activations
[DEBUG] Estimated memory requirement: 1.55 GB
[WARNING] Large memory requirement (1.55 GB).
[DEBUG] Max sequence length: 939
[DEBUG] Shape of first activation: (1, 61, 3584)
[DEBUG] Final array shape: (536, 939, 3584)
[DEBUG] Returning (536, 939, 3584) activations
[DEBUG] test activations shape: (536, 939, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (536, 939, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating validation on '94_better_spam': Got unsupported ScalarType BFloat16
    [BATCH] Evaluating on 94_better_spam test set
Loaded probe from results/spam_gemma_9b/seed_45/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state.npz
Converted attention probe back to bfloat16 for training
Test activations: (536, 939, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating test on '94_better_spam': Got unsupported ScalarType BFloat16
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 1751 full activations
[DEBUG] Estimated memory requirement: 3.30 GB
[WARNING] Large memory requirement (3.30 GB).
[DEBUG] Max sequence length: 520
[DEBUG] Shape of first activation: (1, 172, 3584)
[DEBUG] Final array shape: (1751, 520, 3584)
[DEBUG] Returning (1751, 520, 3584) activations
[DEBUG] train activations shape: (1751, 520, 3584), dtype: float16
[DEBUG] train activations range: [-1674.000000, 237.000000]
Train activations: (1751, 520, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: attention
  [DEBUG] Creating attention probe
  [DEBUG] Fitting attention probe normally...
Loaded probe from results/spam_gemma_9b/seed_45/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 5157
[DEBUG] Processing 1282 full activations
[DEBUG] Estimated memory requirement: 0.60 GB
[WARNING] Large memory requirement (0.60 GB).
[DEBUG] Max sequence length: 210
[DEBUG] Shape of first activation: (1, 65, 3584)
[DEBUG] Final array shape: (1282, 210, 3584)
[DEBUG] Returning (1282, 210, 3584) activations
[DEBUG] test activations shape: (1282, 210, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (1282, 210, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating on '87_is_spam': Got unsupported ScalarType BFloat16

=== Group 16/700 — ('94_better_spam', 20, 'resid_post', 47, '{"class_counts": {"0": 1750, "1": 1}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
Epoch 10/100, Loss: 0.1737
Epoch 20/100, Loss: 0.0534
Epoch 30/100, Loss: 0.0153
Epoch 40/100, Loss: 0.0034
Epoch 50/100, Loss: 0.0012
Early stopping at epoch 50
Saved probe to results/spam_gemma_9b/seed_46/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state.npz
Saved training log to results/spam_gemma_9b/seed_46/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state_train_log.json
  - 🔥 Probe state saved to train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1751, 3584)
[DEBUG] Returning (1751, 3584) activations
[DEBUG] train activations shape: (1751, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 231.875000]
Train activations: (1751, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1751, 3584)
Input y shape: (1751,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.81GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1751, 3584)
[DEBUG] Using pre-aggregated inputs: (1751, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 335.48it/s]
[DEBUG] Final encoded shape: (1751, 16384)
Encoded feature matrix shape: (1751, 16384)
[DEBUG] After encoding. Memory: RAM: 6.81GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1751, 16384), y_train=(1751,)
[DEBUG] Memory at start of feature selection: RAM: 6.81GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1751, 16384)
[DEBUG] Memory after feature matrix build: RAM: 6.81GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 6.81GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 6.81GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1751, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-237.375000, 80.312500]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 412.95it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 426.99it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 411.85it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5167910447761194, 'auc': 0.793369904210292, 'precision': 1.0, 'recall': 0.033582089552238806, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Loaded probe from results/spam_gemma_9b/seed_46/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 536 full activations
[DEBUG] Estimated memory requirement: 1.60 GB
[WARNING] Large memory requirement (1.60 GB).
[DEBUG] Max sequence length: 1167
[DEBUG] Shape of first activation: (1, 225, 3584)
[DEBUG] Final array shape: (536, 1167, 3584)
[DEBUG] Returning (536, 1167, 3584) activations
[DEBUG] test activations shape: (536, 1167, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (536, 1167, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating validation on '94_better_spam': Got unsupported ScalarType BFloat16
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Loaded probe from results/spam_gemma_9b/seed_46/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state.npz
Converted attention probe back to bfloat16 for training
Test activations: (536, 1167, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 360.55it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 399.80it/s]
    💀💀💀 ERROR evaluating test on '94_better_spam': Got unsupported ScalarType BFloat16
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 432.40it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5167910447761194, 'auc': 0.793369904210292, 'precision': 1.0, 'recall': 0.033582089552238806, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-268.250000, 76.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 447.11it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 418.05it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 415.79it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.8469483865158038, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
Loaded probe from results/spam_gemma_9b/seed_46/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 5157
[DEBUG] Processing 1282 full activations
[DEBUG] Estimated memory requirement: 0.60 GB
[WARNING] Large memory requirement (0.60 GB).
[DEBUG] Max sequence length: 163
[DEBUG] Shape of first activation: (1, 26, 3584)
[DEBUG] Final array shape: (1282, 163, 3584)
[DEBUG] Returning (1282, 163, 3584) activations
[DEBUG] test activations shape: (1282, 163, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (1282, 163, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating on '87_is_spam': Got unsupported ScalarType BFloat16

=== Group 17/700 — ('94_better_spam', 20, 'resid_post', 48, '{"class_counts": {"0": 1750, "1": 1}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1751, 3584)
[DEBUG] Returning (1751, 3584) activations
[DEBUG] train activations shape: (1751, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1751, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1751, 3584)
Input y shape: (1751,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.84GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1751, 3584)
[DEBUG] Using pre-aggregated inputs: (1751, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 331.16it/s]
[DEBUG] Final encoded shape: (1751, 16384)
Encoded feature matrix shape: (1751, 16384)
[DEBUG] After encoding. Memory: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1751, 16384), y_train=(1751,)
[DEBUG] Memory at start of feature selection: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1751, 16384)
[DEBUG] Memory after feature matrix build: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1751, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-567.000000, 51.500000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.02it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.97it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.93it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6305970149253731, 'auc': 0.8528207841390064, 'precision': 1.0, 'recall': 0.26119402985074625, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-106.500000, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 409.96it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 396.03it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 385.72it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.7018545333036311, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.02it/s]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.08it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.67it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6305970149253731, 'auc': 0.8528207841390064, 'precision': 1.0, 'recall': 0.26119402985074625, 'fpr': 0.0}
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 417.22it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 388.97it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 392.73it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.7018545333036311, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1505.000000, 110.875000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-177.375000, 237.500000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 445.82it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 429.28it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 409.50it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.6208512927100547, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.42it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.95it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.05it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.7600181074325657, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1751, 3584)
[DEBUG] Returning (1751, 3584) activations
[DEBUG] train activations shape: (1751, 3584), dtype: float16
[DEBUG] train activations range: [-1526.000000, 117.875000]
Train activations: (1751, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_mean
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1751, 3584)
[DEBUG] Returning (1751, 3584) activations
[DEBUG] train activations shape: (1751, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1751, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_max
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1751, 3584)
Input y shape: (1751,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.87GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1751, 3584)
[DEBUG] Using pre-aggregated inputs: (1751, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.72it/s]

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1751, 3584)
Input y shape: (1751,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.73GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1751, 3584)
[DEBUG] Using pre-aggregated inputs: (1751, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.90it/s]
[DEBUG] Final encoded shape: (1751, 262144)
Encoded feature matrix shape: (1751, 262144)
[DEBUG] After encoding. Memory: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1751, 262144), y_train=(1751,)
[DEBUG] Memory at start of feature selection: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1751, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1751, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Final encoded shape: (1751, 262144)
Encoded feature matrix shape: (1751, 262144)
[DEBUG] After encoding. Memory: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1751, 262144), y_train=(1751,)
[DEBUG] Memory at start of feature selection: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1751, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1751, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-106.125000, 237.500000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-570.000000, 51.906250]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s][DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 15.81it/s]
Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 15.29it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.85it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.20it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.39it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.28it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5055970149253731, 'auc': 0.912689351748719, 'precision': 1.0, 'recall': 0.011194029850746268, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.8509829583426153, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.01it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.64it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.67it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5055970149253731, 'auc': 0.912689351748719, 'precision': 1.0, 'recall': 0.011194029850746268, 'fpr': 0.0}
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.60it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.99it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.60it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.8509829583426153, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1336.000000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1049.000000, 65.937500]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.42it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.08it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 32.15it/s]
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.09it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.7868336574336608, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.80it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.67it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.734261258125832, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1751, 3584)
[DEBUG] Returning (1751, 3584) activations
[DEBUG] train activations shape: (1751, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 231.875000]
Train activations: (1751, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_last
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1751, 3584)
[DEBUG] Returning (1751, 3584) activations
[DEBUG] train activations shape: (1751, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1751, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_max
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1751, 3584)
Input y shape: (1751,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.74GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1751, 3584)
[DEBUG] Using pre-aggregated inputs: (1751, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.91it/s]

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1751, 3584)
Input y shape: (1751,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.86GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1751, 3584)
[DEBUG] Using pre-aggregated inputs: (1751, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.74it/s]
[DEBUG] Final encoded shape: (1751, 262144)
Encoded feature matrix shape: (1751, 262144)
[DEBUG] After encoding. Memory: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1751, 262144), y_train=(1751,)
[DEBUG] Memory at start of feature selection: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1751, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.45GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.45GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1751, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Final encoded shape: (1751, 262144)
Encoded feature matrix shape: (1751, 262144)
[DEBUG] After encoding. Memory: RAM: 8.57GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1751, 262144), y_train=(1751,)
[DEBUG] Memory at start of feature selection: RAM: 8.57GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1751, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.57GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.57GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.57GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1751, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-256.250000, 81.875000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-106.125000, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.02it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.75it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.93it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.542910447761194, 'auc': 0.49863555357540656, 'precision': 0.96, 'recall': 0.08955223880597014, 'fpr': 0.0037313432835820895}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.62it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.95it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.77it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.7016317665404321, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.04it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.64it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.90it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.542910447761194, 'auc': 0.49863555357540656, 'precision': 0.96, 'recall': 0.08955223880597014, 'fpr': 0.0037313432835820895}
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.65it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.83it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.92it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.7016317665404321, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1336.000000, 221.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-177.375000, 237.500000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.43it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.07it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.04it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.515600624024961, 'auc': 0.4202822715092691, 'precision': 0.6923076923076923, 'recall': 0.056162246489859596, 'fpr': 0.0249609984399376}
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.11it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.59it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.60it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.5590110031858372, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1751, 3584)
[DEBUG] Returning (1751, 3584) activations
[DEBUG] train activations shape: (1751, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1751, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_softmax
  [DEBUG] Fitting SAE probe normally...
Train activations: (1751, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1751, 3584)
Input y shape: (1751,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.73GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1751, 3584)
[DEBUG] Using pre-aggregated inputs: (1751, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.91it/s]

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1751, 3584)
Input y shape: (1751,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.86GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1751, 3584)
[DEBUG] Using pre-aggregated inputs: (1751, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.68it/s]
[DEBUG] Final encoded shape: (1751, 262144)
Encoded feature matrix shape: (1751, 262144)
[DEBUG] After encoding. Memory: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1751, 262144), y_train=(1751,)
[DEBUG] Memory at start of feature selection: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1751, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1751, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Final encoded shape: (1751, 262144)
Encoded feature matrix shape: (1751, 262144)
[DEBUG] After encoding. Memory: RAM: 8.57GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1751, 262144), y_train=(1751,)
[DEBUG] Memory at start of feature selection: RAM: 8.57GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1751, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.57GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.57GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.57GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1751, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-106.500000, 237.500000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.02it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.60it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.68it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5018656716417911, 'auc': 0.9097098462909333, 'precision': 1.0, 'recall': 0.0037313432835820895, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.66it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.82it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.82it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6119402985074627, 'auc': 0.7827745600356427, 'precision': 0.9838709677419355, 'recall': 0.22761194029850745, 'fpr': 0.0037313432835820895}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.02it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.71it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.68it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5018656716417911, 'auc': 0.9097098462909333, 'precision': 1.0, 'recall': 0.0037313432835820895, 'fpr': 0.0}
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.62it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.86it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.89it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6119402985074627, 'auc': 0.7827745600356427, 'precision': 0.9838709677419355, 'recall': 0.22761194029850745, 'fpr': 0.0037313432835820895}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1336.000000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.42it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.00it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.05it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.84269411338076, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: attention
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: full
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.09it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.64it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.72it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5538221528861155, 'auc': 0.7765727789798019, 'precision': 0.9156626506024096, 'recall': 0.11856474258970359, 'fpr': 0.0109204368174727}
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
Train activations: (1751, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1751, 3584)
Input y shape: (1751,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.87GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1751, 3584)
[DEBUG] Using pre-aggregated inputs: (1751, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.73it/s]
[DEBUG] Final encoded shape: (1751, 262144)
Encoded feature matrix shape: (1751, 262144)
[DEBUG] After encoding. Memory: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1751, 262144), y_train=(1751,)
[DEBUG] Memory at start of feature selection: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1751, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1751, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 1751 full activations
[DEBUG] Estimated memory requirement: 3.24 GB
[WARNING] Large memory requirement (3.24 GB).
[DEBUG] Max sequence length: 520
[DEBUG] Shape of first activation: (1, 198, 3584)
[DEBUG] Final array shape: (1751, 520, 3584)
[DEBUG] Returning (1751, 520, 3584) activations
[DEBUG] train activations shape: (1751, 520, 3584), dtype: float16
[DEBUG] train activations range: [-1674.000000, 237.000000]
Train activations: (1751, 520, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: attention
  [DEBUG] Creating attention probe
  [DEBUG] Fitting attention probe normally...
Epoch 10/100, Loss: 0.1569
Epoch 20/100, Loss: 0.0577
Epoch 30/100, Loss: 0.0289
Epoch 40/100, Loss: 0.0104
Epoch 50/100, Loss: 0.0020
Early stopping at epoch 56
Saved probe to results/spam_gemma_9b/seed_48/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state.npz
Saved training log to results/spam_gemma_9b/seed_48/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state_train_log.json
  - 🔥 Probe state saved to train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.63it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.93it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.68it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.6368205613722432, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.70it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.23it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.89it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.6368205613722432, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
Loaded probe from results/spam_gemma_9b/seed_48/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 536 full activations
[DEBUG] Estimated memory requirement: 1.63 GB
[WARNING] Large memory requirement (1.63 GB).
[DEBUG] Max sequence length: 939
[DEBUG] Shape of first activation: (1, 289, 3584)
[DEBUG] Final array shape: (536, 939, 3584)
[DEBUG] Returning (536, 939, 3584) activations
[DEBUG] test activations shape: (536, 939, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.500000]
Test activations: (536, 939, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating validation on '94_better_spam': Got unsupported ScalarType BFloat16
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.12it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.65it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.77it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.6031795093956644, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: attention
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: full
Loaded probe from results/spam_gemma_9b/seed_48/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state.npz
Converted attention probe back to bfloat16 for training
Test activations: (536, 939, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating test on '94_better_spam': Got unsupported ScalarType BFloat16
Loaded probe from results/spam_gemma_9b/seed_48/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 5157
[DEBUG] Processing 1282 full activations
[DEBUG] Estimated memory requirement: 0.60 GB
[WARNING] Large memory requirement (0.60 GB).
[DEBUG] Max sequence length: 210
[DEBUG] Shape of first activation: (1, 17, 3584)
[DEBUG] Final array shape: (1282, 210, 3584)
[DEBUG] Returning (1282, 210, 3584) activations
[DEBUG] test activations shape: (1282, 210, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (1282, 210, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating on '87_is_spam': Got unsupported ScalarType BFloat16

=== Group 18/700 — ('94_better_spam', 20, 'resid_post', 49, '{"class_counts": {"0": 1750, "1": 1}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-813.000000, 51.906250]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 1751 full activations
[DEBUG] Estimated memory requirement: 3.26 GB
[WARNING] Large memory requirement (3.26 GB).
[DEBUG] Max sequence length: 491
[DEBUG] Shape of first activation: (1, 74, 3584)
[DEBUG] Final array shape: (1751, 491, 3584)
[DEBUG] Returning (1751, 491, 3584) activations
[DEBUG] train activations shape: (1751, 491, 3584), dtype: float16
[DEBUG] train activations range: [-1674.000000, 237.000000]
Train activations: (1751, 491, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: attention
  [DEBUG] Creating attention probe
  [DEBUG] Fitting attention probe normally...
Epoch 10/100, Loss: 0.1651
Epoch 20/100, Loss: 0.0548
Epoch 30/100, Loss: 0.0263
Epoch 40/100, Loss: 0.0144
Epoch 50/100, Loss: 0.0057
Epoch 60/100, Loss: 0.0014
Early stopping at epoch 60
Saved probe to results/spam_gemma_9b/seed_47/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state.npz
Saved training log to results/spam_gemma_9b/seed_47/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state_train_log.json
  - 🔥 Probe state saved to train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.66it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.54it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5186567164179104, 'auc': 0.7194948763644464, 'precision': 1.0, 'recall': 0.03731343283582089, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.04it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.24it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.93it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5186567164179104, 'auc': 0.7194948763644464, 'precision': 1.0, 'recall': 0.03731343283582089, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1053.000000, 69.625000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.46it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.13it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Loaded probe from results/spam_gemma_9b/seed_47/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 536 full activations
[DEBUG] Estimated memory requirement: 1.60 GB
[WARNING] Large memory requirement (1.60 GB).
[DEBUG] Max sequence length: 939
[DEBUG] Shape of first activation: (1, 335, 3584)
[DEBUG] Final array shape: (536, 939, 3584)
[DEBUG] Returning (536, 939, 3584) activations
[DEBUG] test activations shape: (536, 939, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (536, 939, 3584)
  - 🤗 Calculating metrics...
Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 32.97it/s]
    💀💀💀 ERROR evaluating validation on '94_better_spam': Got unsupported ScalarType BFloat16
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.37172563345591547, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
Loaded probe from results/spam_gemma_9b/seed_47/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state.npz
Converted attention probe back to bfloat16 for training
Test activations: (536, 939, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating test on '94_better_spam': Got unsupported ScalarType BFloat16
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1751, 3584)
[DEBUG] Returning (1751, 3584) activations
[DEBUG] train activations shape: (1751, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1751, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_max
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1751, 3584)
Input y shape: (1751,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.73GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1751, 3584)
[DEBUG] Using pre-aggregated inputs: (1751, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.93it/s]
[DEBUG] Final encoded shape: (1751, 262144)
Encoded feature matrix shape: (1751, 262144)
[DEBUG] After encoding. Memory: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1751, 262144), y_train=(1751,)
[DEBUG] Memory at start of feature selection: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1751, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1751, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Loaded probe from results/spam_gemma_9b/seed_47/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 5157
[DEBUG] Processing 1282 full activations
[DEBUG] Estimated memory requirement: 0.59 GB
[WARNING] Large memory requirement (0.59 GB).
[DEBUG] Max sequence length: 210
[DEBUG] Shape of first activation: (1, 20, 3584)
[DEBUG] Final array shape: (1282, 210, 3584)
[DEBUG] Returning (1282, 210, 3584) activations
[DEBUG] test activations shape: (1282, 210, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.500000]
Test activations: (1282, 210, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating on '87_is_spam': Got unsupported ScalarType BFloat16

=== Group 19/700 — ('94_better_spam', 20, 'resid_post', 50, '{"class_counts": {"0": 1750, "1": 1}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-119.687500, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1751, 3584)
[DEBUG] Returning (1751, 3584) activations
[DEBUG] train activations shape: (1751, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 231.875000]
Train activations: (1751, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1751, 3584)
Input y shape: (1751,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.86GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1751, 3584)
[DEBUG] Using pre-aggregated inputs: (1751, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 334.74it/s]
[DEBUG] Final encoded shape: (1751, 16384)
Encoded feature matrix shape: (1751, 16384)
[DEBUG] After encoding. Memory: RAM: 6.86GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1751, 16384), y_train=(1751,)
[DEBUG] Memory at start of feature selection: RAM: 6.86GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1751, 16384)
[DEBUG] Memory after feature matrix build: RAM: 6.86GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 6.86GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 6.86GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1751, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.85it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.84it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5018656716417911, 'auc': 0.7132434840721766, 'precision': 1.0, 'recall': 0.0037313432835820895, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-256.250000, 90.750000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 416.31it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 412.05it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 388.54it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5111940298507462, 'auc': 0.739167966139452, 'precision': 0.875, 'recall': 0.026119402985074626, 'fpr': 0.0037313432835820895}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.01it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.86it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.84it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5018656716417911, 'auc': 0.7132434840721766, 'precision': 1.0, 'recall': 0.0037313432835820895, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-177.375000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 417.26it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 387.61it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 385.86it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5111940298507462, 'auc': 0.739167966139452, 'precision': 0.875, 'recall': 0.026119402985074626, 'fpr': 0.0037313432835820895}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-271.250000, 76.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 431.67it/s]
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s][DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 150.74it/s]
Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 26.51it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 419.20it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5109204368174727, 'auc': 0.6851667514438486, 'precision': 0.7916666666666666, 'recall': 0.029641185647425898, 'fpr': 0.0078003120124804995}
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.08it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.99it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.5683032313492228, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1751, 3584)
[DEBUG] Returning (1751, 3584) activations
[DEBUG] train activations shape: (1751, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1751, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_softmax
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1751, 3584)
[DEBUG] Returning (1751, 3584) activations
[DEBUG] train activations shape: (1751, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 231.875000]
Train activations: (1751, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1751, 3584)
Input y shape: (1751,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.86GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1751, 3584)
[DEBUG] Using pre-aggregated inputs: (1751, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 336.88it/s]
[DEBUG] Final encoded shape: (1751, 16384)
Encoded feature matrix shape: (1751, 16384)
[DEBUG] After encoding. Memory: RAM: 6.86GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1751, 16384), y_train=(1751,)
[DEBUG] Memory at start of feature selection: RAM: 6.86GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1751, 16384)
[DEBUG] Memory after feature matrix build: RAM: 6.86GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 6.86GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 6.86GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1751, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1751, 3584)
Input y shape: (1751,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.72GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1751, 3584)
[DEBUG] Using pre-aggregated inputs: (1751, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.91it/s]
[DEBUG] Final encoded shape: (1751, 262144)
Encoded feature matrix shape: (1751, 262144)
[DEBUG] After encoding. Memory: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1751, 262144), y_train=(1751,)
[DEBUG] Memory at start of feature selection: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1751, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1751, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-157.625000, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 413.72it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 390.06it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 389.81it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5130597014925373, 'auc': 0.8118873914012029, 'precision': 1.0, 'recall': 0.026119402985074626, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-210.625000, 80.875000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.01it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.69it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.60it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6044776119402985, 'auc': 0.6291211851191802, 'precision': 0.9666666666666667, 'recall': 0.21641791044776118, 'fpr': 0.007462686567164179}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 414.74it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 386.43it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 380.75it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5130597014925373, 'auc': 0.8118873914012029, 'precision': 1.0, 'recall': 0.026119402985074626, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-163.875000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 447.39it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 409.58it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 415.71it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5062402496099844, 'auc': 0.8499516891752114, 'precision': 0.75, 'recall': 0.0187207488299532, 'fpr': 0.0062402496099844}
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1751, 3584)
[DEBUG] Returning (1751, 3584) activations
[DEBUG] train activations shape: (1751, 3584), dtype: float16
[DEBUG] train activations range: [-1526.000000, 117.875000]
Train activations: (1751, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_mean
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.02it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.65it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.76it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6044776119402985, 'auc': 0.6291211851191802, 'precision': 0.9666666666666667, 'recall': 0.21641791044776118, 'fpr': 0.007462686567164179}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-263.750000, 76.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1751, 3584)
Input y shape: (1751,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.85GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1751, 3584)
[DEBUG] Using pre-aggregated inputs: (1751, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.71it/s]
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.39it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.02it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.97it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.48985959438377535, 'auc': 0.5876275612646971, 'precision': 0.06666666666666667, 'recall': 0.0015600624024961, 'fpr': 0.0218408736349454}
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
[DEBUG] Final encoded shape: (1751, 262144)
Encoded feature matrix shape: (1751, 262144)
[DEBUG] After encoding. Memory: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1751, 262144), y_train=(1751,)
[DEBUG] Memory at start of feature selection: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1751, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1751, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1751, 3584)
[DEBUG] Returning (1751, 3584) activations
[DEBUG] train activations shape: (1751, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1751, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_softmax
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-1067.000000, 71.062500]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1751, 3584)
Input y shape: (1751,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.72GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1751, 3584)
[DEBUG] Using pre-aggregated inputs: (1751, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.90it/s]
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.69it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.05it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.71it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5074626865671642, 'auc': 0.6338410559144576, 'precision': 1.0, 'recall': 0.014925373134328358, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Final encoded shape: (1751, 262144)
Encoded feature matrix shape: (1751, 262144)
[DEBUG] After encoding. Memory: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1751, 262144), y_train=(1751,)
[DEBUG] Memory at start of feature selection: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1751, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1751, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-119.750000, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.02it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.12it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.66it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5074626865671642, 'auc': 0.6338410559144576, 'precision': 1.0, 'recall': 0.014925373134328358, 'fpr': 0.0}
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.06it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.84it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.78it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5093283582089553, 'auc': 0.7654962129650257, 'precision': 1.0, 'recall': 0.018656716417910446, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1045.000000, 76.437500]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.06it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.57it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.57it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5039001560062403, 'auc': 0.19420464806111742, 'precision': 1.0, 'recall': 0.0078003120124804995, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.01it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.64it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.61it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5093283582089553, 'auc': 0.7654962129650257, 'precision': 1.0, 'recall': 0.018656716417910446, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-177.375000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1751, 3584)
[DEBUG] Returning (1751, 3584) activations
[DEBUG] train activations shape: (1751, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1751, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_max
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.41it/s]

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1751, 3584)
Input y shape: (1751,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.85GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1751, 3584)
[DEBUG] Using pre-aggregated inputs: (1751, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s][DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 14.26it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 14.23it/s]
Encoding activations:  50%|███████████████████████████████████████▌                                       | 1/2 [00:00<00:00,  9.52it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 18.76it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.98it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.5475575653291342, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: attention
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: full
[DEBUG] Final encoded shape: (1751, 262144)
Encoded feature matrix shape: (1751, 262144)
[DEBUG] After encoding. Memory: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1751, 262144), y_train=(1751,)
[DEBUG] Memory at start of feature selection: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1751, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1751, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-157.625000, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.02it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.86it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.57it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.503731343283582, 'auc': 0.7460876587213188, 'precision': 1.0, 'recall': 0.007462686567164179, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 1751 full activations
[DEBUG] Estimated memory requirement: 3.28 GB
[WARNING] Large memory requirement (3.28 GB).
[DEBUG] Max sequence length: 520
[DEBUG] Shape of first activation: (1, 313, 3584)
[DEBUG] Final array shape: (1751, 520, 3584)
[DEBUG] Returning (1751, 520, 3584) activations
[DEBUG] train activations shape: (1751, 520, 3584), dtype: float16
[DEBUG] train activations range: [-1674.000000, 237.000000]
Train activations: (1751, 520, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: attention
  [DEBUG] Creating attention probe
  [DEBUG] Fitting attention probe normally...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.93it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 20.16it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Epoch 10/100, Loss: 0.1126
Epoch 20/100, Loss: 0.0387
Epoch 30/100, Loss: 0.0204
Epoch 40/100, Loss: 0.0088
Early stopping at epoch 48
Saved probe to results/spam_gemma_9b/seed_49/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state.npz
Saved training log to results/spam_gemma_9b/seed_49/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state_train_log.json
  - 🔥 Probe state saved to train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.15it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.503731343283582, 'auc': 0.7460876587213188, 'precision': 1.0, 'recall': 0.007462686567164179, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-163.875000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.10it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.62it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.70it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.8675455910592116, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
Train activations: (1751, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1751, 3584)
Input y shape: (1751,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.85GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1751, 3584)
[DEBUG] Using pre-aggregated inputs: (1751, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.77it/s]
[DEBUG] Final encoded shape: (1751, 262144)
Encoded feature matrix shape: (1751, 262144)
[DEBUG] After encoding. Memory: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1751, 262144), y_train=(1751,)
[DEBUG] Memory at start of feature selection: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1751, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1751, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Loaded probe from results/spam_gemma_9b/seed_49/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 536 full activations
[DEBUG] Estimated memory requirement: 1.55 GB
[WARNING] Large memory requirement (1.55 GB).
[DEBUG] Max sequence length: 1167
[DEBUG] Shape of first activation: (1, 171, 3584)
[DEBUG] Final array shape: (536, 1167, 3584)
[DEBUG] Returning (536, 1167, 3584) activations
[DEBUG] test activations shape: (536, 1167, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (536, 1167, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating validation on '94_better_spam': Got unsupported ScalarType BFloat16
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Loaded probe from results/spam_gemma_9b/seed_49/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state.npz
Converted attention probe back to bfloat16 for training
Test activations: (536, 1167, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating test on '94_better_spam': Got unsupported ScalarType BFloat16
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.73it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.05it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5373134328358209, 'auc': 0.6976776564936511, 'precision': 0.9166666666666666, 'recall': 0.08208955223880597, 'fpr': 0.007462686567164179}
    [BATCH] Evaluating on 94_better_spam test set
Loaded probe from results/spam_gemma_9b/seed_49/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 5157
[DEBUG] Processing 1282 full activations
[DEBUG] Estimated memory requirement: 0.60 GB
[WARNING] Large memory requirement (0.60 GB).
[DEBUG] Max sequence length: 215
[DEBUG] Shape of first activation: (1, 33, 3584)
[DEBUG] Final array shape: (1282, 215, 3584)
[DEBUG] Returning (1282, 215, 3584) activations
[DEBUG] test activations shape: (1282, 215, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (1282, 215, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating on '87_is_spam': Got unsupported ScalarType BFloat16

=== Group 20/700 — ('94_better_spam', 20, 'resid_post', 51, '{"class_counts": {"0": 1750, "1": 1}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1751, 3584)
[DEBUG] Returning (1751, 3584) activations
[DEBUG] train activations shape: (1751, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1751, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1751, 3584)
Input y shape: (1751,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.72GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1751, 3584)
[DEBUG] Using pre-aggregated inputs: (1751, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 338.44it/s]
[DEBUG] Final encoded shape: (1751, 16384)
Encoded feature matrix shape: (1751, 16384)
[DEBUG] After encoding. Memory: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1751, 16384), y_train=(1751,)
[DEBUG] Memory at start of feature selection: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1751, 16384)
[DEBUG] Memory after feature matrix build: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1751, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.76it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.58it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.14it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5373134328358209, 'auc': 0.6976776564936511, 'precision': 0.9166666666666666, 'recall': 0.08208955223880597, 'fpr': 0.007462686567164179}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.08it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.67it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.56it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5280811232449298, 'auc': 0.7070952416879827, 'precision': 0.875, 'recall': 0.0655226209048362, 'fpr': 0.0093603744149766}
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-119.187500, 237.500000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 420.52it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 387.21it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 386.39it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5186567164179104, 'auc': 0.8492286700824236, 'precision': 1.0, 'recall': 0.03731343283582089, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Train activations: (1751, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_softmax
  [DEBUG] Fitting SAE probe normally...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 416.47it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 412.38it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 393.50it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5186567164179104, 'auc': 0.8492286700824236, 'precision': 1.0, 'recall': 0.03731343283582089, 'fpr': 0.0}

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1751, 3584)
Input y shape: (1751,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.85GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1751, 3584)
[DEBUG] Using pre-aggregated inputs: (1751, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.70it/s]
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1371.000000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 428.14it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 411.53it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 388.25it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.6349380964318136, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
[DEBUG] Final encoded shape: (1751, 262144)
Encoded feature matrix shape: (1751, 262144)
[DEBUG] After encoding. Memory: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1751, 262144), y_train=(1751,)
[DEBUG] Memory at start of feature selection: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1751, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1751, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1751, 3584)
[DEBUG] Returning (1751, 3584) activations
[DEBUG] train activations shape: (1751, 3584), dtype: float16
[DEBUG] train activations range: [-1526.000000, 117.875000]
Train activations: (1751, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_mean
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.70it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 33.01it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.96it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.503731343283582, 'auc': 0.771886834484295, 'precision': 1.0, 'recall': 0.007462686567164179, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1751, 3584)
Input y shape: (1751,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.72GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1751, 3584)
[DEBUG] Using pre-aggregated inputs: (1751, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.91it/s]
[DEBUG] Final encoded shape: (1751, 262144)
Encoded feature matrix shape: (1751, 262144)
[DEBUG] After encoding. Memory: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1751, 262144), y_train=(1751,)
[DEBUG] Memory at start of feature selection: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1751, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1751, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.65it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.11it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.87it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.503731343283582, 'auc': 0.771886834484295, 'precision': 1.0, 'recall': 0.007462686567164179, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-782.000000, 52.031250]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.13it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.57it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.58it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.8590029716633285, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: attention
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: full
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.04it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.75it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.72it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5242537313432836, 'auc': 0.6733125417687681, 'precision': 1.0, 'recall': 0.048507462686567165, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.02it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.04it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.02it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5242537313432836, 'auc': 0.6733125417687681, 'precision': 1.0, 'recall': 0.048507462686567165, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1522.000000, 113.625000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.43it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.12it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.14it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.1457404942063517, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 1751 full activations
[DEBUG] Estimated memory requirement: 3.28 GB
[WARNING] Large memory requirement (3.28 GB).
[DEBUG] Max sequence length: 501
[DEBUG] Shape of first activation: (1, 223, 3584)
[DEBUG] Final array shape: (1751, 501, 3584)
[DEBUG] Returning (1751, 501, 3584) activations
[DEBUG] train activations shape: (1751, 501, 3584), dtype: float16
[DEBUG] train activations range: [-1674.000000, 237.000000]
Train activations: (1751, 501, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: attention
  [DEBUG] Creating attention probe
  [DEBUG] Fitting attention probe normally...
Epoch 10/100, Loss: 0.1148
Epoch 20/100, Loss: 0.0339
Epoch 30/100, Loss: 0.0140
Epoch 40/100, Loss: 0.0043
Epoch 50/100, Loss: 0.0011
Early stopping at epoch 52
Saved probe to results/spam_gemma_9b/seed_50/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state.npz
Saved training log to results/spam_gemma_9b/seed_50/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state_train_log.json
  - 🔥 Probe state saved to train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1751, 3584)
[DEBUG] Returning (1751, 3584) activations
[DEBUG] train activations shape: (1751, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1751, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_max
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1751, 3584)
Input y shape: (1751,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.72GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1751, 3584)
[DEBUG] Using pre-aggregated inputs: (1751, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.91it/s]
[DEBUG] Final encoded shape: (1751, 262144)
Encoded feature matrix shape: (1751, 262144)
[DEBUG] After encoding. Memory: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1751, 262144), y_train=(1751,)
[DEBUG] Memory at start of feature selection: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1751, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1751, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-118.875000, 237.500000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Loaded probe from results/spam_gemma_9b/seed_50/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 536 full activations
[DEBUG] Estimated memory requirement: 1.58 GB
[WARNING] Large memory requirement (1.58 GB).
[DEBUG] Max sequence length: 1167
[DEBUG] Shape of first activation: (1, 106, 3584)
[DEBUG] Final array shape: (536, 1167, 3584)
[DEBUG] Returning (536, 1167, 3584) activations
[DEBUG] test activations shape: (536, 1167, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (536, 1167, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating validation on '94_better_spam': Got unsupported ScalarType BFloat16
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.78it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.67it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5186567164179104, 'auc': 0.8432139674760526, 'precision': 1.0, 'recall': 0.03731343283582089, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Loaded probe from results/spam_gemma_9b/seed_50/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state.npz
Converted attention probe back to bfloat16 for training
Test activations: (536, 1167, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating test on '94_better_spam': Got unsupported ScalarType BFloat16
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.06it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.08it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.94it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5186567164179104, 'auc': 0.8432139674760526, 'precision': 1.0, 'recall': 0.03731343283582089, 'fpr': 0.0}
Loaded probe from results/spam_gemma_9b/seed_50/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 5157
[DEBUG] Processing 1282 full activations
[DEBUG] Estimated memory requirement: 0.60 GB
[WARNING] Large memory requirement (0.60 GB).
[DEBUG] Max sequence length: 210
[DEBUG] Shape of first activation: (1, 19, 3584)
[DEBUG] Final array shape: (1282, 210, 3584)
[DEBUG] Returning (1282, 210, 3584) activations
[DEBUG] test activations shape: (1282, 210, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (1282, 210, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating on '87_is_spam': Got unsupported ScalarType BFloat16

=== Group 21/700 — ('94_better_spam', 20, 'resid_post', 42, '{"class_counts": {"0": 1750, "1": 2}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1371.000000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.40it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.17it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.05it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.6655381971909141, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-1337.000000, 219.750000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 415.03it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 381.34it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 386.22it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6660447761194029, 'auc': 0.8809729338382715, 'precision': 0.989010989010989, 'recall': 0.3358208955223881, 'fpr': 0.0037313432835820895}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-259.000000, 76.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 446.58it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 407.21it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 436.43it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.500780031201248, 'auc': 0.6361306558346578, 'precision': 0.5384615384615384, 'recall': 0.0109204368174727, 'fpr': 0.0093603744149766}
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1751, 3584)
[DEBUG] Returning (1751, 3584) activations
[DEBUG] train activations shape: (1751, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 231.875000]
Train activations: (1751, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1751, 3584)
Input y shape: (1751,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.72GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1751, 3584)
[DEBUG] Using pre-aggregated inputs: (1751, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.93it/s]
[DEBUG] Final encoded shape: (1751, 262144)
Encoded feature matrix shape: (1751, 262144)
[DEBUG] After encoding. Memory: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1751, 262144), y_train=(1751,)
[DEBUG] Memory at start of feature selection: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1751, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1751, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1752, 3584)
[DEBUG] Returning (1752, 3584) activations
[DEBUG] train activations shape: (1752, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1752, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1752, 3584)
Input y shape: (1752,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.85GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1752, 3584)
[DEBUG] Using pre-aggregated inputs: (1752, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 334.45it/s]
[DEBUG] Final encoded shape: (1752, 16384)
Encoded feature matrix shape: (1752, 16384)
[DEBUG] After encoding. Memory: RAM: 6.85GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1752, 16384), y_train=(1752,)
[DEBUG] Memory at start of feature selection: RAM: 6.85GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1752, 16384)
[DEBUG] Memory after feature matrix build: RAM: 6.85GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 6.85GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 6.85GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1752, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-210.750000, 80.687500]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-1337.000000, 237.500000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 411.77it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 375.36it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 425.69it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5074626865671642, 'auc': 0.9356482512809089, 'precision': 1.0, 'recall': 0.014925373134328358, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.64it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.74it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5746268656716418, 'auc': 0.7592169748273558, 'precision': 0.9761904761904762, 'recall': 0.15298507462686567, 'fpr': 0.0037313432835820895}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 413.72it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 390.79it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 381.06it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5074626865671642, 'auc': 0.9356482512809089, 'precision': 1.0, 'recall': 0.014925373134328358, 'fpr': 0.0}
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-150.125000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 436.79it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 428.51it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 427.64it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.8076085289901455, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.01it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.71it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5746268656716418, 'auc': 0.7592169748273558, 'precision': 0.9761904761904762, 'recall': 0.15298507462686567, 'fpr': 0.0037313432835820895}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1371.000000, 229.375000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.39it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.02it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.02it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5920436817472698, 'auc': 0.8311165519943731, 'precision': 1.0, 'recall': 0.18408736349453977, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1752, 3584)
[DEBUG] Returning (1752, 3584) activations
[DEBUG] train activations shape: (1752, 3584), dtype: float16
[DEBUG] train activations range: [-1526.000000, 117.875000]
Train activations: (1752, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_mean
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1752, 3584)
Input y shape: (1752,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.87GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1752, 3584)
[DEBUG] Using pre-aggregated inputs: (1752, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.57it/s]
Train activations: (1751, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_softmax
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Final encoded shape: (1752, 262144)
Encoded feature matrix shape: (1752, 262144)
[DEBUG] After encoding. Memory: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1752, 262144), y_train=(1752,)
[DEBUG] Memory at start of feature selection: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1752, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1752, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1751, 3584)
Input y shape: (1751,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.72GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1751, 3584)
[DEBUG] Using pre-aggregated inputs: (1751, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.92it/s]
[DEBUG] Final encoded shape: (1751, 262144)
Encoded feature matrix shape: (1751, 262144)
[DEBUG] After encoding. Memory: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1751, 262144), y_train=(1751,)
[DEBUG] Memory at start of feature selection: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1751, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1751, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-1506.000000, 112.125000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.62it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.16it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.83it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5541044776119403, 'auc': 0.923549231454667, 'precision': 1.0, 'recall': 0.10820895522388059, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.91it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.97it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5167910447761194, 'auc': 0.8273557585208287, 'precision': 1.0, 'recall': 0.033582089552238806, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.62it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.92it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.09it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5541044776119403, 'auc': 0.923549231454667, 'precision': 1.0, 'recall': 0.10820895522388059, 'fpr': 0.0}
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1063.000000, 70.687500]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.75it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.77it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5167910447761194, 'auc': 0.8273557585208287, 'precision': 1.0, 'recall': 0.033582089552238806, 'fpr': 0.0}
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.15it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.77it/s]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.79it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.3905364326897569, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.40it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.17it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.02it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.525203161012556, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: attention
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: full
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1752, 3584)
[DEBUG] Returning (1752, 3584) activations
[DEBUG] train activations shape: (1752, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1752, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_max
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1752, 3584)
Input y shape: (1752,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.88GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1752, 3584)
[DEBUG] Using pre-aggregated inputs: (1752, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.58it/s]
[DEBUG] Final encoded shape: (1752, 262144)
Encoded feature matrix shape: (1752, 262144)
[DEBUG] After encoding. Memory: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1752, 262144), y_train=(1752,)
[DEBUG] Memory at start of feature selection: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1752, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1752, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-1337.000000, 237.500000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.70it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.04it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.88it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.503731343283582, 'auc': 0.9497521719759412, 'precision': 1.0, 'recall': 0.007462686567164179, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 1751 full activations
[DEBUG] Estimated memory requirement: 3.32 GB
[WARNING] Large memory requirement (3.32 GB).
[DEBUG] Max sequence length: 520
[DEBUG] Shape of first activation: (1, 100, 3584)
[DEBUG] Final array shape: (1751, 520, 3584)
[DEBUG] Returning (1751, 520, 3584) activations
[DEBUG] train activations shape: (1751, 520, 3584), dtype: float16
[DEBUG] train activations range: [-1674.000000, 237.000000]
Train activations: (1751, 520, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: attention
  [DEBUG] Creating attention probe
  [DEBUG] Fitting attention probe normally...
Epoch 10/100, Loss: 0.1416
Epoch 20/100, Loss: 0.0403
Epoch 30/100, Loss: 0.0131
Epoch 40/100, Loss: 0.0028
Early stopping at epoch 49
Saved probe to results/spam_gemma_9b/seed_51/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state.npz
Saved training log to results/spam_gemma_9b/seed_51/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state_train_log.json
  - 🔥 Probe state saved to train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.62it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.78it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.92it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.503731343283582, 'auc': 0.9497521719759412, 'precision': 1.0, 'recall': 0.007462686567164179, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-150.125000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.08it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.89it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.69it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.8929398049556927, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
Loaded probe from results/spam_gemma_9b/seed_51/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 536 full activations
[DEBUG] Estimated memory requirement: 1.54 GB
[WARNING] Large memory requirement (1.54 GB).
[DEBUG] Max sequence length: 1167
[DEBUG] Shape of first activation: (1, 121, 3584)
[DEBUG] Final array shape: (536, 1167, 3584)
[DEBUG] Returning (536, 1167, 3584) activations
[DEBUG] test activations shape: (536, 1167, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.500000]
Test activations: (536, 1167, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating validation on '94_better_spam': Got unsupported ScalarType BFloat16
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1752, 3584)
[DEBUG] Returning (1752, 3584) activations
[DEBUG] train activations shape: (1752, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 231.875000]
Train activations: (1752, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1752, 3584)
Input y shape: (1752,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.88GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1752, 3584)
[DEBUG] Using pre-aggregated inputs: (1752, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.58it/s]
Loaded probe from results/spam_gemma_9b/seed_51/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state.npz
Converted attention probe back to bfloat16 for training
Test activations: (536, 1167, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating test on '94_better_spam': Got unsupported ScalarType BFloat16
[DEBUG] Final encoded shape: (1752, 262144)
Encoded feature matrix shape: (1752, 262144)
[DEBUG] After encoding. Memory: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1752, 262144), y_train=(1752,)
[DEBUG] Memory at start of feature selection: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1752, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1752, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Loaded probe from results/spam_gemma_9b/seed_51/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_1_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 5157
[DEBUG] Processing 1282 full activations
[DEBUG] Estimated memory requirement: 0.60 GB
[WARNING] Large memory requirement (0.60 GB).
[DEBUG] Max sequence length: 99
[DEBUG] Shape of first activation: (1, 38, 3584)
[DEBUG] Final array shape: (1282, 99, 3584)
[DEBUG] Returning (1282, 99, 3584) activations
[DEBUG] test activations shape: (1282, 99, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (1282, 99, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating on '87_is_spam': Got unsupported ScalarType BFloat16

=== Group 22/700 — ('94_better_spam', 20, 'resid_post', 43, '{"class_counts": {"0": 1750, "1": 2}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1752, 3584)
[DEBUG] Returning (1752, 3584) activations
[DEBUG] train activations shape: (1752, 3584), dtype: float16
[DEBUG] train activations range: [-162.375000, 237.000000]
Train activations: (1752, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_softmax
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.64it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]
=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1752, 3584)
Input y shape: (1752,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.72GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1752, 3584)
[DEBUG] Using pre-aggregated inputs: (1752, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 149.61it/s]
Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 30.04it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.20it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.753731343283582, 'auc': 0.7864919803965249, 'precision': 0.9788732394366197, 'recall': 0.5186567164179104, 'fpr': 0.011194029850746268}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Final encoded shape: (1752, 16384)
Encoded feature matrix shape: (1752, 16384)
[DEBUG] After encoding. Memory: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1752, 16384), y_train=(1752,)
[DEBUG] Memory at start of feature selection: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1752, 16384)
[DEBUG] Memory after feature matrix build: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1752, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-1337.000000, 237.500000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 413.03it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 390.46it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 389.01it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.503731343283582, 'auc': 0.921474715972377, 'precision': 1.0, 'recall': 0.007462686567164179, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.61it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.04it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.09it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.753731343283582, 'auc': 0.7864919803965249, 'precision': 0.9788732394366197, 'recall': 0.5186567164179104, 'fpr': 0.011194029850746268}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.09it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.52it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.60it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5663026521060842, 'auc': 0.5707710991747003, 'precision': 0.8148148148148148, 'recall': 0.17160686427457097, 'fpr': 0.0390015600624025}
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 418.47it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 389.66it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 388.04it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.503731343283582, 'auc': 0.921474715972377, 'precision': 1.0, 'recall': 0.007462686567164179, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-147.875000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 443.98it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 427.88it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 416.14it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.8921634244464943, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
Train activations: (1752, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_softmax
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1752, 3584)
[DEBUG] Returning (1752, 3584) activations
[DEBUG] train activations shape: (1752, 3584), dtype: float16
[DEBUG] train activations range: [-1067.000000, 73.875000]
Train activations: (1752, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_mean
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1752, 3584)
Input y shape: (1752,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.88GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1752, 3584)
[DEBUG] Using pre-aggregated inputs: (1752, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.61it/s]
[DEBUG] Final encoded shape: (1752, 262144)
Encoded feature matrix shape: (1752, 262144)
[DEBUG] After encoding. Memory: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1752, 262144), y_train=(1752,)
[DEBUG] Memory at start of feature selection: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1752, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1752, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1752, 3584)
Input y shape: (1752,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.72GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1752, 3584)
[DEBUG] Using pre-aggregated inputs: (1752, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.91it/s]
[DEBUG] Final encoded shape: (1752, 262144)
Encoded feature matrix shape: (1752, 262144)
[DEBUG] After encoding. Memory: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1752, 262144), y_train=(1752,)
[DEBUG] Memory at start of feature selection: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1752, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1752, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.63it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.10it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.79it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.503731343283582, 'auc': 0.9565883270216083, 'precision': 1.0, 'recall': 0.007462686567164179, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-1506.000000, 112.125000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.02it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.94it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.97it/s]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.621268656716418, 'auc': 0.9383632212073957, 'precision': 0.9850746268656716, 'recall': 0.2462686567164179, 'fpr': 0.0037313432835820895}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.61it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.04it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.79it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.503731343283582, 'auc': 0.9565883270216083, 'precision': 1.0, 'recall': 0.007462686567164179, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.06it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.51it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.51it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.9024583760261486, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: attention
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: full
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.00it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.76it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.65it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.621268656716418, 'auc': 0.9383632212073957, 'precision': 0.9850746268656716, 'recall': 0.2462686567164179, 'fpr': 0.0037313432835820895}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1063.000000, 70.687500]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.41it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.06it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.99it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.6878366242293997, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1752, 3584)
[DEBUG] Returning (1752, 3584) activations
[DEBUG] train activations shape: (1752, 3584), dtype: float16
[DEBUG] train activations range: [-162.375000, 237.000000]
Train activations: (1752, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_max
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1752, 3584)
Input y shape: (1752,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.72GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1752, 3584)
[DEBUG] Using pre-aggregated inputs: (1752, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.95it/s]
[DEBUG] Final encoded shape: (1752, 262144)
Encoded feature matrix shape: (1752, 262144)
[DEBUG] After encoding. Memory: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1752, 262144), y_train=(1752,)
[DEBUG] Memory at start of feature selection: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1752, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1752, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 1752 full activations
[DEBUG] Estimated memory requirement: 3.29 GB
[WARNING] Large memory requirement (3.29 GB).
[DEBUG] Max sequence length: 520
[DEBUG] Shape of first activation: (1, 70, 3584)
[DEBUG] Final array shape: (1752, 520, 3584)
[DEBUG] Returning (1752, 520, 3584) activations
[DEBUG] train activations shape: (1752, 520, 3584), dtype: float16
[DEBUG] train activations range: [-1674.000000, 237.000000]
Train activations: (1752, 520, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: attention
  [DEBUG] Creating attention probe
  [DEBUG] Fitting attention probe normally...
Epoch 10/100, Loss: 0.1425
Epoch 20/100, Loss: 0.0486
Epoch 30/100, Loss: 0.0222
Epoch 40/100, Loss: 0.0141
Epoch 50/100, Loss: 0.0084
Early stopping at epoch 52
Saved probe to results/spam_gemma_9b/seed_42/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state.npz
Saved training log to results/spam_gemma_9b/seed_42/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state_train_log.json
  - 🔥 Probe state saved to train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-1337.000000, 237.500000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.62it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.01it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5055970149253731, 'auc': 0.9298702383604366, 'precision': 1.0, 'recall': 0.011194029850746268, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Loaded probe from results/spam_gemma_9b/seed_42/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 536 full activations
[DEBUG] Estimated memory requirement: 1.58 GB
[WARNING] Large memory requirement (1.58 GB).
[DEBUG] Max sequence length: 939
[DEBUG] Shape of first activation: (1, 151, 3584)
[DEBUG] Final array shape: (536, 939, 3584)
[DEBUG] Returning (536, 939, 3584) activations
[DEBUG] test activations shape: (536, 939, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.500000]
Test activations: (536, 939, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating validation on '94_better_spam': Got unsupported ScalarType BFloat16
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.04it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.54it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.65it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5055970149253731, 'auc': 0.9298702383604366, 'precision': 1.0, 'recall': 0.011194029850746268, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-147.875000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Loaded probe from results/spam_gemma_9b/seed_42/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state.npz
Converted attention probe back to bfloat16 for training
Test activations: (536, 939, 3584)
  - 🤗 Calculating metrics...
Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 26.75it/s]
    💀💀💀 ERROR evaluating test on '94_better_spam': Got unsupported ScalarType BFloat16
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.99it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.05it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.8624613939315763, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
Loaded probe from results/spam_gemma_9b/seed_42/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 5157
[DEBUG] Processing 1282 full activations
[DEBUG] Estimated memory requirement: 0.60 GB
[WARNING] Large memory requirement (0.60 GB).
[DEBUG] Max sequence length: 210
[DEBUG] Shape of first activation: (1, 54, 3584)
[DEBUG] Final array shape: (1282, 210, 3584)
[DEBUG] Returning (1282, 210, 3584) activations
[DEBUG] test activations shape: (1282, 210, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (1282, 210, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating on '87_is_spam': Got unsupported ScalarType BFloat16

=== Group 23/700 — ('94_better_spam', 20, 'resid_post', 44, '{"class_counts": {"0": 1750, "1": 2}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1752, 3584)
[DEBUG] Returning (1752, 3584) activations
[DEBUG] train activations shape: (1752, 3584), dtype: float16
[DEBUG] train activations range: [-256.250000, 92.687500]
Train activations: (1752, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1752, 3584)
Input y shape: (1752,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.72GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1752, 3584)
[DEBUG] Using pre-aggregated inputs: (1752, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.89it/s]
[DEBUG] Final encoded shape: (1752, 262144)
Encoded feature matrix shape: (1752, 262144)
[DEBUG] After encoding. Memory: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1752, 262144), y_train=(1752,)
[DEBUG] Memory at start of feature selection: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1752, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1752, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-109.562500, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 416.10it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 379.30it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 382.06it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5111940298507462, 'auc': 0.9249276008019603, 'precision': 1.0, 'recall': 0.022388059701492536, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-1337.000000, 219.750000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 410.40it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 384.83it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 387.86it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5111940298507462, 'auc': 0.9249276008019603, 'precision': 1.0, 'recall': 0.022388059701492536, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-143.875000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 436.91it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 415.98it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 408.26it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.7572508828590273, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.02it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.59it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.59it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5708955223880597, 'auc': 0.7265677210960124, 'precision': 0.975, 'recall': 0.1455223880597015, 'fpr': 0.0037313432835820895}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1752, 3584)
[DEBUG] Returning (1752, 3584) activations
[DEBUG] train activations shape: (1752, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 231.875000]
Train activations: (1752, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1752, 3584)
Input y shape: (1752,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.87GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1752, 3584)
[DEBUG] Using pre-aggregated inputs: (1752, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 337.28it/s]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Final encoded shape: (1752, 16384)
Encoded feature matrix shape: (1752, 16384)
[DEBUG] After encoding. Memory: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1752, 16384), y_train=(1752,)
[DEBUG] Memory at start of feature selection: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1752, 16384)
[DEBUG] Memory after feature matrix build: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1752, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.02it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.71it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.58it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5708955223880597, 'auc': 0.7265677210960124, 'precision': 0.975, 'recall': 0.1455223880597015, 'fpr': 0.0037313432835820895}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-252.250000, 76.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.43it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s][DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-238.250000, 80.812500]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.04it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.14it/s]
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 406.62it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 400.07it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s][DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5046801872074883, 'auc': 0.6139417495576578, 'precision': 0.5535714285714286, 'recall': 0.0483619344773791, 'fpr': 0.0390015600624025}
Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 214.33it/s]
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5447761194029851, 'auc': 0.8541852305635997, 'precision': 0.9615384615384616, 'recall': 0.09328358208955224, 'fpr': 0.0037313432835820895}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Train activations: (1752, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_softmax
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 409.60it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 389.15it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 387.75it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5447761194029851, 'auc': 0.8541852305635997, 'precision': 0.9615384615384616, 'recall': 0.09328358208955224, 'fpr': 0.0037313432835820895}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-274.000000, 76.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 446.92it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 420.52it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 412.83it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5280811232449298, 'auc': 0.7499446311705822, 'precision': 1.0, 'recall': 0.056162246489859596, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1752, 3584)
Input y shape: (1752,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.73GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1752, 3584)
[DEBUG] Using pre-aggregated inputs: (1752, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.91it/s]
[DEBUG] Final encoded shape: (1752, 262144)
Encoded feature matrix shape: (1752, 262144)
[DEBUG] After encoding. Memory: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1752, 262144), y_train=(1752,)
[DEBUG] Memory at start of feature selection: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1752, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1752, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1752, 3584)
[DEBUG] Returning (1752, 3584) activations
[DEBUG] train activations shape: (1752, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1752, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1752, 3584)
Input y shape: (1752,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.83GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1752, 3584)
[DEBUG] Using pre-aggregated inputs: (1752, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 339.33it/s]
[DEBUG] Final encoded shape: (1752, 16384)
Encoded feature matrix shape: (1752, 16384)
[DEBUG] After encoding. Memory: RAM: 6.83GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1752, 16384), y_train=(1752,)
[DEBUG] Memory at start of feature selection: RAM: 6.83GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1752, 16384)
[DEBUG] Memory after feature matrix build: RAM: 6.83GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 6.83GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 6.83GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1752, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.01it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.79it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.66it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.503731343283582, 'auc': 0.9278931833370462, 'precision': 1.0, 'recall': 0.007462686567164179, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-109.625000, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 413.88it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 387.04it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 394.57it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5018656716417911, 'auc': 0.9153068612163066, 'precision': 1.0, 'recall': 0.0037313432835820895, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 422.51it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 390.39it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 424.74it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5018656716417911, 'auc': 0.9153068612163066, 'precision': 1.0, 'recall': 0.0037313432835820895, 'fpr': 0.0}
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.05it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.79it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.55it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.503731343283582, 'auc': 0.9278931833370462, 'precision': 1.0, 'recall': 0.007462686567164179, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-143.875000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 437.32it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 439.70it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 439.54it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.8191885241712321, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.41it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.06it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.01it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.8870694921400599, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: attention
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: full
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1752, 3584)
[DEBUG] Returning (1752, 3584) activations
[DEBUG] train activations shape: (1752, 3584), dtype: float16
[DEBUG] train activations range: [-1526.000000, 117.875000]
Train activations: (1752, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_mean
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1752, 3584)
Input y shape: (1752,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.85GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1752, 3584)
[DEBUG] Using pre-aggregated inputs: (1752, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.71it/s]
[DEBUG] Final encoded shape: (1752, 262144)
Encoded feature matrix shape: (1752, 262144)
[DEBUG] After encoding. Memory: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1752, 262144), y_train=(1752,)
[DEBUG] Memory at start of feature selection: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1752, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1752, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-593.000000, 51.843750]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 1752 full activations
[DEBUG] Estimated memory requirement: 3.19 GB
[WARNING] Large memory requirement (3.19 GB).
[DEBUG] Max sequence length: 520
[DEBUG] Shape of first activation: (1, 240, 3584)
[DEBUG] Final array shape: (1752, 520, 3584)
[DEBUG] Returning (1752, 520, 3584) activations
[DEBUG] train activations shape: (1752, 520, 3584), dtype: float16
[DEBUG] train activations range: [-1674.000000, 237.000000]
Train activations: (1752, 520, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: attention
  [DEBUG] Creating attention probe
  [DEBUG] Fitting attention probe normally...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.59it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 20.04it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 19.88it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6511194029850746, 'auc': 0.8781744263755847, 'precision': 0.9879518072289156, 'recall': 0.30597014925373134, 'fpr': 0.0037313432835820895}
    [BATCH] Evaluating on 94_better_spam test set
Epoch 10/100, Loss: 0.2353
Epoch 20/100, Loss: 0.0987
Epoch 30/100, Loss: 0.0397
Epoch 40/100, Loss: 0.0110
Epoch 50/100, Loss: 0.0036
Early stopping at epoch 54
Saved probe to results/spam_gemma_9b/seed_43/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state.npz
Saved training log to results/spam_gemma_9b/seed_43/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state_train_log.json
  - 🔥 Probe state saved to train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.71it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.43it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.07it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6511194029850746, 'auc': 0.8781744263755847, 'precision': 0.9879518072289156, 'recall': 0.30597014925373134, 'fpr': 0.0037313432835820895}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1053.000000, 69.625000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.16it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.69it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.71it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5195007800312013, 'auc': 0.5031590655201871, 'precision': 0.7906976744186046, 'recall': 0.0530421216848674, 'fpr': 0.014040561622464899}
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
Loaded probe from results/spam_gemma_9b/seed_43/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 536 full activations
[DEBUG] Estimated memory requirement: 1.61 GB
[WARNING] Large memory requirement (1.61 GB).
[DEBUG] Max sequence length: 939
[DEBUG] Shape of first activation: (1, 62, 3584)
[DEBUG] Final array shape: (536, 939, 3584)
[DEBUG] Returning (536, 939, 3584) activations
[DEBUG] test activations shape: (536, 939, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.500000]
Test activations: (536, 939, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating validation on '94_better_spam': Got unsupported ScalarType BFloat16
    [BATCH] Evaluating on 94_better_spam test set
Loaded probe from results/spam_gemma_9b/seed_43/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state.npz
Converted attention probe back to bfloat16 for training
Test activations: (536, 939, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating test on '94_better_spam': Got unsupported ScalarType BFloat16
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1752, 3584)
[DEBUG] Returning (1752, 3584) activations
[DEBUG] train activations shape: (1752, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1752, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_max
  [DEBUG] Fitting SAE probe normally...
Loaded probe from results/spam_gemma_9b/seed_43/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 5157
[DEBUG] Processing 1282 full activations
[DEBUG] Estimated memory requirement: 0.60 GB
[WARNING] Large memory requirement (0.60 GB).
[DEBUG] Max sequence length: 119
[DEBUG] Shape of first activation: (1, 8, 3584)
[DEBUG] Final array shape: (1282, 119, 3584)
[DEBUG] Returning (1282, 119, 3584) activations
[DEBUG] test activations shape: (1282, 119, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (1282, 119, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating on '87_is_spam': Got unsupported ScalarType BFloat16

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1752, 3584)
Input y shape: (1752,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.84GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1752, 3584)
[DEBUG] Using pre-aggregated inputs: (1752, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.72it/s]

=== Group 24/700 — ('94_better_spam', 20, 'resid_post', 45, '{"class_counts": {"0": 1750, "1": 2}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Final encoded shape: (1752, 262144)
Encoded feature matrix shape: (1752, 262144)
[DEBUG] After encoding. Memory: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1752, 262144), y_train=(1752,)
[DEBUG] Memory at start of feature selection: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1752, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1752, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-227.500000, 81.187500]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 419.77it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 387.50it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 385.58it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5652985074626866, 'auc': 0.8538371574961016, 'precision': 0.8723404255319149, 'recall': 0.15298507462686567, 'fpr': 0.022388059701492536}
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1336.000000, 221.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 439.15it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 404.66it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 412.28it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.49453978159126366, 'auc': 0.6702329871666005, 'precision': 0.3157894736842105, 'recall': 0.0093603744149766, 'fpr': 0.0202808112324493}
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.63it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.89it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.06it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.503731343283582, 'auc': 0.8983069725996881, 'precision': 1.0, 'recall': 0.007462686567164179, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1752, 3584)
[DEBUG] Returning (1752, 3584) activations
[DEBUG] train activations shape: (1752, 3584), dtype: float16
[DEBUG] train activations range: [-1337.000000, 237.000000]
Train activations: (1752, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1752, 3584)
Input y shape: (1752,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.74GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1752, 3584)
[DEBUG] Using pre-aggregated inputs: (1752, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 337.95it/s]
[DEBUG] Final encoded shape: (1752, 16384)
Encoded feature matrix shape: (1752, 16384)
[DEBUG] After encoding. Memory: RAM: 9.74GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1752, 16384), y_train=(1752,)
[DEBUG] Memory at start of feature selection: RAM: 9.74GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1752, 16384)
[DEBUG] Memory after feature matrix build: RAM: 9.74GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 9.74GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 9.74GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1752, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.62it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.11it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.86it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.503731343283582, 'auc': 0.8983069725996881, 'precision': 1.0, 'recall': 0.007462686567164179, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-120.625000, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 418.47it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 387.04it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 390.60it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5242537313432836, 'auc': 0.9435147026063712, 'precision': 1.0, 'recall': 0.048507462686567165, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.07it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.58it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.56it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.7583947663678778, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 410.84it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 384.34it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 377.87it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5242537313432836, 'auc': 0.9435147026063712, 'precision': 1.0, 'recall': 0.048507462686567165, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1336.000000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 442.97it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 425.82it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 429.17it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.7019185603617593, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
Train activations: (1752, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1752, 3584)
Input y shape: (1752,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.84GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1752, 3584)
[DEBUG] Using pre-aggregated inputs: (1752, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.59it/s]
[DEBUG] Final encoded shape: (1752, 262144)
Encoded feature matrix shape: (1752, 262144)
[DEBUG] After encoding. Memory: RAM: 8.55GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1752, 262144), y_train=(1752,)
[DEBUG] Memory at start of feature selection: RAM: 8.55GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1752, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.55GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.55GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.55GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1752, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1752, 3584)
[DEBUG] Returning (1752, 3584) activations
[DEBUG] train activations shape: (1752, 3584), dtype: float16
[DEBUG] train activations range: [-1506.000000, 112.125000]
Train activations: (1752, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_mean
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1752, 3584)
Input y shape: (1752,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.75GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1752, 3584)
[DEBUG] Using pre-aggregated inputs: (1752, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.91it/s]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Final encoded shape: (1752, 262144)
Encoded feature matrix shape: (1752, 262144)
[DEBUG] After encoding. Memory: RAM: 11.46GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1752, 262144), y_train=(1752,)
[DEBUG] Memory at start of feature selection: RAM: 11.46GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1752, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.46GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.46GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.46GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1752, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.67it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.92it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.92it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6399253731343284, 'auc': 0.7886918021831142, 'precision': 0.9411764705882353, 'recall': 0.29850746268656714, 'fpr': 0.018656716417910446}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-677.000000, 50.625000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.01it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.75it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.01it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5335820895522388, 'auc': 0.7618901759857429, 'precision': 0.95, 'recall': 0.0708955223880597, 'fpr': 0.0037313432835820895}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.63it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.92it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.87it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6399253731343284, 'auc': 0.7886918021831142, 'precision': 0.9411764705882353, 'recall': 0.29850746268656714, 'fpr': 0.018656716417910446}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.10it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.64it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.64it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.546801872074883, 'auc': 0.6450432120248929, 'precision': 0.7777777777777778, 'recall': 0.1310452418096724, 'fpr': 0.0374414976599064}
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.04it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.55it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.04it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5335820895522388, 'auc': 0.7618901759857429, 'precision': 0.95, 'recall': 0.0708955223880597, 'fpr': 0.0037313432835820895}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1505.000000, 110.875000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
Train activations: (1752, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_softmax
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.41it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.16it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.06it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5023400936037441, 'auc': 0.7463693867567496, 'precision': 0.5714285714285714, 'recall': 0.0187207488299532, 'fpr': 0.014040561622464899}
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1752, 3584)
Input y shape: (1752,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.85GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1752, 3584)
[DEBUG] Using pre-aggregated inputs: (1752, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.60it/s]
[DEBUG] Final encoded shape: (1752, 262144)
Encoded feature matrix shape: (1752, 262144)
[DEBUG] After encoding. Memory: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1752, 262144), y_train=(1752,)
[DEBUG] Memory at start of feature selection: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1752, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1752, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1752, 3584)
[DEBUG] Returning (1752, 3584) activations
[DEBUG] train activations shape: (1752, 3584), dtype: float16
[DEBUG] train activations range: [-1337.000000, 237.000000]
Train activations: (1752, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_max
  [DEBUG] Fitting SAE probe normally...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1752, 3584)
Input y shape: (1752,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.75GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1752, 3584)
[DEBUG] Using pre-aggregated inputs: (1752, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.92it/s]
[DEBUG] Final encoded shape: (1752, 262144)
Encoded feature matrix shape: (1752, 262144)
[DEBUG] After encoding. Memory: RAM: 11.46GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1752, 262144), y_train=(1752,)
[DEBUG] Memory at start of feature selection: RAM: 11.46GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1752, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.46GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.46GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.46GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1752, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.68it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.21it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.92it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5055970149253731, 'auc': 0.9075517932724438, 'precision': 1.0, 'recall': 0.011194029850746268, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-120.437500, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.60it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5093283582089553, 'auc': 0.9359267097349075, 'precision': 1.0, 'recall': 0.018656716417910446, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.60it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.87it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.87it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5055970149253731, 'auc': 0.9075517932724438, 'precision': 1.0, 'recall': 0.011194029850746268, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.08it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.64it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.74it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.7151608373227285, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: attention
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: full
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.65it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.87it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5093283582089553, 'auc': 0.9359267097349075, 'precision': 1.0, 'recall': 0.018656716417910446, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1336.000000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.41it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.12it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.05it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.6687264682474975, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1752, 3584)
[DEBUG] Returning (1752, 3584) activations
[DEBUG] train activations shape: (1752, 3584), dtype: float16
[DEBUG] train activations range: [-1337.000000, 219.750000]
Train activations: (1752, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1752, 3584)
Input y shape: (1752,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.75GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1752, 3584)
[DEBUG] Using pre-aggregated inputs: (1752, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.93it/s]
[DEBUG] Final encoded shape: (1752, 262144)
Encoded feature matrix shape: (1752, 262144)
[DEBUG] After encoding. Memory: RAM: 11.46GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1752, 262144), y_train=(1752,)
[DEBUG] Memory at start of feature selection: RAM: 11.46GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1752, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.46GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.46GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.46GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1752, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 1752 full activations
[DEBUG] Estimated memory requirement: 3.33 GB
[WARNING] Large memory requirement (3.33 GB).
[DEBUG] Max sequence length: 520
[DEBUG] Shape of first activation: (1, 27, 3584)
[DEBUG] Final array shape: (1752, 520, 3584)
[DEBUG] Returning (1752, 520, 3584) activations
[DEBUG] train activations shape: (1752, 520, 3584), dtype: float16
[DEBUG] train activations range: [-1674.000000, 237.000000]
Train activations: (1752, 520, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: attention
  [DEBUG] Creating attention probe
  [DEBUG] Fitting attention probe normally...
Epoch 10/100, Loss: 0.1923
Epoch 20/100, Loss: 0.0693
Epoch 30/100, Loss: 0.0306
Epoch 40/100, Loss: 0.0159
Epoch 50/100, Loss: 0.0084
Early stopping at epoch 55
Saved probe to results/spam_gemma_9b/seed_44/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state.npz
Saved training log to results/spam_gemma_9b/seed_44/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state_train_log.json
  - 🔥 Probe state saved to train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.04it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.83it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.80it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6138059701492538, 'auc': 0.776439630207173, 'precision': 0.9178082191780822, 'recall': 0.25, 'fpr': 0.022388059701492536}
    [BATCH] Evaluating on 94_better_spam test set
Loaded probe from results/spam_gemma_9b/seed_44/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 536 full activations
[DEBUG] Estimated memory requirement: 1.59 GB
[WARNING] Large memory requirement (1.59 GB).
[DEBUG] Max sequence length: 935
[DEBUG] Shape of first activation: (1, 56, 3584)
[DEBUG] Final array shape: (536, 935, 3584)
[DEBUG] Returning (536, 935, 3584) activations
[DEBUG] test activations shape: (536, 935, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (536, 935, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating validation on '94_better_spam': Got unsupported ScalarType BFloat16
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.01it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.89it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.91it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6138059701492538, 'auc': 0.776439630207173, 'precision': 0.9178082191780822, 'recall': 0.25, 'fpr': 0.022388059701492536}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
Loaded probe from results/spam_gemma_9b/seed_44/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state.npz
Converted attention probe back to bfloat16 for training
Test activations: (536, 935, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating test on '94_better_spam': Got unsupported ScalarType BFloat16
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.42it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.22it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.15it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5561622464898596, 'auc': 0.7101326174731857, 'precision': 0.9186046511627907, 'recall': 0.12324492979719189, 'fpr': 0.0109204368174727}
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
Loaded probe from results/spam_gemma_9b/seed_44/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 5157
[DEBUG] Processing 1282 full activations
[DEBUG] Estimated memory requirement: 0.60 GB
[WARNING] Large memory requirement (0.60 GB).
[DEBUG] Max sequence length: 163
[DEBUG] Shape of first activation: (1, 45, 3584)
[DEBUG] Final array shape: (1282, 163, 3584)
[DEBUG] Returning (1282, 163, 3584) activations
[DEBUG] test activations shape: (1282, 163, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (1282, 163, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating on '87_is_spam': Got unsupported ScalarType BFloat16

=== Group 25/700 — ('94_better_spam', 20, 'resid_post', 46, '{"class_counts": {"0": 1750, "1": 2}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
Train activations: (1752, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_softmax
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-231.750000, 90.750000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 411.53it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 384.62it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 388.83it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6697761194029851, 'auc': 0.8941161728670082, 'precision': 0.989247311827957, 'recall': 0.34328358208955223, 'fpr': 0.0037313432835820895}

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1752, 3584)
Input y shape: (1752,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.75GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1752, 3584)
[DEBUG] Using pre-aggregated inputs: (1752, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.93it/s]
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1306.000000, 211.500000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Final encoded shape: (1752, 262144)
Encoded feature matrix shape: (1752, 262144)
[DEBUG] After encoding. Memory: RAM: 11.46GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1752, 262144), y_train=(1752,)
[DEBUG] Memory at start of feature selection: RAM: 11.46GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1752, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.46GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.46GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.46GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1752, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 441.04it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 408.66it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 410.66it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5054602184087363, 'auc': 0.6233142929461328, 'precision': 0.5686274509803921, 'recall': 0.0452418096723869, 'fpr': 0.0343213728549142}
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1752, 3584)
[DEBUG] Returning (1752, 3584) activations
[DEBUG] train activations shape: (1752, 3584), dtype: float16
[DEBUG] train activations range: [-1337.000000, 237.000000]
Train activations: (1752, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1752, 3584)
Input y shape: (1752,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.85GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1752, 3584)
[DEBUG] Using pre-aggregated inputs: (1752, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 336.46it/s]
[DEBUG] Final encoded shape: (1752, 16384)
Encoded feature matrix shape: (1752, 16384)
[DEBUG] After encoding. Memory: RAM: 6.85GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1752, 16384), y_train=(1752,)
[DEBUG] Memory at start of feature selection: RAM: 6.85GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1752, 16384)
[DEBUG] Memory after feature matrix build: RAM: 6.85GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 6.85GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 6.85GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1752, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.57it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.77it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5111940298507462, 'auc': 0.947691579416351, 'precision': 1.0, 'recall': 0.022388059701492536, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-162.375000, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 407.33it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 413.97it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 387.61it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5093283582089553, 'auc': 0.9542075072399198, 'precision': 1.0, 'recall': 0.018656716417910446, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.01it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.67it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.46it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5111940298507462, 'auc': 0.947691579416351, 'precision': 1.0, 'recall': 0.022388059701492536, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 415.40it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 426.08it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 410.88it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5093283582089553, 'auc': 0.9542075072399198, 'precision': 1.0, 'recall': 0.018656716417910446, 'fpr': 0.0}
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.44it/s]
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1306.000000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.21it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.05it/s]
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 447.49it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 431.34it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 431.11it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.6258064987186072, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: attention
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: full
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5015600624024961, 'auc': 0.8374541533923449, 'precision': 0.625, 'recall': 0.0078003120124804995, 'fpr': 0.0046801872074883}
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1752, 3584)
[DEBUG] Returning (1752, 3584) activations
[DEBUG] train activations shape: (1752, 3584), dtype: float16
[DEBUG] train activations range: [-1506.000000, 112.125000]
Train activations: (1752, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_mean
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1752, 3584)
Input y shape: (1752,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.85GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1752, 3584)
[DEBUG] Using pre-aggregated inputs: (1752, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.76it/s]
[DEBUG] Final encoded shape: (1752, 262144)
Encoded feature matrix shape: (1752, 262144)
[DEBUG] After encoding. Memory: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1752, 262144), y_train=(1752,)
[DEBUG] Memory at start of feature selection: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1752, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1752, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 1752 full activations
[DEBUG] Estimated memory requirement: 3.26 GB
[WARNING] Large memory requirement (3.26 GB).
[DEBUG] Max sequence length: 501
[DEBUG] Shape of first activation: (1, 207, 3584)
[DEBUG] Final array shape: (1752, 501, 3584)
[DEBUG] Returning (1752, 501, 3584) activations
[DEBUG] train activations shape: (1752, 501, 3584), dtype: float16
[DEBUG] train activations range: [-1674.000000, 237.000000]
Train activations: (1752, 501, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: attention
  [DEBUG] Creating attention probe
  [DEBUG] Fitting attention probe normally...
Epoch 10/100, Loss: 0.1592
Epoch 20/100, Loss: 0.0582
Epoch 30/100, Loss: 0.0240
Epoch 40/100, Loss: 0.0069
Epoch 50/100, Loss: 0.0020
Early stopping at epoch 56
Saved probe to results/spam_gemma_9b/seed_45/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state.npz
Saved training log to results/spam_gemma_9b/seed_45/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state_train_log.json
  - 🔥 Probe state saved to train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-1053.000000, 68.750000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.02it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.84it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.98it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.539179104477612, 'auc': 0.8969286032523948, 'precision': 1.0, 'recall': 0.07835820895522388, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.04it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.00it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.18it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.539179104477612, 'auc': 0.8969286032523948, 'precision': 1.0, 'recall': 0.07835820895522388, 'fpr': 0.0}
Loaded probe from results/spam_gemma_9b/seed_45/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 536 full activations
[DEBUG] Estimated memory requirement: 1.55 GB
[WARNING] Large memory requirement (1.55 GB).
[DEBUG] Max sequence length: 939
[DEBUG] Shape of first activation: (1, 61, 3584)
[DEBUG] Final array shape: (536, 939, 3584)
[DEBUG] Returning (536, 939, 3584) activations
[DEBUG] test activations shape: (536, 939, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (536, 939, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating validation on '94_better_spam': Got unsupported ScalarType BFloat16
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1490.000000, 111.750000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.10it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.64it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.57it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5990639625585024, 'auc': 0.4428605849382181, 'precision': 1.0, 'recall': 0.1981279251170047, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
Loaded probe from results/spam_gemma_9b/seed_45/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state.npz
Converted attention probe back to bfloat16 for training
Test activations: (536, 939, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating test on '94_better_spam': Got unsupported ScalarType BFloat16
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1752, 3584)
[DEBUG] Returning (1752, 3584) activations
[DEBUG] train activations shape: (1752, 3584), dtype: float16
[DEBUG] train activations range: [-1337.000000, 237.000000]
Train activations: (1752, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_max
  [DEBUG] Fitting SAE probe normally...
Loaded probe from results/spam_gemma_9b/seed_45/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 5157
[DEBUG] Processing 1282 full activations
[DEBUG] Estimated memory requirement: 0.60 GB
[WARNING] Large memory requirement (0.60 GB).
[DEBUG] Max sequence length: 210
[DEBUG] Shape of first activation: (1, 65, 3584)
[DEBUG] Final array shape: (1282, 210, 3584)
[DEBUG] Returning (1282, 210, 3584) activations
[DEBUG] test activations shape: (1282, 210, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (1282, 210, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating on '87_is_spam': Got unsupported ScalarType BFloat16

=== Group 26/700 — ('94_better_spam', 20, 'resid_post', 47, '{"class_counts": {"0": 1750, "1": 2}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1752, 3584)
Input y shape: (1752,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.86GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1752, 3584)
[DEBUG] Using pre-aggregated inputs: (1752, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.71it/s]
[DEBUG] Final encoded shape: (1752, 262144)
Encoded feature matrix shape: (1752, 262144)
[DEBUG] After encoding. Memory: RAM: 8.57GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1752, 262144), y_train=(1752,)
[DEBUG] Memory at start of feature selection: RAM: 8.57GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1752, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.57GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.57GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.57GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1752, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-237.375000, 80.312500]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 418.76it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 403.69it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 388.97it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5764925373134329, 'auc': 0.8457340164847404, 'precision': 0.9767441860465116, 'recall': 0.15671641791044777, 'fpr': 0.0037313432835820895}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-268.250000, 76.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 440.16it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 423.69it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 429.68it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.4921996879875195, 'auc': 0.5207225449704416, 'precision': 0.46621621621621623, 'recall': 0.10764430577223089, 'fpr': 0.12324492979719189}
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-162.375000, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.61it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.80it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.00it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5074626865671642, 'auc': 0.9607930496769882, 'precision': 1.0, 'recall': 0.014925373134328358, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1752, 3584)
[DEBUG] Returning (1752, 3584) activations
[DEBUG] train activations shape: (1752, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1752, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1752, 3584)
Input y shape: (1752,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.74GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1752, 3584)
[DEBUG] Using pre-aggregated inputs: (1752, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 339.22it/s]
[DEBUG] Final encoded shape: (1752, 16384)
Encoded feature matrix shape: (1752, 16384)
[DEBUG] After encoding. Memory: RAM: 9.74GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1752, 16384), y_train=(1752,)
[DEBUG] Memory at start of feature selection: RAM: 9.74GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1752, 16384)
[DEBUG] Memory after feature matrix build: RAM: 9.74GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 9.74GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 9.74GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1752, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-106.500000, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 411.69it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 382.24it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 389.81it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.8319920917799064, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.64it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.80it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.07it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5074626865671642, 'auc': 0.9607930496769882, 'precision': 1.0, 'recall': 0.014925373134328358, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1306.000000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.11it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.62it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 26.75it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.9328905449509712, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 417.72it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 381.86it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 387.64it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.8319920917799064, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-177.375000, 237.500000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 446.65it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 402.33it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 404.72it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.6049853850628284, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1752, 3584)
[DEBUG] Returning (1752, 3584) activations
[DEBUG] train activations shape: (1752, 3584), dtype: float16
[DEBUG] train activations range: [-1337.000000, 219.750000]
Train activations: (1752, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_last
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1752, 3584)
[DEBUG] Returning (1752, 3584) activations
[DEBUG] train activations shape: (1752, 3584), dtype: float16
[DEBUG] train activations range: [-1526.000000, 117.875000]
Train activations: (1752, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_mean
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1752, 3584)
Input y shape: (1752,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.86GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1752, 3584)
[DEBUG] Using pre-aggregated inputs: (1752, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.71it/s]
[DEBUG] Final encoded shape: (1752, 262144)
Encoded feature matrix shape: (1752, 262144)
[DEBUG] After encoding. Memory: RAM: 8.57GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1752, 262144), y_train=(1752,)
[DEBUG] Memory at start of feature selection: RAM: 8.57GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1752, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.57GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.57GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.57GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1752, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1752, 3584)
Input y shape: (1752,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.74GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1752, 3584)
[DEBUG] Using pre-aggregated inputs: (1752, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.91it/s]
[DEBUG] Final encoded shape: (1752, 262144)
Encoded feature matrix shape: (1752, 262144)
[DEBUG] After encoding. Memory: RAM: 11.45GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1752, 262144), y_train=(1752,)
[DEBUG] Memory at start of feature selection: RAM: 11.45GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1752, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.45GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.45GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.45GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1752, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.63it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.79it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.90it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7033582089552238, 'auc': 0.8857345734016486, 'precision': 0.990990990990991, 'recall': 0.41044776119402987, 'fpr': 0.0037313432835820895}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-570.000000, 51.906250]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.07it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.71it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.04it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5261194029850746, 'auc': 0.7100133660057918, 'precision': 1.0, 'recall': 0.05223880597014925, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.61it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.76it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.71it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7033582089552238, 'auc': 0.8857345734016486, 'precision': 0.990990990990991, 'recall': 0.41044776119402987, 'fpr': 0.0037313432835820895}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.11it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.76it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.62it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5351014040561622, 'auc': 0.641981498292693, 'precision': 0.6890756302521008, 'recall': 0.12792511700468018, 'fpr': 0.057722308892355696}
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.01it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.66it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.94it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5261194029850746, 'auc': 0.7100133660057918, 'precision': 1.0, 'recall': 0.05223880597014925, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1049.000000, 65.937500]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.39it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.17it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.15it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.4828393135725429, 'auc': 0.4925805768580197, 'precision': 0.041666666666666664, 'recall': 0.0015600624024961, 'fpr': 0.0358814352574103}
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
Train activations: (1752, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1752, 3584)
Input y shape: (1752,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.86GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1752, 3584)
[DEBUG] Using pre-aggregated inputs: (1752, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.60it/s]
[DEBUG] Final encoded shape: (1752, 262144)
Encoded feature matrix shape: (1752, 262144)
[DEBUG] After encoding. Memory: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1752, 262144), y_train=(1752,)
[DEBUG] Memory at start of feature selection: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1752, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1752, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1752, 3584)
[DEBUG] Returning (1752, 3584) activations
[DEBUG] train activations shape: (1752, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1752, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_max
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1752, 3584)
Input y shape: (1752,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.74GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1752, 3584)
[DEBUG] Using pre-aggregated inputs: (1752, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.92it/s]
[DEBUG] Final encoded shape: (1752, 262144)
Encoded feature matrix shape: (1752, 262144)
[DEBUG] After encoding. Memory: RAM: 11.45GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1752, 262144), y_train=(1752,)
[DEBUG] Memory at start of feature selection: RAM: 11.45GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1752, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.45GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.45GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.45GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1752, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.63it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.88it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.76it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5055970149253731, 'auc': 0.9689797282245489, 'precision': 1.0, 'recall': 0.011194029850746268, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-106.125000, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.74it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.74it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.8910809757184227, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.62it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.80it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.66it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5055970149253731, 'auc': 0.9689797282245489, 'precision': 1.0, 'recall': 0.011194029850746268, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.10it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.65it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.58it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.886373426855951, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: attention
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: full
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.69it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.99it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.8910809757184227, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-177.375000, 237.500000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.40it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.02it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.01it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.7419788211185233, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1752, 3584)
[DEBUG] Returning (1752, 3584) activations
[DEBUG] train activations shape: (1752, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 231.875000]
Train activations: (1752, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1752, 3584)
Input y shape: (1752,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.73GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1752, 3584)
[DEBUG] Using pre-aggregated inputs: (1752, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.94it/s]
[DEBUG] Final encoded shape: (1752, 262144)
Encoded feature matrix shape: (1752, 262144)
[DEBUG] After encoding. Memory: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1752, 262144), y_train=(1752,)
[DEBUG] Memory at start of feature selection: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1752, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1752, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 1752 full activations
[DEBUG] Estimated memory requirement: 3.31 GB
[WARNING] Large memory requirement (3.31 GB).
[DEBUG] Max sequence length: 520
[DEBUG] Shape of first activation: (1, 109, 3584)
[DEBUG] Final array shape: (1752, 520, 3584)
[DEBUG] Returning (1752, 520, 3584) activations
[DEBUG] train activations shape: (1752, 520, 3584), dtype: float16
[DEBUG] train activations range: [-1674.000000, 237.000000]
Train activations: (1752, 520, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: attention
  [DEBUG] Creating attention probe
  [DEBUG] Fitting attention probe normally...
Epoch 10/100, Loss: 0.1850
Epoch 20/100, Loss: 0.0706
Epoch 30/100, Loss: 0.0262
Epoch 40/100, Loss: 0.0131
Epoch 50/100, Loss: 0.0037
Early stopping at epoch 54
Saved probe to results/spam_gemma_9b/seed_46/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state.npz
Saved training log to results/spam_gemma_9b/seed_46/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state_train_log.json
  - 🔥 Probe state saved to train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.71it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.88it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.628731343283582, 'auc': 0.7717754511026955, 'precision': 0.96, 'recall': 0.26865671641791045, 'fpr': 0.011194029850746268}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.04it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.07it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.91it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.628731343283582, 'auc': 0.7717754511026955, 'precision': 0.96, 'recall': 0.26865671641791045, 'fpr': 0.011194029850746268}
Loaded probe from results/spam_gemma_9b/seed_46/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 536 full activations
[DEBUG] Estimated memory requirement: 1.60 GB
[WARNING] Large memory requirement (1.60 GB).
[DEBUG] Max sequence length: 1167
[DEBUG] Shape of first activation: (1, 225, 3584)
[DEBUG] Final array shape: (536, 1167, 3584)
[DEBUG] Returning (536, 1167, 3584) activations
[DEBUG] test activations shape: (536, 1167, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (536, 1167, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating validation on '94_better_spam': Got unsupported ScalarType BFloat16
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.42it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.04it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.06it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5624024960998439, 'auc': 0.6274566115249915, 'precision': 0.696078431372549, 'recall': 0.22152886115444617, 'fpr': 0.0967238689547582}
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
Loaded probe from results/spam_gemma_9b/seed_46/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state.npz
Converted attention probe back to bfloat16 for training
Test activations: (536, 1167, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating test on '94_better_spam': Got unsupported ScalarType BFloat16
Train activations: (1752, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_softmax
  [DEBUG] Fitting SAE probe normally...
Loaded probe from results/spam_gemma_9b/seed_46/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 5157
[DEBUG] Processing 1282 full activations
[DEBUG] Estimated memory requirement: 0.60 GB
[WARNING] Large memory requirement (0.60 GB).
[DEBUG] Max sequence length: 163
[DEBUG] Shape of first activation: (1, 26, 3584)
[DEBUG] Final array shape: (1282, 163, 3584)
[DEBUG] Returning (1282, 163, 3584) activations
[DEBUG] test activations shape: (1282, 163, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (1282, 163, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating on '87_is_spam': Got unsupported ScalarType BFloat16

=== Group 27/700 — ('94_better_spam', 20, 'resid_post', 48, '{"class_counts": {"0": 1750, "1": 2}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1752, 3584)
Input y shape: (1752,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.73GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1752, 3584)
[DEBUG] Using pre-aggregated inputs: (1752, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.91it/s]
[DEBUG] Final encoded shape: (1752, 262144)
Encoded feature matrix shape: (1752, 262144)
[DEBUG] After encoding. Memory: RAM: 11.45GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1752, 262144), y_train=(1752,)
[DEBUG] Memory at start of feature selection: RAM: 11.45GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1752, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.45GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.45GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.45GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1752, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-256.250000, 81.875000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 414.62it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 375.40it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 424.48it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7220149253731343, 'auc': 0.8368790376475829, 'precision': 0.976, 'recall': 0.4552238805970149, 'fpr': 0.011194029850746268}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.72it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.65it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.8834512140788593, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 409.76it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 390.75it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 391.77it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7220149253731343, 'auc': 0.8368790376475829, 'precision': 0.976, 'recall': 0.4552238805970149, 'fpr': 0.011194029850746268}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1336.000000, 221.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 442.41it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 410.20it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 420.21it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.4859594383775351, 'auc': 0.43666170983812824, 'precision': 0.3977272727272727, 'recall': 0.054602184087363496, 'fpr': 0.08268330733229329}
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1752, 3584)
[DEBUG] Returning (1752, 3584) activations
[DEBUG] train activations shape: (1752, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1752, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1752, 3584)
Input y shape: (1752,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.85GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1752, 3584)
[DEBUG] Using pre-aggregated inputs: (1752, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 337.72it/s]
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.99it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.49it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.53it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.8834512140788593, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
[DEBUG] Final encoded shape: (1752, 16384)
Encoded feature matrix shape: (1752, 16384)
[DEBUG] After encoding. Memory: RAM: 6.85GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1752, 16384), y_train=(1752,)
[DEBUG] Memory at start of feature selection: RAM: 6.85GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1752, 16384)
[DEBUG] Memory after feature matrix build: RAM: 6.85GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 6.85GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 6.85GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1752, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.40it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.04it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.05it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.540879232673207, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: attention
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: full
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-106.500000, 237.500000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 413.07it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 385.40it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 389.66it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5055970149253731, 'auc': 0.9493484072176431, 'precision': 1.0, 'recall': 0.011194029850746268, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 412.14it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 426.08it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 394.98it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5055970149253731, 'auc': 0.9493484072176431, 'precision': 1.0, 'recall': 0.011194029850746268, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1336.000000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 446.01it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 420.67it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 418.89it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.8243286985769602, 'precision': 0.5, 'recall': 0.0031201248049922, 'fpr': 0.0031201248049922}
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1752, 3584)
[DEBUG] Returning (1752, 3584) activations
[DEBUG] train activations shape: (1752, 3584), dtype: float16
[DEBUG] train activations range: [-1526.000000, 117.875000]
Train activations: (1752, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_mean
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 1752 full activations
[DEBUG] Estimated memory requirement: 3.27 GB
[WARNING] Large memory requirement (3.27 GB).
[DEBUG] Max sequence length: 511
[DEBUG] Shape of first activation: (1, 74, 3584)
[DEBUG] Final array shape: (1752, 511, 3584)
[DEBUG] Returning (1752, 511, 3584) activations
[DEBUG] train activations shape: (1752, 511, 3584), dtype: float16
[DEBUG] train activations range: [-1674.000000, 237.000000]
Train activations: (1752, 511, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: attention
  [DEBUG] Creating attention probe
  [DEBUG] Fitting attention probe normally...
Epoch 10/100, Loss: 0.1376
Epoch 20/100, Loss: 0.0537
Epoch 30/100, Loss: 0.0238
Epoch 40/100, Loss: 0.0135
Epoch 50/100, Loss: 0.0069
Early stopping at epoch 52
Saved probe to results/spam_gemma_9b/seed_47/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state.npz
Saved training log to results/spam_gemma_9b/seed_47/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state_train_log.json
  - 🔥 Probe state saved to train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1752, 3584)
Input y shape: (1752,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.83GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1752, 3584)
[DEBUG] Using pre-aggregated inputs: (1752, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.60it/s]
[DEBUG] Final encoded shape: (1752, 262144)
Encoded feature matrix shape: (1752, 262144)
[DEBUG] After encoding. Memory: RAM: 8.55GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1752, 262144), y_train=(1752,)
[DEBUG] Memory at start of feature selection: RAM: 8.55GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1752, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.55GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.55GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.55GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1752, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-567.000000, 51.500000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.70it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.27it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.39it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6511194029850746, 'auc': 0.8606593896190688, 'precision': 1.0, 'recall': 0.30223880597014924, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Loaded probe from results/spam_gemma_9b/seed_47/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 536 full activations
[DEBUG] Estimated memory requirement: 1.60 GB
[WARNING] Large memory requirement (1.60 GB).
[DEBUG] Max sequence length: 939
[DEBUG] Shape of first activation: (1, 335, 3584)
[DEBUG] Final array shape: (536, 939, 3584)
[DEBUG] Returning (536, 939, 3584) activations
[DEBUG] test activations shape: (536, 939, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (536, 939, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating validation on '94_better_spam': Got unsupported ScalarType BFloat16
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Loaded probe from results/spam_gemma_9b/seed_47/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state.npz
Converted attention probe back to bfloat16 for training
Test activations: (536, 939, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating test on '94_better_spam': Got unsupported ScalarType BFloat16
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.66it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.25it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.06it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6511194029850746, 'auc': 0.8606593896190688, 'precision': 1.0, 'recall': 0.30223880597014924, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1505.000000, 110.875000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
Loaded probe from results/spam_gemma_9b/seed_47/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 5157
[DEBUG] Processing 1282 full activations
[DEBUG] Estimated memory requirement: 0.59 GB
[WARNING] Large memory requirement (0.59 GB).
[DEBUG] Max sequence length: 210
[DEBUG] Shape of first activation: (1, 20, 3584)
[DEBUG] Final array shape: (1282, 210, 3584)
[DEBUG] Returning (1282, 210, 3584) activations
[DEBUG] test activations shape: (1282, 210, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.500000]
Test activations: (1282, 210, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating on '87_is_spam': Got unsupported ScalarType BFloat16

=== Group 28/700 — ('94_better_spam', 20, 'resid_post', 49, '{"class_counts": {"0": 1750, "1": 2}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.11it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.63it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.76it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.764170161190223, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-119.687500, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 414.78it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 385.33it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 382.06it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.503731343283582, 'auc': 0.8381460236132767, 'precision': 1.0, 'recall': 0.007462686567164179, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-177.375000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 438.94it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 409.42it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 412.10it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.6777266410469212, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1752, 3584)
[DEBUG] Returning (1752, 3584) activations
[DEBUG] train activations shape: (1752, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1752, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_max
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1752, 3584)
Input y shape: (1752,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.83GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1752, 3584)
[DEBUG] Using pre-aggregated inputs: (1752, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.71it/s]
[DEBUG] Final encoded shape: (1752, 262144)
Encoded feature matrix shape: (1752, 262144)
[DEBUG] After encoding. Memory: RAM: 8.54GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1752, 262144), y_train=(1752,)
[DEBUG] Memory at start of feature selection: RAM: 8.54GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1752, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.54GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.54GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.54GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1752, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1752, 3584)
[DEBUG] Returning (1752, 3584) activations
[DEBUG] train activations shape: (1752, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 231.875000]
Train activations: (1752, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1752, 3584)
Input y shape: (1752,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.75GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1752, 3584)
[DEBUG] Using pre-aggregated inputs: (1752, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 337.64it/s]
[DEBUG] Final encoded shape: (1752, 16384)
Encoded feature matrix shape: (1752, 16384)
[DEBUG] After encoding. Memory: RAM: 9.75GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1752, 16384), y_train=(1752,)
[DEBUG] Memory at start of feature selection: RAM: 9.75GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1752, 16384)
[DEBUG] Memory after feature matrix build: RAM: 9.75GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 9.75GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 9.75GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1752, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-106.125000, 237.500000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.63it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.85it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.13it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5167910447761194, 'auc': 0.9448234573401649, 'precision': 1.0, 'recall': 0.033582089552238806, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-210.625000, 80.875000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 414.83it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 387.39it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 422.34it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6828358208955224, 'auc': 0.8640287369124526, 'precision': 1.0, 'recall': 0.3656716417910448, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 413.56it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 388.69it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 427.03it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6828358208955224, 'auc': 0.8640287369124526, 'precision': 1.0, 'recall': 0.3656716417910448, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-263.750000, 76.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 443.75it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 430.98it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 408.50it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.4734789391575663, 'auc': 0.5702867740294635, 'precision': 0.11363636363636363, 'recall': 0.0078003120124804995, 'fpr': 0.060842433697347896}
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.62it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.89it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.04it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5167910447761194, 'auc': 0.9448234573401649, 'precision': 1.0, 'recall': 0.033582089552238806, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1336.000000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.10it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.74it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.55it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.7138709261318971, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1752, 3584)
[DEBUG] Returning (1752, 3584) activations
[DEBUG] train activations shape: (1752, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1752, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1752, 3584)
Input y shape: (1752,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.75GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1752, 3584)
[DEBUG] Using pre-aggregated inputs: (1752, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 336.92it/s]
[DEBUG] Final encoded shape: (1752, 16384)
Encoded feature matrix shape: (1752, 16384)
[DEBUG] After encoding. Memory: RAM: 9.75GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1752, 16384), y_train=(1752,)
[DEBUG] Memory at start of feature selection: RAM: 9.75GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1752, 16384)
[DEBUG] Memory after feature matrix build: RAM: 9.75GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 9.75GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 9.75GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1752, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1752, 3584)
[DEBUG] Returning (1752, 3584) activations
[DEBUG] train activations shape: (1752, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 231.875000]
Train activations: (1752, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_last
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-119.750000, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 418.76it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 380.64it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 389.55it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5130597014925373, 'auc': 0.8472933838271328, 'precision': 0.8888888888888888, 'recall': 0.029850746268656716, 'fpr': 0.0037313432835820895}
    [BATCH] Evaluating on 94_better_spam test set

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1752, 3584)
Input y shape: (1752,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.84GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1752, 3584)
[DEBUG] Using pre-aggregated inputs: (1752, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.72it/s]
[DEBUG] Final encoded shape: (1752, 262144)
Encoded feature matrix shape: (1752, 262144)
[DEBUG] After encoding. Memory: RAM: 8.55GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1752, 262144), y_train=(1752,)
[DEBUG] Memory at start of feature selection: RAM: 8.55GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1752, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.55GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.55GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.55GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1752, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 415.85it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 407.81it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 426.64it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5130597014925373, 'auc': 0.8472933838271328, 'precision': 0.8888888888888888, 'recall': 0.029850746268656716, 'fpr': 0.0037313432835820895}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-177.375000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 443.30it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 433.05it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 410.34it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5031201248049922, 'auc': 0.5862695038222746, 'precision': 0.75, 'recall': 0.0093603744149766, 'fpr': 0.0031201248049922}
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.63it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.11it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.11it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7052238805970149, 'auc': 0.7932167520605925, 'precision': 0.9910714285714286, 'recall': 0.4141791044776119, 'fpr': 0.0037313432835820895}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1752, 3584)
[DEBUG] Returning (1752, 3584) activations
[DEBUG] train activations shape: (1752, 3584), dtype: float16
[DEBUG] train activations range: [-1526.000000, 117.875000]
Train activations: (1752, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_mean
  [DEBUG] Fitting SAE probe normally...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1752, 3584)
Input y shape: (1752,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.76GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1752, 3584)
[DEBUG] Using pre-aggregated inputs: (1752, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.92it/s]
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.76it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.06it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.37it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7052238805970149, 'auc': 0.7932167520605925, 'precision': 0.9910714285714286, 'recall': 0.4141791044776119, 'fpr': 0.0037313432835820895}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Final encoded shape: (1752, 262144)
Encoded feature matrix shape: (1752, 262144)
[DEBUG] After encoding. Memory: RAM: 11.47GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1752, 262144), y_train=(1752,)
[DEBUG] Memory at start of feature selection: RAM: 11.47GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1752, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.47GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.47GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.47GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1752, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.11it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.65it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.69it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5187207488299532, 'auc': 0.5605905359459309, 'precision': 0.6463414634146342, 'recall': 0.08268330733229329, 'fpr': 0.0452418096723869}
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-813.000000, 51.906250]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Train activations: (1752, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_softmax
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.95it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.91it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5186567164179104, 'auc': 0.90415460013366, 'precision': 1.0, 'recall': 0.03731343283582089, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1752, 3584)
Input y shape: (1752,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.84GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1752, 3584)
[DEBUG] Using pre-aggregated inputs: (1752, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.72it/s]
[DEBUG] Final encoded shape: (1752, 262144)
Encoded feature matrix shape: (1752, 262144)
[DEBUG] After encoding. Memory: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1752, 262144), y_train=(1752,)
[DEBUG] Memory at start of feature selection: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1752, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1752, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.06it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.99it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.94it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5186567164179104, 'auc': 0.90415460013366, 'precision': 1.0, 'recall': 0.03731343283582089, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1053.000000, 69.625000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.67it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.16it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.22it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5093283582089553, 'auc': 0.9394074404098909, 'precision': 1.0, 'recall': 0.018656716417910446, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.41it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.04it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.98it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.31494520311233665, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1752, 3584)
[DEBUG] Returning (1752, 3584) activations
[DEBUG] train activations shape: (1752, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1752, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_max
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.66it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.17it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.02it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5093283582089553, 'auc': 0.9394074404098909, 'precision': 1.0, 'recall': 0.018656716417910446, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1752, 3584)
Input y shape: (1752,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.76GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1752, 3584)
[DEBUG] Using pre-aggregated inputs: (1752, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.90it/s]
[DEBUG] Final encoded shape: (1752, 262144)
Encoded feature matrix shape: (1752, 262144)
[DEBUG] After encoding. Memory: RAM: 11.47GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1752, 262144), y_train=(1752,)
[DEBUG] Memory at start of feature selection: RAM: 11.47GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1752, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.47GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.47GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.47GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1752, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.07it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.67it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.78it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.7135009893375454, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: attention
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: full
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.02it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.06it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5018656716417911, 'auc': 0.8938098685676097, 'precision': 1.0, 'recall': 0.0037313432835820895, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.05it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.94it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.11it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5018656716417911, 'auc': 0.8938098685676097, 'precision': 1.0, 'recall': 0.0037313432835820895, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 1752 full activations
[DEBUG] Estimated memory requirement: 3.24 GB
[WARNING] Large memory requirement (3.24 GB).
[DEBUG] Max sequence length: 520
[DEBUG] Shape of first activation: (1, 178, 3584)
[DEBUG] Final array shape: (1752, 520, 3584)
[DEBUG] Returning (1752, 520, 3584) activations
[DEBUG] train activations shape: (1752, 520, 3584), dtype: float16
[DEBUG] train activations range: [-1674.000000, 237.000000]
Train activations: (1752, 520, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: attention
  [DEBUG] Creating attention probe
  [DEBUG] Fitting attention probe normally...
Epoch 10/100, Loss: 0.1479
Epoch 20/100, Loss: 0.0512
Epoch 30/100, Loss: 0.0220
Epoch 40/100, Loss: 0.0111
Epoch 50/100, Loss: 0.0042
Early stopping at epoch 56
Saved probe to results/spam_gemma_9b/seed_48/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state.npz
Saved training log to results/spam_gemma_9b/seed_48/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state_train_log.json
  - 🔥 Probe state saved to train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.43it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.18it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.03it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.7761541662914567, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
Train activations: (1752, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1752, 3584)
Input y shape: (1752,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.76GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1752, 3584)
[DEBUG] Using pre-aggregated inputs: (1752, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.93it/s]
[DEBUG] Final encoded shape: (1752, 262144)
Encoded feature matrix shape: (1752, 262144)
[DEBUG] After encoding. Memory: RAM: 11.47GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1752, 262144), y_train=(1752,)
[DEBUG] Memory at start of feature selection: RAM: 11.47GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1752, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.47GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.47GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.47GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1752, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Loaded probe from results/spam_gemma_9b/seed_48/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 536 full activations
[DEBUG] Estimated memory requirement: 1.63 GB
[WARNING] Large memory requirement (1.63 GB).
[DEBUG] Max sequence length: 939
[DEBUG] Shape of first activation: (1, 289, 3584)
[DEBUG] Final array shape: (536, 939, 3584)
[DEBUG] Returning (536, 939, 3584) activations
[DEBUG] test activations shape: (536, 939, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.500000]
Test activations: (536, 939, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating validation on '94_better_spam': Got unsupported ScalarType BFloat16
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Loaded probe from results/spam_gemma_9b/seed_48/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state.npz
Converted attention probe back to bfloat16 for training
Test activations: (536, 939, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating test on '94_better_spam': Got unsupported ScalarType BFloat16
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.06it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.78it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.77it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6529850746268657, 'auc': 0.77709400757407, 'precision': 0.9659090909090909, 'recall': 0.31716417910447764, 'fpr': 0.011194029850746268}
    [BATCH] Evaluating on 94_better_spam test set
Loaded probe from results/spam_gemma_9b/seed_48/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 5157
[DEBUG] Processing 1282 full activations
[DEBUG] Estimated memory requirement: 0.60 GB
[WARNING] Large memory requirement (0.60 GB).
[DEBUG] Max sequence length: 210
[DEBUG] Shape of first activation: (1, 17, 3584)
[DEBUG] Final array shape: (1282, 210, 3584)
[DEBUG] Returning (1282, 210, 3584) activations
[DEBUG] test activations shape: (1282, 210, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (1282, 210, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating on '87_is_spam': Got unsupported ScalarType BFloat16

=== Group 29/700 — ('94_better_spam', 20, 'resid_post', 50, '{"class_counts": {"0": 1750, "1": 2}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.01it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.71it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.75it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6529850746268657, 'auc': 0.77709400757407, 'precision': 0.9659090909090909, 'recall': 0.31716417910447764, 'fpr': 0.011194029850746268}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-256.250000, 90.750000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 412.78it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 388.87it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 423.50it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6119402985074627, 'auc': 0.832312319002005, 'precision': 0.9838709677419355, 'recall': 0.22761194029850745, 'fpr': 0.0037313432835820895}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.40it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.20it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.98it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.48985959438377535, 'auc': 0.5671325760986757, 'precision': 0.3488372093023256, 'recall': 0.0234009360374415, 'fpr': 0.0436817472698908}
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 414.09it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 389.59it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 394.02it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6119402985074627, 'auc': 0.832312319002005, 'precision': 0.9838709677419355, 'recall': 0.22761194029850745, 'fpr': 0.0037313432835820895}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-271.250000, 76.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 442.74it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 408.01it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 413.46it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5085803432137286, 'auc': 0.5969976708584724, 'precision': 0.6078431372549019, 'recall': 0.0483619344773791, 'fpr': 0.031201248049921998}
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
Train activations: (1752, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1752, 3584)
Input y shape: (1752,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.75GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1752, 3584)
[DEBUG] Using pre-aggregated inputs: (1752, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.91it/s]
[DEBUG] Final encoded shape: (1752, 262144)
Encoded feature matrix shape: (1752, 262144)
[DEBUG] After encoding. Memory: RAM: 11.46GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1752, 262144), y_train=(1752,)
[DEBUG] Memory at start of feature selection: RAM: 11.46GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1752, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.46GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.46GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.46GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1752, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1752, 3584)
[DEBUG] Returning (1752, 3584) activations
[DEBUG] train activations shape: (1752, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1752, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1752, 3584)
Input y shape: (1752,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.83GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1752, 3584)
[DEBUG] Using pre-aggregated inputs: (1752, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 335.75it/s]
[DEBUG] Final encoded shape: (1752, 16384)
Encoded feature matrix shape: (1752, 16384)
[DEBUG] After encoding. Memory: RAM: 6.83GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1752, 16384), y_train=(1752,)
[DEBUG] Memory at start of feature selection: RAM: 6.83GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1752, 16384)
[DEBUG] Memory after feature matrix build: RAM: 6.83GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 6.83GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 6.83GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1752, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s][DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-157.625000, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.02it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.96it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.02it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.503731343283582, 'auc': 0.9221569391846737, 'precision': 1.0, 'recall': 0.007462686567164179, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 415.77it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 418.47it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 424.83it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5167910447761194, 'auc': 0.9199431944753843, 'precision': 1.0, 'recall': 0.033582089552238806, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 416.06it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 385.79it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 390.97it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5167910447761194, 'auc': 0.9199431944753843, 'precision': 1.0, 'recall': 0.033582089552238806, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-163.875000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 447.03it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 430.34it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 430.80it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5039001560062403, 'auc': 0.7125445080205705, 'precision': 0.5675675675675675, 'recall': 0.0327613104524181, 'fpr': 0.0249609984399376}
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.04it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.97it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.90it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.503731343283582, 'auc': 0.9221569391846737, 'precision': 1.0, 'recall': 0.007462686567164179, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.41it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.05it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.19it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.6777777507356144, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: attention
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: full
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1752, 3584)
[DEBUG] Returning (1752, 3584) activations
[DEBUG] train activations shape: (1752, 3584), dtype: float16
[DEBUG] train activations range: [-1526.000000, 117.875000]
Train activations: (1752, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_mean
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1752, 3584)
Input y shape: (1752,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.85GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1752, 3584)
[DEBUG] Using pre-aggregated inputs: (1752, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.60it/s]
[DEBUG] Final encoded shape: (1752, 262144)
Encoded feature matrix shape: (1752, 262144)
[DEBUG] After encoding. Memory: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1752, 262144), y_train=(1752,)
[DEBUG] Memory at start of feature selection: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1752, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1752, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-1067.000000, 71.062500]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.73it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.06it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5541044776119403, 'auc': 0.808531967030519, 'precision': 1.0, 'recall': 0.10820895522388059, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 1752 full activations
[DEBUG] Estimated memory requirement: 3.28 GB
[WARNING] Large memory requirement (3.28 GB).
[DEBUG] Max sequence length: 520
[DEBUG] Shape of first activation: (1, 313, 3584)
[DEBUG] Final array shape: (1752, 520, 3584)
[DEBUG] Returning (1752, 520, 3584) activations
[DEBUG] train activations shape: (1752, 520, 3584), dtype: float16
[DEBUG] train activations range: [-1674.000000, 237.000000]
Train activations: (1752, 520, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: attention
  [DEBUG] Creating attention probe
  [DEBUG] Fitting attention probe normally...
Epoch 10/100, Loss: 0.2287
Epoch 20/100, Loss: 0.0621
Epoch 30/100, Loss: 0.0131
Epoch 40/100, Loss: 0.0047
Early stopping at epoch 44
Saved probe to results/spam_gemma_9b/seed_49/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state.npz
Saved training log to results/spam_gemma_9b/seed_49/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state_train_log.json
  - 🔥 Probe state saved to train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.62it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.88it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.11it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5541044776119403, 'auc': 0.808531967030519, 'precision': 1.0, 'recall': 0.10820895522388059, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1045.000000, 76.437500]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.16it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.87it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.74it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5015600624024961, 'auc': 0.3498020108011809, 'precision': 0.6, 'recall': 0.0093603744149766, 'fpr': 0.0062402496099844}
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
Loaded probe from results/spam_gemma_9b/seed_49/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 536 full activations
[DEBUG] Estimated memory requirement: 1.55 GB
[WARNING] Large memory requirement (1.55 GB).
[DEBUG] Max sequence length: 1167
[DEBUG] Shape of first activation: (1, 171, 3584)
[DEBUG] Final array shape: (536, 1167, 3584)
[DEBUG] Returning (536, 1167, 3584) activations
[DEBUG] test activations shape: (536, 1167, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (536, 1167, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating validation on '94_better_spam': Got unsupported ScalarType BFloat16
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1752, 3584)
[DEBUG] Returning (1752, 3584) activations
[DEBUG] train activations shape: (1752, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1752, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_max
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1752, 3584)
Input y shape: (1752,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.85GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1752, 3584)
[DEBUG] Using pre-aggregated inputs: (1752, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.73it/s]
[DEBUG] Final encoded shape: (1752, 262144)
Encoded feature matrix shape: (1752, 262144)
[DEBUG] After encoding. Memory: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1752, 262144), y_train=(1752,)
[DEBUG] Memory at start of feature selection: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1752, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1752, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Loaded probe from results/spam_gemma_9b/seed_49/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state.npz
Converted attention probe back to bfloat16 for training
Test activations: (536, 1167, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating test on '94_better_spam': Got unsupported ScalarType BFloat16
Loaded probe from results/spam_gemma_9b/seed_49/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 5157
[DEBUG] Processing 1282 full activations
[DEBUG] Estimated memory requirement: 0.60 GB
[WARNING] Large memory requirement (0.60 GB).
[DEBUG] Max sequence length: 215
[DEBUG] Shape of first activation: (1, 33, 3584)
[DEBUG] Final array shape: (1282, 215, 3584)
[DEBUG] Returning (1282, 215, 3584) activations
[DEBUG] test activations shape: (1282, 215, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (1282, 215, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating on '87_is_spam': Got unsupported ScalarType BFloat16

=== Group 30/700 — ('94_better_spam', 20, 'resid_post', 51, '{"class_counts": {"0": 1750, "1": 2}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-157.625000, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.63it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.15it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.17it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5093283582089553, 'auc': 0.9400339719313877, 'precision': 1.0, 'recall': 0.018656716417910446, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-210.750000, 80.687500]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 413.19it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 378.38it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 408.24it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5485074626865671, 'auc': 0.735645466696369, 'precision': 0.8611111111111112, 'recall': 0.11567164179104478, 'fpr': 0.018656716417910446}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1371.000000, 229.375000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 443.84it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 421.64it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 401.81it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5491419656786272, 'auc': 0.5407770132958205, 'precision': 0.6387665198237885, 'recall': 0.22620904836193448, 'fpr': 0.12792511700468018}
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.64it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.87it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.09it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5093283582089553, 'auc': 0.9400339719313877, 'precision': 1.0, 'recall': 0.018656716417910446, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-163.875000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1752, 3584)
[DEBUG] Returning (1752, 3584) activations
[DEBUG] train activations shape: (1752, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1752, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1752, 3584)
Input y shape: (1752,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.69GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1752, 3584)
[DEBUG] Using pre-aggregated inputs: (1752, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 335.14it/s]
[DEBUG] Final encoded shape: (1752, 16384)
Encoded feature matrix shape: (1752, 16384)
[DEBUG] After encoding. Memory: RAM: 9.69GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1752, 16384), y_train=(1752,)
[DEBUG] Memory at start of feature selection: RAM: 9.69GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1752, 16384)
[DEBUG] Memory after feature matrix build: RAM: 9.69GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 9.69GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 9.69GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1752, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.11it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.66it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.59it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.8615195153827995, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-119.187500, 237.500000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 412.58it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 408.24it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 418.97it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5354477611940298, 'auc': 0.917896524838494, 'precision': 1.0, 'recall': 0.0708955223880597, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1752, 3584)
[DEBUG] Returning (1752, 3584) activations
[DEBUG] train activations shape: (1752, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 231.875000]
Train activations: (1752, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1752, 3584)
Input y shape: (1752,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.85GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1752, 3584)
[DEBUG] Using pre-aggregated inputs: (1752, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.71it/s]
[DEBUG] Final encoded shape: (1752, 262144)
Encoded feature matrix shape: (1752, 262144)
[DEBUG] After encoding. Memory: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1752, 262144), y_train=(1752,)
[DEBUG] Memory at start of feature selection: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1752, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1752, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 412.70it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 415.28it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 387.29it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5354477611940298, 'auc': 0.917896524838494, 'precision': 1.0, 'recall': 0.0708955223880597, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1371.000000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 444.12it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 424.57it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 409.58it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.4578783151326053, 'auc': 0.5502517760616821, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0842433697347894}
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1752, 3584)
[DEBUG] Returning (1752, 3584) activations
[DEBUG] train activations shape: (1752, 3584), dtype: float16
[DEBUG] train activations range: [-1526.000000, 117.875000]
Train activations: (1752, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_mean
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.64it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.19it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.87it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6753731343283582, 'auc': 0.8410559144575629, 'precision': 0.9795918367346939, 'recall': 0.3582089552238806, 'fpr': 0.007462686567164179}
    [BATCH] Evaluating on 94_better_spam test set

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1752, 3584)
Input y shape: (1752,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.69GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1752, 3584)
[DEBUG] Using pre-aggregated inputs: (1752, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.91it/s]
[DEBUG] Final encoded shape: (1752, 262144)
Encoded feature matrix shape: (1752, 262144)
[DEBUG] After encoding. Memory: RAM: 11.40GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1752, 262144), y_train=(1752,)
[DEBUG] Memory at start of feature selection: RAM: 11.40GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1752, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.40GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.40GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.40GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1752, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.61it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.84it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.82it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6753731343283582, 'auc': 0.8410559144575629, 'precision': 0.9795918367346939, 'recall': 0.3582089552238806, 'fpr': 0.007462686567164179}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-782.000000, 52.031250]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.10it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.65it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.62it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5405616224648986, 'auc': 0.672318749224228, 'precision': 0.678082191780822, 'recall': 0.1544461778471139, 'fpr': 0.07332293291731669}
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.04it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.05it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.72it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5317164179104478, 'auc': 0.8277873691245266, 'precision': 1.0, 'recall': 0.06343283582089553, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Train activations: (1752, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_softmax
  [DEBUG] Fitting SAE probe normally...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1752, 3584)
Input y shape: (1752,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.85GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1752, 3584)
[DEBUG] Using pre-aggregated inputs: (1752, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.60it/s]
[DEBUG] Final encoded shape: (1752, 262144)
Encoded feature matrix shape: (1752, 262144)
[DEBUG] After encoding. Memory: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1752, 262144), y_train=(1752,)
[DEBUG] Memory at start of feature selection: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1752, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1752, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.96it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.72it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5317164179104478, 'auc': 0.8277873691245266, 'precision': 1.0, 'recall': 0.06343283582089553, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1522.000000, 113.625000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.41it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.18it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.98it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.17276778434631923, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.60it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.86it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.503731343283582, 'auc': 0.9409668077522834, 'precision': 1.0, 'recall': 0.007462686567164179, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1752, 3584)
[DEBUG] Returning (1752, 3584) activations
[DEBUG] train activations shape: (1752, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1752, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_max
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1752, 3584)
Input y shape: (1752,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.69GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1752, 3584)
[DEBUG] Using pre-aggregated inputs: (1752, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.92it/s]
[DEBUG] Final encoded shape: (1752, 262144)
Encoded feature matrix shape: (1752, 262144)
[DEBUG] After encoding. Memory: RAM: 11.40GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1752, 262144), y_train=(1752,)
[DEBUG] Memory at start of feature selection: RAM: 11.40GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1752, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.40GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.40GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.40GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1752, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.65it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.17it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.13it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.503731343283582, 'auc': 0.9409668077522834, 'precision': 1.0, 'recall': 0.007462686567164179, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-118.875000, 237.500000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.11it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.66it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.61it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.8679301306217615, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: attention
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: full
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.02it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.73it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.68it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5261194029850746, 'auc': 0.9259718200044554, 'precision': 1.0, 'recall': 0.05223880597014925, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.04it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.98it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.89it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5261194029850746, 'auc': 0.9259718200044554, 'precision': 1.0, 'recall': 0.05223880597014925, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1371.000000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.45it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.13it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.23it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.683175907379509, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 1752 full activations
[DEBUG] Estimated memory requirement: 3.28 GB
[WARNING] Large memory requirement (3.28 GB).
[DEBUG] Max sequence length: 501
[DEBUG] Shape of first activation: (1, 25, 3584)
[DEBUG] Final array shape: (1752, 501, 3584)
[DEBUG] Returning (1752, 501, 3584) activations
[DEBUG] train activations shape: (1752, 501, 3584), dtype: float16
[DEBUG] train activations range: [-1674.000000, 237.000000]
Train activations: (1752, 501, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: attention
  [DEBUG] Creating attention probe
  [DEBUG] Fitting attention probe normally...
Epoch 10/100, Loss: 0.1856
Epoch 20/100, Loss: 0.0632
Epoch 30/100, Loss: 0.0263
Epoch 40/100, Loss: 0.0102
Epoch 50/100, Loss: 0.0026
Early stopping at epoch 53
Saved probe to results/spam_gemma_9b/seed_50/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state.npz
Saved training log to results/spam_gemma_9b/seed_50/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state_train_log.json
  - 🔥 Probe state saved to train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1752, 3584)
[DEBUG] Returning (1752, 3584) activations
[DEBUG] train activations shape: (1752, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 231.875000]
Train activations: (1752, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1752, 3584)
Input y shape: (1752,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.68GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1752, 3584)
[DEBUG] Using pre-aggregated inputs: (1752, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.91it/s]
[DEBUG] Final encoded shape: (1752, 262144)
Encoded feature matrix shape: (1752, 262144)
[DEBUG] After encoding. Memory: RAM: 11.39GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1752, 262144), y_train=(1752,)
[DEBUG] Memory at start of feature selection: RAM: 11.39GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1752, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.39GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.39GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.39GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1752, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Loaded probe from results/spam_gemma_9b/seed_50/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 536 full activations
[DEBUG] Estimated memory requirement: 1.58 GB
[WARNING] Large memory requirement (1.58 GB).
[DEBUG] Max sequence length: 1167
[DEBUG] Shape of first activation: (1, 106, 3584)
[DEBUG] Final array shape: (536, 1167, 3584)
[DEBUG] Returning (536, 1167, 3584) activations
[DEBUG] test activations shape: (536, 1167, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (536, 1167, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating validation on '94_better_spam': Got unsupported ScalarType BFloat16
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.73it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.98it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5652985074626866, 'auc': 0.6562151926932502, 'precision': 0.8723404255319149, 'recall': 0.15298507462686567, 'fpr': 0.022388059701492536}
    [BATCH] Evaluating on 94_better_spam test set
Loaded probe from results/spam_gemma_9b/seed_50/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state.npz
Converted attention probe back to bfloat16 for training
Test activations: (536, 1167, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating test on '94_better_spam': Got unsupported ScalarType BFloat16
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.04it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.94it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.95it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5652985074626866, 'auc': 0.6562151926932502, 'precision': 0.8723404255319149, 'recall': 0.15298507462686567, 'fpr': 0.022388059701492536}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
Loaded probe from results/spam_gemma_9b/seed_50/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 5157
[DEBUG] Processing 1282 full activations
[DEBUG] Estimated memory requirement: 0.60 GB
[WARNING] Large memory requirement (0.60 GB).
[DEBUG] Max sequence length: 210
[DEBUG] Shape of first activation: (1, 19, 3584)
[DEBUG] Final array shape: (1282, 210, 3584)
[DEBUG] Returning (1282, 210, 3584) activations
[DEBUG] test activations shape: (1282, 210, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (1282, 210, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating on '87_is_spam': Got unsupported ScalarType BFloat16

=== Group 31/700 — ('94_better_spam', 20, 'resid_post', 42, '{"class_counts": {"0": 1750, "1": 5}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.42it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.03it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.94it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5577223088923557, 'auc': 0.6357777556032038, 'precision': 0.7283950617283951, 'recall': 0.18408736349453977, 'fpr': 0.0686427457098284}
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1755, 3584)
[DEBUG] Returning (1755, 3584) activations
[DEBUG] train activations shape: (1755, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_max
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.88GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 335.76it/s]
[DEBUG] Final encoded shape: (1755, 16384)
Encoded feature matrix shape: (1755, 16384)
[DEBUG] After encoding. Memory: RAM: 6.88GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 16384), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 6.88GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1755, 16384)
[DEBUG] Memory after feature matrix build: RAM: 6.88GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 6.88GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 6.88GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Train activations: (1752, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1752, 3584)
Input y shape: (1752,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.69GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1752, 3584)
[DEBUG] Using pre-aggregated inputs: (1752, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.92it/s]
[DEBUG] Final encoded shape: (1752, 262144)
Encoded feature matrix shape: (1752, 262144)
[DEBUG] After encoding. Memory: RAM: 11.40GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1752, 262144), y_train=(1752,)
[DEBUG] Memory at start of feature selection: RAM: 11.40GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1752, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.40GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.40GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.40GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1752, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-1337.000000, 237.500000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 417.34it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 389.01it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 389.01it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5951492537313433, 'auc': 0.9881933615504567, 'precision': 1.0, 'recall': 0.19029850746268656, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 413.76it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 418.22it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 388.33it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5951492537313433, 'auc': 0.9881933615504567, 'precision': 1.0, 'recall': 0.19029850746268656, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-150.125000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 444.43it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 437.39it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 434.42it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5132605304212169, 'auc': 0.9135078039627046, 'precision': 1.0, 'recall': 0.0265210608424337, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.04it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.82it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.71it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5261194029850746, 'auc': 0.9150701715304076, 'precision': 1.0, 'recall': 0.05223880597014925, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1755, 3584)
[DEBUG] Returning (1755, 3584) activations
[DEBUG] train activations shape: (1755, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 231.875000]
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.87GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 334.83it/s]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Final encoded shape: (1755, 16384)
Encoded feature matrix shape: (1755, 16384)
[DEBUG] After encoding. Memory: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 16384), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1755, 16384)
[DEBUG] Memory after feature matrix build: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.78it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.90it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5261194029850746, 'auc': 0.9150701715304076, 'precision': 1.0, 'recall': 0.05223880597014925, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.42it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.20it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.04it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.6080154594639324, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: attention
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: full
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-1337.000000, 219.750000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 415.07it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 391.26it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 383.81it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7406716417910447, 'auc': 0.9315270661617286, 'precision': 0.9777777777777777, 'recall': 0.4925373134328358, 'fpr': 0.011194029850746268}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 415.81it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 411.29it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 427.90it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7406716417910447, 'auc': 0.9315270661617286, 'precision': 0.9777777777777777, 'recall': 0.4925373134328358, 'fpr': 0.011194029850746268}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-259.000000, 76.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 438.99it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 419.26it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 412.40it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5218408736349454, 'auc': 0.6747355073610121, 'precision': 0.6794871794871795, 'recall': 0.08268330733229329, 'fpr': 0.0390015600624025}
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1755, 3584)
[DEBUG] Returning (1755, 3584) activations
[DEBUG] train activations shape: (1755, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.87GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 336.03it/s]
[DEBUG] Final encoded shape: (1755, 16384)
Encoded feature matrix shape: (1755, 16384)
[DEBUG] After encoding. Memory: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 16384), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1755, 16384)
[DEBUG] Memory after feature matrix build: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 1752 full activations
[DEBUG] Estimated memory requirement: 3.32 GB
[WARNING] Large memory requirement (3.32 GB).
[DEBUG] Max sequence length: 520
[DEBUG] Shape of first activation: (1, 100, 3584)
[DEBUG] Final array shape: (1752, 520, 3584)
[DEBUG] Returning (1752, 520, 3584) activations
[DEBUG] train activations shape: (1752, 520, 3584), dtype: float16
[DEBUG] train activations range: [-1674.000000, 237.000000]
Train activations: (1752, 520, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: attention
  [DEBUG] Creating attention probe
  [DEBUG] Fitting attention probe normally...
Epoch 10/100, Loss: 0.1460
Epoch 20/100, Loss: 0.0386
Epoch 30/100, Loss: 0.0069
Epoch 40/100, Loss: 0.0013
Early stopping at epoch 44
Saved probe to results/spam_gemma_9b/seed_51/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state.npz
Saved training log to results/spam_gemma_9b/seed_51/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state_train_log.json
  - 🔥 Probe state saved to train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-1337.000000, 237.500000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 413.52it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 420.99it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 391.30it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6063432835820896, 'auc': 0.9888059701492538, 'precision': 1.0, 'recall': 0.2126865671641791, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 415.44it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 390.20it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 399.80it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6063432835820896, 'auc': 0.9888059701492538, 'precision': 1.0, 'recall': 0.2126865671641791, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-150.125000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 444.22it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 439.82it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 444.88it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.8221480185260454, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
Loaded probe from results/spam_gemma_9b/seed_51/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 536 full activations
[DEBUG] Estimated memory requirement: 1.54 GB
[WARNING] Large memory requirement (1.54 GB).
[DEBUG] Max sequence length: 1167
[DEBUG] Shape of first activation: (1, 121, 3584)
[DEBUG] Final array shape: (536, 1167, 3584)
[DEBUG] Returning (536, 1167, 3584) activations
[DEBUG] test activations shape: (536, 1167, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.500000]
Test activations: (536, 1167, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating validation on '94_better_spam': Got unsupported ScalarType BFloat16
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1755, 3584)
[DEBUG] Returning (1755, 3584) activations
[DEBUG] train activations shape: (1755, 3584), dtype: float16
[DEBUG] train activations range: [-1526.000000, 117.875000]
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_mean
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.87GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.57it/s]
Loaded probe from results/spam_gemma_9b/seed_51/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state.npz
Converted attention probe back to bfloat16 for training
Test activations: (536, 1167, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating test on '94_better_spam': Got unsupported ScalarType BFloat16
Loaded probe from results/spam_gemma_9b/seed_51/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_2_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 5157
[DEBUG] Processing 1282 full activations
[DEBUG] Estimated memory requirement: 0.60 GB
[WARNING] Large memory requirement (0.60 GB).
[DEBUG] Max sequence length: 99
[DEBUG] Shape of first activation: (1, 38, 3584)
[DEBUG] Final array shape: (1282, 99, 3584)
[DEBUG] Returning (1282, 99, 3584) activations
[DEBUG] test activations shape: (1282, 99, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (1282, 99, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating on '87_is_spam': Got unsupported ScalarType BFloat16

=== Group 32/700 — ('94_better_spam', 20, 'resid_post', 43, '{"class_counts": {"0": 1750, "1": 5}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Final encoded shape: (1755, 262144)
Encoded feature matrix shape: (1755, 262144)
[DEBUG] After encoding. Memory: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 262144), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1755, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-1337.000000, 237.500000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 418.18it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 384.27it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 386.22it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.582089552238806, 'auc': 0.9800762976163956, 'precision': 1.0, 'recall': 0.16417910447761194, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-1506.000000, 112.125000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.62it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.84it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.10it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7705223880597015, 'auc': 0.933754733793718, 'precision': 1.0, 'recall': 0.5410447761194029, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 413.56it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 403.26it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 385.51it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.582089552238806, 'auc': 0.9800762976163956, 'precision': 1.0, 'recall': 0.16417910447761194, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-147.875000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 445.37it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 433.14it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 430.78it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5015600624024961, 'auc': 0.8676745821782947, 'precision': 1.0, 'recall': 0.0031201248049922, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.61it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.89it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.87it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7705223880597015, 'auc': 0.933754733793718, 'precision': 1.0, 'recall': 0.5410447761194029, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1755, 3584)
[DEBUG] Returning (1755, 3584) activations
[DEBUG] train activations shape: (1755, 3584), dtype: float16
[DEBUG] train activations range: [-1067.000000, 73.875000]
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_mean
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1063.000000, 70.687500]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.75GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.90it/s]
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.15it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.56it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.65it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5639625585023401, 'auc': 0.6945514638058221, 'precision': 0.9270833333333334, 'recall': 0.13884555382215288, 'fpr': 0.0109204368174727}
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
[DEBUG] Final encoded shape: (1755, 262144)
Encoded feature matrix shape: (1755, 262144)
[DEBUG] After encoding. Memory: RAM: 11.47GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 262144), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 11.47GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1755, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.47GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.47GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.47GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_max
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.87GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.73it/s]
[DEBUG] Final encoded shape: (1755, 262144)
Encoded feature matrix shape: (1755, 262144)
[DEBUG] After encoding. Memory: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 262144), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1755, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-1506.000000, 112.125000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.04it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.74it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.69it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6921641791044776, 'auc': 0.9538176654043216, 'precision': 0.9904761904761905, 'recall': 0.3880597014925373, 'fpr': 0.0037313432835820895}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.63it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.86it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.15it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5970149253731343, 'auc': 0.9883465137001559, 'precision': 1.0, 'recall': 0.19402985074626866, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.01it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.68it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6921641791044776, 'auc': 0.9538176654043216, 'precision': 0.9904761904761905, 'recall': 0.3880597014925373, 'fpr': 0.0037313432835820895}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1063.000000, 70.687500]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.41it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.22it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.04it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.578003120124805, 'auc': 0.7178672170287749, 'precision': 1.0, 'recall': 0.15600624024961, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.64it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.10it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.62it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5970149253731343, 'auc': 0.9883465137001559, 'precision': 1.0, 'recall': 0.19402985074626866, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.09it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.65it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.57it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5140405616224649, 'auc': 0.9245791360515576, 'precision': 1.0, 'recall': 0.028081123244929798, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1755, 3584)
[DEBUG] Returning (1755, 3584) activations
[DEBUG] train activations shape: (1755, 3584), dtype: float16
[DEBUG] train activations range: [-162.375000, 237.000000]
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_max
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.76GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.92it/s]
[DEBUG] Final encoded shape: (1755, 262144)
Encoded feature matrix shape: (1755, 262144)
[DEBUG] After encoding. Memory: RAM: 11.47GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 262144), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 11.47GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1755, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.47GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.47GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.47GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.88GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.70it/s]
[DEBUG] Final encoded shape: (1755, 262144)
Encoded feature matrix shape: (1755, 262144)
[DEBUG] After encoding. Memory: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 262144), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1755, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-1337.000000, 237.500000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.02it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.64it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.68it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6548507462686567, 'auc': 0.9836684116729784, 'precision': 1.0, 'recall': 0.30970149253731344, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.63it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.14it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.14it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8134328358208955, 'auc': 0.8520271775451101, 'precision': 0.9719101123595506, 'recall': 0.6455223880597015, 'fpr': 0.018656716417910446}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.99it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.71it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6548507462686567, 'auc': 0.9836684116729784, 'precision': 1.0, 'recall': 0.30970149253731344, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-147.875000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.42it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.04it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.04it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.500780031201248, 'auc': 0.9302328411389187, 'precision': 1.0, 'recall': 0.0015600624024961, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.62it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.13it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.99it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8134328358208955, 'auc': 0.8520271775451101, 'precision': 0.9719101123595506, 'recall': 0.6455223880597015, 'fpr': 0.018656716417910446}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.07it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.74it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.57it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6154446177847114, 'auc': 0.7338231750798893, 'precision': 0.8109243697478992, 'recall': 0.30109204368174725, 'fpr': 0.07020280811232449}
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1755, 3584)
[DEBUG] Returning (1755, 3584) activations
[DEBUG] train activations shape: (1755, 3584), dtype: float16
[DEBUG] train activations range: [-256.250000, 92.687500]
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.76GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.90it/s]
[DEBUG] Final encoded shape: (1755, 262144)
Encoded feature matrix shape: (1755, 262144)
[DEBUG] After encoding. Memory: RAM: 11.47GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 262144), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 11.47GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1755, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.47GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.47GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.47GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.88GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.70it/s]
[DEBUG] Final encoded shape: (1755, 262144)
Encoded feature matrix shape: (1755, 262144)
[DEBUG] After encoding. Memory: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 262144), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1755, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-1337.000000, 219.750000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.72it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.77it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8544776119402985, 'auc': 0.9366367787926041, 'precision': 0.9896907216494846, 'recall': 0.7164179104477612, 'fpr': 0.007462686567164179}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.63it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.21it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.82it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5895522388059702, 'auc': 0.9922449320561373, 'precision': 1.0, 'recall': 0.1791044776119403, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.04it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.77it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.65it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8544776119402985, 'auc': 0.9366367787926041, 'precision': 0.9896907216494846, 'recall': 0.7164179104477612, 'fpr': 0.007462686567164179}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-252.250000, 76.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.41it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.03it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.07it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5639625585023401, 'auc': 0.6587089692636067, 'precision': 0.7009803921568627, 'recall': 0.22308892355694226, 'fpr': 0.09516380655226209}
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.60it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.84it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.77it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5895522388059702, 'auc': 0.9922449320561373, 'precision': 1.0, 'recall': 0.1791044776119403, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.11it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.62it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.63it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5054602184087363, 'auc': 0.8730094601599976, 'precision': 1.0, 'recall': 0.0109204368174727, 'fpr': 0.0}
  🚀 [BATCH] Training probe: attention
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: full
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1755, 3584)
[DEBUG] Returning (1755, 3584) activations
[DEBUG] train activations shape: (1755, 3584), dtype: float16
[DEBUG] train activations range: [-162.375000, 237.000000]
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.76GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.90it/s]
[DEBUG] Final encoded shape: (1755, 262144)
Encoded feature matrix shape: (1755, 262144)
[DEBUG] After encoding. Memory: RAM: 11.47GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 262144), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 11.47GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1755, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.47GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.47GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.47GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.00it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.27it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5970149253731343, 'auc': 0.9694531075963467, 'precision': 1.0, 'recall': 0.19402985074626866, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 1755 full activations
[DEBUG] Estimated memory requirement: 3.30 GB
[WARNING] Large memory requirement (3.30 GB).
[DEBUG] Max sequence length: 520
[DEBUG] Shape of first activation: (1, 70, 3584)
[DEBUG] Final array shape: (1755, 520, 3584)
[DEBUG] Returning (1755, 520, 3584) activations
[DEBUG] train activations shape: (1755, 520, 3584), dtype: float16
[DEBUG] train activations range: [-1674.000000, 237.000000]
Train activations: (1755, 520, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: attention
  [DEBUG] Creating attention probe
  [DEBUG] Fitting attention probe normally...
Epoch 10/100, Loss: 0.1534
Epoch 20/100, Loss: 0.0497
Epoch 30/100, Loss: 0.0254
Epoch 40/100, Loss: 0.0147
Early stopping at epoch 49
Saved probe to results/spam_gemma_9b/seed_42/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state.npz
Saved training log to results/spam_gemma_9b/seed_42/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state_train_log.json
  - 🔥 Probe state saved to train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.73it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5970149253731343, 'auc': 0.9694531075963467, 'precision': 1.0, 'recall': 0.19402985074626866, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.40it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.19it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.92it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.8959747469461962, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: attention
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: full
Loaded probe from results/spam_gemma_9b/seed_42/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 536 full activations
[DEBUG] Estimated memory requirement: 1.58 GB
[WARNING] Large memory requirement (1.58 GB).
[DEBUG] Max sequence length: 939
[DEBUG] Shape of first activation: (1, 151, 3584)
[DEBUG] Final array shape: (536, 939, 3584)
[DEBUG] Returning (536, 939, 3584) activations
[DEBUG] test activations shape: (536, 939, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.500000]
Test activations: (536, 939, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating validation on '94_better_spam': Got unsupported ScalarType BFloat16
    [BATCH] Evaluating on 94_better_spam test set
Loaded probe from results/spam_gemma_9b/seed_42/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state.npz
Converted attention probe back to bfloat16 for training
Test activations: (536, 939, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating test on '94_better_spam': Got unsupported ScalarType BFloat16
Loaded probe from results/spam_gemma_9b/seed_42/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 5157
[DEBUG] Processing 1282 full activations
[DEBUG] Estimated memory requirement: 0.60 GB
[WARNING] Large memory requirement (0.60 GB).
[DEBUG] Max sequence length: 210
[DEBUG] Shape of first activation: (1, 54, 3584)
[DEBUG] Final array shape: (1282, 210, 3584)
[DEBUG] Returning (1282, 210, 3584) activations
[DEBUG] test activations shape: (1282, 210, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (1282, 210, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating on '87_is_spam': Got unsupported ScalarType BFloat16

=== Group 33/700 — ('94_better_spam', 20, 'resid_post', 44, '{"class_counts": {"0": 1750, "1": 5}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 1755 full activations
[DEBUG] Estimated memory requirement: 3.20 GB
[WARNING] Large memory requirement (3.20 GB).
[DEBUG] Max sequence length: 520
[DEBUG] Shape of first activation: (1, 247, 3584)
[DEBUG] Final array shape: (1755, 520, 3584)
[DEBUG] Returning (1755, 520, 3584) activations
[DEBUG] train activations shape: (1755, 520, 3584), dtype: float16
[DEBUG] train activations range: [-1674.000000, 237.000000]
Train activations: (1755, 520, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: attention
  [DEBUG] Creating attention probe
  [DEBUG] Fitting attention probe normally...
Epoch 10/100, Loss: 0.2137
Epoch 20/100, Loss: 0.0806
Epoch 30/100, Loss: 0.0351
Epoch 40/100, Loss: 0.0135
Epoch 50/100, Loss: 0.0045
Early stopping at epoch 57
Saved probe to results/spam_gemma_9b/seed_43/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state.npz
Saved training log to results/spam_gemma_9b/seed_43/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state_train_log.json
  - 🔥 Probe state saved to train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-593.000000, 51.843750]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 416.80it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 383.67it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 388.40it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7033582089552238, 'auc': 0.9252339051013589, 'precision': 0.990990990990991, 'recall': 0.41044776119402987, 'fpr': 0.0037313432835820895}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 415.90it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 396.62it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 398.62it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7033582089552238, 'auc': 0.9252339051013589, 'precision': 0.990990990990991, 'recall': 0.41044776119402987, 'fpr': 0.0037313432835820895}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1053.000000, 69.625000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 446.73it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 424.55it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 432.58it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6318252730109204, 'auc': 0.8539674504296864, 'precision': 0.9567567567567568, 'recall': 0.27613104524180965, 'fpr': 0.0124804992199688}
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
Loaded probe from results/spam_gemma_9b/seed_43/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 536 full activations
[DEBUG] Estimated memory requirement: 1.61 GB
[WARNING] Large memory requirement (1.61 GB).
[DEBUG] Max sequence length: 939
[DEBUG] Shape of first activation: (1, 62, 3584)
[DEBUG] Final array shape: (536, 939, 3584)
[DEBUG] Returning (536, 939, 3584) activations
[DEBUG] test activations shape: (536, 939, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.500000]
Test activations: (536, 939, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating validation on '94_better_spam': Got unsupported ScalarType BFloat16
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1755, 3584)
[DEBUG] Returning (1755, 3584) activations
[DEBUG] train activations shape: (1755, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_max
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.88GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 337.08it/s]
[DEBUG] Final encoded shape: (1755, 16384)
Encoded feature matrix shape: (1755, 16384)
[DEBUG] After encoding. Memory: RAM: 6.88GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 16384), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 6.88GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1755, 16384)
[DEBUG] Memory after feature matrix build: RAM: 6.88GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 6.88GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 6.88GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Loaded probe from results/spam_gemma_9b/seed_43/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state.npz
Converted attention probe back to bfloat16 for training
Test activations: (536, 939, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating test on '94_better_spam': Got unsupported ScalarType BFloat16
Loaded probe from results/spam_gemma_9b/seed_43/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 5157
[DEBUG] Processing 1282 full activations
[DEBUG] Estimated memory requirement: 0.60 GB
[WARNING] Large memory requirement (0.60 GB).
[DEBUG] Max sequence length: 119
[DEBUG] Shape of first activation: (1, 8, 3584)
[DEBUG] Final array shape: (1282, 119, 3584)
[DEBUG] Returning (1282, 119, 3584) activations
[DEBUG] test activations shape: (1282, 119, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (1282, 119, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating on '87_is_spam': Got unsupported ScalarType BFloat16

=== Group 34/700 — ('94_better_spam', 20, 'resid_post', 45, '{"class_counts": {"0": 1750, "1": 5}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-109.562500, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 412.42it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 382.34it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 385.08it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5317164179104478, 'auc': 0.9764145689463133, 'precision': 1.0, 'recall': 0.06343283582089553, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1755, 3584)
[DEBUG] Returning (1755, 3584) activations
[DEBUG] train activations shape: (1755, 3584), dtype: float16
[DEBUG] train activations range: [-1506.000000, 112.125000]
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_mean
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.76GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 336.84it/s]
[DEBUG] Final encoded shape: (1755, 16384)
Encoded feature matrix shape: (1755, 16384)
[DEBUG] After encoding. Memory: RAM: 9.76GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 16384), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 9.76GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1755, 16384)
[DEBUG] Memory after feature matrix build: RAM: 9.76GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 9.76GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 9.76GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 413.80it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 416.51it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 415.69it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5317164179104478, 'auc': 0.9764145689463133, 'precision': 1.0, 'recall': 0.06343283582089553, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-143.875000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 445.44it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 420.76it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 405.23it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.7723136382553586, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-677.000000, 50.625000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 416.85it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 385.86it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 414.05it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.621268656716418, 'auc': 0.9076770995767431, 'precision': 0.9850746268656716, 'recall': 0.2462686567164179, 'fpr': 0.0037313432835820895}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1755, 3584)
[DEBUG] Returning (1755, 3584) activations
[DEBUG] train activations shape: (1755, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 231.875000]
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.85GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 336.42it/s]
[DEBUG] Final encoded shape: (1755, 16384)
Encoded feature matrix shape: (1755, 16384)
[DEBUG] After encoding. Memory: RAM: 6.88GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 16384), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 6.88GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1755, 16384)
[DEBUG] Memory after feature matrix build: RAM: 6.88GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 6.88GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 6.88GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 418.72it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 383.88it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 421.79it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.621268656716418, 'auc': 0.9076770995767431, 'precision': 0.9850746268656716, 'recall': 0.2462686567164179, 'fpr': 0.0037313432835820895}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1505.000000, 110.875000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 441.13it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 435.25it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 413.29it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5335413416536662, 'auc': 0.808584480664718, 'precision': 0.9056603773584906, 'recall': 0.0748829953198128, 'fpr': 0.0078003120124804995}
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-238.250000, 80.812500]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 414.42it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 389.88it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 377.66it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6548507462686567, 'auc': 0.8900924482067275, 'precision': 0.9882352941176471, 'recall': 0.31343283582089554, 'fpr': 0.0037313432835820895}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1755, 3584)
[DEBUG] Returning (1755, 3584) activations
[DEBUG] train activations shape: (1755, 3584), dtype: float16
[DEBUG] train activations range: [-1337.000000, 237.000000]
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_max
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.76GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 335.93it/s]
[DEBUG] Final encoded shape: (1755, 16384)
Encoded feature matrix shape: (1755, 16384)
[DEBUG] After encoding. Memory: RAM: 9.76GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 16384), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 9.76GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1755, 16384)
[DEBUG] Memory after feature matrix build: RAM: 9.76GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 9.76GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 9.76GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 413.07it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 397.34it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 387.00it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6548507462686567, 'auc': 0.8900924482067275, 'precision': 0.9882352941176471, 'recall': 0.31343283582089554, 'fpr': 0.0037313432835820895}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-274.000000, 76.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 441.00it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 433.92it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 410.68it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.516380655226209, 'auc': 0.7335213845371287, 'precision': 0.6521739130434783, 'recall': 0.07020280811232449, 'fpr': 0.0374414976599064}
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-120.437500, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 415.65it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 380.85it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 388.36it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.539179104477612, 'auc': 0.966250835375362, 'precision': 1.0, 'recall': 0.07835820895522388, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1755, 3584)
[DEBUG] Returning (1755, 3584) activations
[DEBUG] train activations shape: (1755, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.84GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 337.39it/s]
[DEBUG] Final encoded shape: (1755, 16384)
Encoded feature matrix shape: (1755, 16384)
[DEBUG] After encoding. Memory: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 16384), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1755, 16384)
[DEBUG] Memory after feature matrix build: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 416.35it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 385.22it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 401.18it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.539179104477612, 'auc': 0.966250835375362, 'precision': 1.0, 'recall': 0.07835820895522388, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1336.000000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 442.25it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 426.40it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 407.99it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.8683146701843112, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-109.625000, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 411.93it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 423.97it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 388.07it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5223880597014925, 'auc': 0.9600412118511917, 'precision': 0.9285714285714286, 'recall': 0.048507462686567165, 'fpr': 0.0037313432835820895}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1755, 3584)
[DEBUG] Returning (1755, 3584) activations
[DEBUG] train activations shape: (1755, 3584), dtype: float16
[DEBUG] train activations range: [-1337.000000, 219.750000]
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.77GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 336.53it/s]
[DEBUG] Final encoded shape: (1755, 16384)
Encoded feature matrix shape: (1755, 16384)
[DEBUG] After encoding. Memory: RAM: 9.77GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 16384), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 9.77GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1755, 16384)
[DEBUG] Memory after feature matrix build: RAM: 9.77GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 9.77GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 9.77GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 409.76it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 424.70it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 415.20it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5223880597014925, 'auc': 0.9600412118511917, 'precision': 0.9285714285714286, 'recall': 0.048507462686567165, 'fpr': 0.0037313432835820895}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-143.875000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 441.44it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 409.54it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 428.69it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.6741513966330884, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-227.500000, 81.187500]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 416.31it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 385.44it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 407.06it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6977611940298507, 'auc': 0.9218367119625752, 'precision': 0.9344262295081968, 'recall': 0.4253731343283582, 'fpr': 0.029850746268656716}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1755, 3584)
[DEBUG] Returning (1755, 3584) activations
[DEBUG] train activations shape: (1755, 3584), dtype: float16
[DEBUG] train activations range: [-1526.000000, 117.875000]
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_mean
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.86GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.53it/s]
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 413.19it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 384.90it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 375.73it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6977611940298507, 'auc': 0.9218367119625752, 'precision': 0.9344262295081968, 'recall': 0.4253731343283582, 'fpr': 0.029850746268656716}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1336.000000, 221.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 437.66it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 407.39it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 393.78it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5210608424336973, 'auc': 0.6753123167048366, 'precision': 0.6626506024096386, 'recall': 0.08580343213728549, 'fpr': 0.0436817472698908}
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
[DEBUG] Final encoded shape: (1755, 262144)
Encoded feature matrix shape: (1755, 262144)
[DEBUG] After encoding. Memory: RAM: 8.57GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 262144), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 8.57GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1755, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.57GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.57GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.57GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1755, 3584)
[DEBUG] Returning (1755, 3584) activations
[DEBUG] train activations shape: (1755, 3584), dtype: float16
[DEBUG] train activations range: [-1337.000000, 237.000000]
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.77GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 330.29it/s]
[DEBUG] Final encoded shape: (1755, 16384)
Encoded feature matrix shape: (1755, 16384)
[DEBUG] After encoding. Memory: RAM: 9.77GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 16384), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 9.77GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1755, 16384)
[DEBUG] Memory after feature matrix build: RAM: 9.77GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 9.77GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 9.77GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.61it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.97it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.95it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6958955223880597, 'auc': 0.8887976163956337, 'precision': 0.9906542056074766, 'recall': 0.39552238805970147, 'fpr': 0.0037313432835820895}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-120.625000, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 410.80it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 388.18it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 389.95it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5597014925373134, 'auc': 0.9740615950100245, 'precision': 1.0, 'recall': 0.11940298507462686, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.64it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.71it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 35.60it/s]
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 381.23it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 342.11it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 334.87it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6958955223880597, 'auc': 0.8887976163956337, 'precision': 0.9906542056074766, 'recall': 0.39552238805970147, 'fpr': 0.0037313432835820895}
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5597014925373134, 'auc': 0.9740615950100245, 'precision': 1.0, 'recall': 0.11940298507462686, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1336.000000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 441.44it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 436.04it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 411.49it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5031201248049922, 'auc': 0.8295686585653752, 'precision': 1.0, 'recall': 0.0062402496099844, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.10it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.57it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.65it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5288611544461779, 'auc': 0.4989546851764866, 'precision': 0.8363636363636363, 'recall': 0.0717628705148206, 'fpr': 0.014040561622464899}
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_mean
  [DEBUG] Fitting SAE probe normally...
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_max
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.77GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.91it/s]

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.86GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.69it/s]
[DEBUG] Final encoded shape: (1755, 262144)
Encoded feature matrix shape: (1755, 262144)
[DEBUG] After encoding. Memory: RAM: 8.57GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 262144), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 8.57GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1755, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.57GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.57GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.57GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Final encoded shape: (1755, 262144)
Encoded feature matrix shape: (1755, 262144)
[DEBUG] After encoding. Memory: RAM: 11.48GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 262144), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 11.48GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1755, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.48GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.48GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.48GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.67it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.26it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.95it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5541044776119403, 'auc': 0.967907663176654, 'precision': 1.0, 'recall': 0.10820895522388059, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.01it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.67it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.55it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6380597014925373, 'auc': 0.8910809757184229, 'precision': 0.9868421052631579, 'recall': 0.2798507462686567, 'fpr': 0.0037313432835820895}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.67it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.92it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.92it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5541044776119403, 'auc': 0.967907663176654, 'precision': 1.0, 'recall': 0.10820895522388059, 'fpr': 0.0}
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.06it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.01it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.95it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6380597014925373, 'auc': 0.8910809757184229, 'precision': 0.9868421052631579, 'recall': 0.2798507462686567, 'fpr': 0.0037313432835820895}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.14it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.69it/s]
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s][DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 25.90it/s]
Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 31.43it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.98it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.500780031201248, 'auc': 0.7458363857175192, 'precision': 1.0, 'recall': 0.0015600624024961, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.93it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5733229329173167, 'auc': 0.8091710251873414, 'precision': 0.9272727272727272, 'recall': 0.15912636505460218, 'fpr': 0.0124804992199688}
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_max
  [DEBUG] Fitting SAE probe normally...
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.86GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]
=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.77GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations:  50%|███████████████████████████████████████▌                                       | 1/2 [00:00<00:00,  7.39it/s]Encoding activations:  50%|███████████████████████████████████████▌                                       | 1/2 [00:00<00:00,  7.03it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 11.38it/s]
Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 11.38it/s]
[DEBUG] Final encoded shape: (1755, 262144)
Encoded feature matrix shape: (1755, 262144)
[DEBUG] After encoding. Memory: RAM: 8.57GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 262144), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 8.57GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1755, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.57GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.57GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.57GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Final encoded shape: (1755, 262144)
Encoded feature matrix shape: (1755, 262144)
[DEBUG] After encoding. Memory: RAM: 11.47GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 262144), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 11.47GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1755, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.47GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.47GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.47GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.69it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.94it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.14it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7817164179104478, 'auc': 0.8763505235018934, 'precision': 0.968944099378882, 'recall': 0.582089552238806, 'fpr': 0.018656716417910446}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.05it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.91it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.02it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5559701492537313, 'auc': 0.9805635999108933, 'precision': 1.0, 'recall': 0.11194029850746269, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.64it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.96it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.20it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7817164179104478, 'auc': 0.8763505235018934, 'precision': 0.968944099378882, 'recall': 0.582089552238806, 'fpr': 0.018656716417910446}
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.01it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.98it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.81it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5559701492537313, 'auc': 0.9805635999108933, 'precision': 1.0, 'recall': 0.11194029850746269, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.13it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.69it/s]
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.31it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.66it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.93it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6053042121684867, 'auc': 0.7141678490852583, 'precision': 0.8026905829596412, 'recall': 0.2792511700468019, 'fpr': 0.0686427457098284}
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.07it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.7970945358875199, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_softmax
  [DEBUG] Fitting SAE probe normally...
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.86GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 20.23it/s]

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.76GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.83it/s]
[DEBUG] Final encoded shape: (1755, 262144)
Encoded feature matrix shape: (1755, 262144)
[DEBUG] After encoding. Memory: RAM: 11.47GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 262144), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 11.47GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1755, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.47GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.48GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.48GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Final encoded shape: (1755, 262144)
Encoded feature matrix shape: (1755, 262144)
[DEBUG] After encoding. Memory: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 262144), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1755, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.04it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.01it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.99it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7854477611940298, 'auc': 0.8766568278012921, 'precision': 0.9421965317919075, 'recall': 0.6082089552238806, 'fpr': 0.03731343283582089}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.66it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.86it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.75it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5485074626865671, 'auc': 0.955627645355313, 'precision': 1.0, 'recall': 0.09701492537313433, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.06it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.79it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.77it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7854477611940298, 'auc': 0.8766568278012921, 'precision': 0.9421965317919075, 'recall': 0.6082089552238806, 'fpr': 0.03731343283582089}
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.65it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.98it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.86it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5485074626865671, 'auc': 0.955627645355313, 'precision': 1.0, 'recall': 0.09701492537313433, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.40it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 31.35it/s]
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.08it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.10it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.67it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7051482059282371, 'auc': 0.8149099130891913, 'precision': 0.8767908309455588, 'recall': 0.47737909516380655, 'fpr': 0.06708268330733229}
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.73it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.77413898428012, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: attention
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: full
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.76GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.92it/s]
[DEBUG] Final encoded shape: (1755, 262144)
Encoded feature matrix shape: (1755, 262144)
[DEBUG] After encoding. Memory: RAM: 11.47GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 262144), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 11.47GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1755, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.47GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.48GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.48GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.04it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.00it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.21it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5466417910447762, 'auc': 0.9637864780574739, 'precision': 1.0, 'recall': 0.09328358208955224, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.05it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.29it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.00it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5466417910447762, 'auc': 0.9637864780574739, 'precision': 1.0, 'recall': 0.09328358208955224, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 1755 full activations
[DEBUG] Estimated memory requirement: 3.36 GB
[WARNING] Large memory requirement (3.36 GB).
[DEBUG] Max sequence length: 1167
[DEBUG] Shape of first activation: (1, 183, 3584)
[DEBUG] Final array shape: (1755, 1167, 3584)
[DEBUG] Returning (1755, 1167, 3584) activations
[DEBUG] train activations shape: (1755, 1167, 3584), dtype: float16
[DEBUG] train activations range: [-1674.000000, 237.000000]
Train activations: (1755, 1167, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: attention
  [DEBUG] Creating attention probe
  [DEBUG] Fitting attention probe normally...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations:  50%|███████████████████████████████████████▌                                       | 1/2 [00:00<00:00,  6.89it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 13.63it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations:  50%|███████████████████████████████████████▌                                       | 1/2 [00:00<00:00,  9.24it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 17.59it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations:  50%|███████████████████████████████████████▌                                       | 1/2 [00:00<00:00,  9.20it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 17.51it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.6665068474813876, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: attention
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: full
Epoch 10/100, Loss: 0.3356
Epoch 20/100, Loss: 0.1543
Epoch 30/100, Loss: 0.0732
Epoch 40/100, Loss: 0.0343
Epoch 50/100, Loss: 0.0194
Epoch 60/100, Loss: 0.0119
Early stopping at epoch 65
Saved probe to results/spam_gemma_9b/seed_44/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state.npz
Saved training log to results/spam_gemma_9b/seed_44/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state_train_log.json
  - 🔥 Probe state saved to train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Loaded probe from results/spam_gemma_9b/seed_44/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 536 full activations
[DEBUG] Estimated memory requirement: 1.59 GB
[WARNING] Large memory requirement (1.59 GB).
[DEBUG] Max sequence length: 935
[DEBUG] Shape of first activation: (1, 56, 3584)
[DEBUG] Final array shape: (536, 935, 3584)
[DEBUG] Returning (536, 935, 3584) activations
[DEBUG] test activations shape: (536, 935, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (536, 935, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating validation on '94_better_spam': Got unsupported ScalarType BFloat16
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 1755 full activations
[DEBUG] Estimated memory requirement: 3.27 GB
[WARNING] Large memory requirement (3.27 GB).
[DEBUG] Max sequence length: 501
[DEBUG] Shape of first activation: (1, 50, 3584)
[DEBUG] Final array shape: (1755, 501, 3584)
[DEBUG] Returning (1755, 501, 3584) activations
[DEBUG] train activations shape: (1755, 501, 3584), dtype: float16
[DEBUG] train activations range: [-1674.000000, 237.000000]
Train activations: (1755, 501, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: attention
  [DEBUG] Creating attention probe
  [DEBUG] Fitting attention probe normally...
Epoch 10/100, Loss: 0.1758
Epoch 20/100, Loss: 0.0638
Epoch 30/100, Loss: 0.0320
Epoch 40/100, Loss: 0.0148
Epoch 50/100, Loss: 0.0068
Epoch 60/100, Loss: 0.0026
Early stopping at epoch 63
Saved probe to results/spam_gemma_9b/seed_45/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state.npz
Saved training log to results/spam_gemma_9b/seed_45/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state_train_log.json
  - 🔥 Probe state saved to train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Loaded probe from results/spam_gemma_9b/seed_44/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state.npz
Converted attention probe back to bfloat16 for training
Test activations: (536, 935, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating test on '94_better_spam': Got unsupported ScalarType BFloat16
Loaded probe from results/spam_gemma_9b/seed_44/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 5157
[DEBUG] Processing 1282 full activations
[DEBUG] Estimated memory requirement: 0.60 GB
[WARNING] Large memory requirement (0.60 GB).
[DEBUG] Max sequence length: 163
[DEBUG] Shape of first activation: (1, 45, 3584)
[DEBUG] Final array shape: (1282, 163, 3584)
[DEBUG] Returning (1282, 163, 3584) activations
[DEBUG] test activations shape: (1282, 163, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (1282, 163, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating on '87_is_spam': Got unsupported ScalarType BFloat16

=== Group 35/700 — ('94_better_spam', 20, 'resid_post', 46, '{"class_counts": {"0": 1750, "1": 5}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1755, 3584)
[DEBUG] Returning (1755, 3584) activations
[DEBUG] train activations shape: (1755, 3584), dtype: float16
[DEBUG] train activations range: [-1337.000000, 237.000000]
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_max
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.84GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 336.84it/s]
[DEBUG] Final encoded shape: (1755, 16384)
Encoded feature matrix shape: (1755, 16384)
[DEBUG] After encoding. Memory: RAM: 6.84GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 16384), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 6.84GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1755, 16384)
[DEBUG] Memory after feature matrix build: RAM: 6.84GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 6.84GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 6.84GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Loaded probe from results/spam_gemma_9b/seed_45/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 536 full activations
[DEBUG] Estimated memory requirement: 1.55 GB
[WARNING] Large memory requirement (1.55 GB).
[DEBUG] Max sequence length: 939
[DEBUG] Shape of first activation: (1, 61, 3584)
[DEBUG] Final array shape: (536, 939, 3584)
[DEBUG] Returning (536, 939, 3584) activations
[DEBUG] test activations shape: (536, 939, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (536, 939, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating validation on '94_better_spam': Got unsupported ScalarType BFloat16
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-162.375000, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 418.30it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 383.67it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 385.44it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5410447761194029, 'auc': 0.9805357540654934, 'precision': 1.0, 'recall': 0.08208955223880597, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Loaded probe from results/spam_gemma_9b/seed_45/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state.npz
Converted attention probe back to bfloat16 for training
Test activations: (536, 939, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating test on '94_better_spam': Got unsupported ScalarType BFloat16
Loaded probe from results/spam_gemma_9b/seed_45/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 5157
[DEBUG] Processing 1282 full activations
[DEBUG] Estimated memory requirement: 0.60 GB
[WARNING] Large memory requirement (0.60 GB).
[DEBUG] Max sequence length: 210
[DEBUG] Shape of first activation: (1, 65, 3584)
[DEBUG] Final array shape: (1282, 210, 3584)
[DEBUG] Returning (1282, 210, 3584) activations
[DEBUG] test activations shape: (1282, 210, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (1282, 210, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating on '87_is_spam': Got unsupported ScalarType BFloat16

=== Group 36/700 — ('94_better_spam', 20, 'resid_post', 47, '{"class_counts": {"0": 1750, "1": 5}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 418.09it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 414.95it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 394.42it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5410447761194029, 'auc': 0.9805357540654934, 'precision': 1.0, 'recall': 0.08208955223880597, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1306.000000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 438.46it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 414.87it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 433.50it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5046801872074883, 'auc': 0.9168080295754732, 'precision': 1.0, 'recall': 0.0093603744149766, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1755, 3584)
[DEBUG] Returning (1755, 3584) activations
[DEBUG] train activations shape: (1755, 3584), dtype: float16
[DEBUG] train activations range: [-1526.000000, 117.875000]
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_mean
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.71GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 336.97it/s]
[DEBUG] Final encoded shape: (1755, 16384)
Encoded feature matrix shape: (1755, 16384)
[DEBUG] After encoding. Memory: RAM: 9.71GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 16384), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 9.71GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1755, 16384)
[DEBUG] Memory after feature matrix build: RAM: 9.71GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 9.71GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 9.71GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1755, 3584)
[DEBUG] Returning (1755, 3584) activations
[DEBUG] train activations shape: (1755, 3584), dtype: float16
[DEBUG] train activations range: [-1337.000000, 219.750000]
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.86GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 335.46it/s]
[DEBUG] Final encoded shape: (1755, 16384)
Encoded feature matrix shape: (1755, 16384)
[DEBUG] After encoding. Memory: RAM: 6.83GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 16384), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 6.83GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1755, 16384)
[DEBUG] Memory after feature matrix build: RAM: 6.83GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 6.83GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 6.83GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-570.000000, 51.906250]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 414.62it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 385.36it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 388.97it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7126865671641791, 'auc': 0.8795388728001782, 'precision': 1.0, 'recall': 0.4253731343283582, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-231.750000, 90.750000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 410.80it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 389.19it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 407.61it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.75, 'auc': 0.933991423479617, 'precision': 0.9926470588235294, 'recall': 0.503731343283582, 'fpr': 0.0037313432835820895}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 417.63it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 383.53it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 416.23it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7126865671641791, 'auc': 0.8795388728001782, 'precision': 1.0, 'recall': 0.4253731343283582, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1049.000000, 65.937500]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 444.19it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 410.96it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 409.74it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7277691107644306, 'auc': 0.89632764717765, 'precision': 0.974025974025974, 'recall': 0.46801872074882994, 'fpr': 0.0124804992199688}
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 410.40it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 391.92it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 409.56it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.75, 'auc': 0.933991423479617, 'precision': 0.9926470588235294, 'recall': 0.503731343283582, 'fpr': 0.0037313432835820895}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1306.000000, 211.500000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 435.00it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 410.10it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 410.02it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5187207488299532, 'auc': 0.7006383843497266, 'precision': 0.6538461538461539, 'recall': 0.07956318252730109, 'fpr': 0.0421216848673947}
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1755, 3584)
[DEBUG] Returning (1755, 3584) activations
[DEBUG] train activations shape: (1755, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_max
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.71GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 337.01it/s]
[DEBUG] Final encoded shape: (1755, 16384)
Encoded feature matrix shape: (1755, 16384)
[DEBUG] After encoding. Memory: RAM: 9.71GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 16384), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 9.71GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1755, 16384)
[DEBUG] Memory after feature matrix build: RAM: 9.71GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 9.71GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 9.71GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1755, 3584)
[DEBUG] Returning (1755, 3584) activations
[DEBUG] train activations shape: (1755, 3584), dtype: float16
[DEBUG] train activations range: [-1337.000000, 237.000000]
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.87GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 335.89it/s]
[DEBUG] Final encoded shape: (1755, 16384)
Encoded feature matrix shape: (1755, 16384)
[DEBUG] After encoding. Memory: RAM: 6.83GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 16384), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 6.83GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1755, 16384)
[DEBUG] Memory after feature matrix build: RAM: 6.83GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 6.83GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 6.83GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-106.125000, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 412.01it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 387.43it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 385.08it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5503731343283582, 'auc': 0.9366924704834039, 'precision': 1.0, 'recall': 0.10074626865671642, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-162.375000, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 415.07it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 389.26it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 426.21it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5373134328358209, 'auc': 0.9846708621073736, 'precision': 1.0, 'recall': 0.07462686567164178, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 414.99it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 378.48it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 387.29it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5503731343283582, 'auc': 0.9366924704834039, 'precision': 1.0, 'recall': 0.10074626865671642, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-177.375000, 237.500000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 443.75it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 413.66it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 430.89it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.8422097882355233, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 416.35it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 388.22it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 393.54it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5373134328358209, 'auc': 0.9846708621073736, 'precision': 1.0, 'recall': 0.07462686567164178, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1306.000000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 444.22it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 427.29it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 410.54it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5046801872074883, 'auc': 0.8684120219722986, 'precision': 1.0, 'recall': 0.0093603744149766, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1755, 3584)
[DEBUG] Returning (1755, 3584) activations
[DEBUG] train activations shape: (1755, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 231.875000]
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.72GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 337.73it/s]
[DEBUG] Final encoded shape: (1755, 16384)
Encoded feature matrix shape: (1755, 16384)
[DEBUG] After encoding. Memory: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 16384), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1755, 16384)
[DEBUG] Memory after feature matrix build: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1755, 3584)
[DEBUG] Returning (1755, 3584) activations
[DEBUG] train activations shape: (1755, 3584), dtype: float16
[DEBUG] train activations range: [-1506.000000, 112.125000]
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_mean
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.86GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.58it/s]
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-237.375000, 80.312500]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 406.11it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 360.68it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 386.18it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6865671641791045, 'auc': 0.9093896190688349, 'precision': 0.9545454545454546, 'recall': 0.3917910447761194, 'fpr': 0.018656716417910446}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Final encoded shape: (1755, 262144)
Encoded feature matrix shape: (1755, 262144)
[DEBUG] After encoding. Memory: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 262144), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1755, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 414.87it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 385.12it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 389.55it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6865671641791045, 'auc': 0.9093896190688349, 'precision': 0.9545454545454546, 'recall': 0.3917910447761194, 'fpr': 0.018656716417910446}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-1053.000000, 68.750000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-268.250000, 76.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 442.39it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 426.06it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 411.49it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5124804992199687, 'auc': 0.5893020120180783, 'precision': 0.5305343511450382, 'recall': 0.21684867394695787, 'fpr': 0.1918876755070203}
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.63it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.74it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6436567164179104, 'auc': 0.9031243038538649, 'precision': 0.9873417721518988, 'recall': 0.291044776119403, 'fpr': 0.0037313432835820895}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1755, 3584)
[DEBUG] Returning (1755, 3584) activations
[DEBUG] train activations shape: (1755, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.72GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 337.91it/s]
[DEBUG] Final encoded shape: (1755, 16384)
Encoded feature matrix shape: (1755, 16384)
[DEBUG] After encoding. Memory: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 16384), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1755, 16384)
[DEBUG] Memory after feature matrix build: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.62it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 32.79it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.97it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6436567164179104, 'auc': 0.9031243038538649, 'precision': 0.9873417721518988, 'recall': 0.291044776119403, 'fpr': 0.0037313432835820895}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1490.000000, 111.750000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-106.500000, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 418.80it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 377.29it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 394.87it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5485074626865671, 'auc': 0.9574097794609043, 'precision': 1.0, 'recall': 0.09701492537313433, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.08it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.58it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.57it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5998439937597504, 'auc': 0.5081885022670798, 'precision': 0.9923076923076923, 'recall': 0.20124804992199688, 'fpr': 0.0015600624024961}
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 414.05it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 385.15it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 413.07it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5485074626865671, 'auc': 0.9574097794609043, 'precision': 1.0, 'recall': 0.09701492537313433, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-177.375000, 237.500000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 441.39it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 402.00it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 429.44it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.4984399375975039, 'auc': 0.7516628902285576, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0031201248049922}
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_max
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.86GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.72it/s]
[DEBUG] Final encoded shape: (1755, 262144)
Encoded feature matrix shape: (1755, 262144)
[DEBUG] After encoding. Memory: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 262144), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1755, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_mean
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.72GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.90it/s]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Final encoded shape: (1755, 262144)
Encoded feature matrix shape: (1755, 262144)
[DEBUG] After encoding. Memory: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 262144), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1755, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.75it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.19it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.11it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5708955223880597, 'auc': 0.9857011583871687, 'precision': 1.0, 'recall': 0.1417910447761194, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.98it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.96it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7332089552238806, 'auc': 0.9020522388059701, 'precision': 0.9921259842519685, 'recall': 0.4701492537313433, 'fpr': 0.0037313432835820895}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.59it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.79it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.72it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5708955223880597, 'auc': 0.9857011583871687, 'precision': 1.0, 'recall': 0.1417910447761194, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.11it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.61it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.58it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5039001560062403, 'auc': 0.9300308361788449, 'precision': 1.0, 'recall': 0.0078003120124804995, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.02it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.93it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.78it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7332089552238806, 'auc': 0.9020522388059701, 'precision': 0.9921259842519685, 'recall': 0.4701492537313433, 'fpr': 0.0037313432835820895}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_last
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.41it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.07it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.05it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.4906396255850234, 'auc': 0.5462773893171016, 'precision': 0.07142857142857142, 'recall': 0.0015600624024961, 'fpr': 0.0202808112324493}
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.86GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.71it/s]
[DEBUG] Final encoded shape: (1755, 262144)
Encoded feature matrix shape: (1755, 262144)
[DEBUG] After encoding. Memory: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 262144), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1755, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_max
  [DEBUG] Fitting SAE probe normally...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.73GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.90it/s]
[DEBUG] Final encoded shape: (1755, 262144)
Encoded feature matrix shape: (1755, 262144)
[DEBUG] After encoding. Memory: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 262144), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1755, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.69it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.08it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.99it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8190298507462687, 'auc': 0.931053686789931, 'precision': 0.9776536312849162, 'recall': 0.6529850746268657, 'fpr': 0.014925373134328358}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.07it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.74it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.78it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5559701492537313, 'auc': 0.9685759634662507, 'precision': 1.0, 'recall': 0.11194029850746269, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.62it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.79it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.85it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8190298507462687, 'auc': 0.931053686789931, 'precision': 0.9776536312849162, 'recall': 0.6529850746268657, 'fpr': 0.014925373134328358}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.10it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.70it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.51it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.625585023400936, 'auc': 0.803198492994322, 'precision': 0.8232931726907631, 'recall': 0.31981279251170047, 'fpr': 0.0686427457098284}
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.70it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.71it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5559701492537313, 'auc': 0.9685759634662507, 'precision': 1.0, 'recall': 0.11194029850746269, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_softmax
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.39it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.04it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.04it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.847169861833475, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.86GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.73it/s]
[DEBUG] Final encoded shape: (1755, 262144)
Encoded feature matrix shape: (1755, 262144)
[DEBUG] After encoding. Memory: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 262144), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1755, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_last
  [DEBUG] Fitting SAE probe normally...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.72GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.89it/s]
[DEBUG] Final encoded shape: (1755, 262144)
Encoded feature matrix shape: (1755, 262144)
[DEBUG] After encoding. Memory: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 262144), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1755, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.67it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.91it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.15it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5503731343283582, 'auc': 0.9849214747159725, 'precision': 1.0, 'recall': 0.10074626865671642, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.04it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.72it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.69it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7481343283582089, 'auc': 0.8487692136333259, 'precision': 0.9716312056737588, 'recall': 0.5111940298507462, 'fpr': 0.014925373134328358}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.65it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.79it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5503731343283582, 'auc': 0.9849214747159725, 'precision': 1.0, 'recall': 0.10074626865671642, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.09it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.60it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.52it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.500780031201248, 'auc': 0.9248833603890176, 'precision': 1.0, 'recall': 0.0015600624024961, 'fpr': 0.0}
  🚀 [BATCH] Training probe: attention
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: full
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.02it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.86it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.96it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7481343283582089, 'auc': 0.8487692136333259, 'precision': 0.9716312056737588, 'recall': 0.5111940298507462, 'fpr': 0.014925373134328358}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.42it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.06it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.04it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6427457098283932, 'auc': 0.7459142671479091, 'precision': 0.7886435331230284, 'recall': 0.39001560062402496, 'fpr': 0.10452418096723869}
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.73GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.93it/s]
[DEBUG] Final encoded shape: (1755, 262144)
Encoded feature matrix shape: (1755, 262144)
[DEBUG] After encoding. Memory: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 262144), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1755, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 1755 full activations
[DEBUG] Estimated memory requirement: 3.32 GB
[WARNING] Large memory requirement (3.32 GB).
[DEBUG] Max sequence length: 520
[DEBUG] Shape of first activation: (1, 71, 3584)
[DEBUG] Final array shape: (1755, 520, 3584)
[DEBUG] Returning (1755, 520, 3584) activations
[DEBUG] train activations shape: (1755, 520, 3584), dtype: float16
[DEBUG] train activations range: [-1674.000000, 237.000000]
Train activations: (1755, 520, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: attention
  [DEBUG] Creating attention probe
  [DEBUG] Fitting attention probe normally...
Epoch 10/100, Loss: 0.2615
Epoch 20/100, Loss: 0.0966
Epoch 30/100, Loss: 0.0336
Epoch 40/100, Loss: 0.0111
Epoch 50/100, Loss: 0.0061
Epoch 60/100, Loss: 0.0047
Early stopping at epoch 61
Saved probe to results/spam_gemma_9b/seed_46/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state.npz
Saved training log to results/spam_gemma_9b/seed_46/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state_train_log.json
  - 🔥 Probe state saved to train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.04it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.05it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.07it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5578358208955224, 'auc': 0.9641623969703721, 'precision': 1.0, 'recall': 0.11567164179104478, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Loaded probe from results/spam_gemma_9b/seed_46/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 536 full activations
[DEBUG] Estimated memory requirement: 1.60 GB
[WARNING] Large memory requirement (1.60 GB).
[DEBUG] Max sequence length: 1167
[DEBUG] Shape of first activation: (1, 225, 3584)
[DEBUG] Final array shape: (536, 1167, 3584)
[DEBUG] Returning (536, 1167, 3584) activations
[DEBUG] test activations shape: (536, 1167, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (536, 1167, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating validation on '94_better_spam': Got unsupported ScalarType BFloat16
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.04it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.62it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5578358208955224, 'auc': 0.9641623969703721, 'precision': 1.0, 'recall': 0.11567164179104478, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.41it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.07it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.99it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.500780031201248, 'auc': 0.7891092554778634, 'precision': 1.0, 'recall': 0.0015600624024961, 'fpr': 0.0}
  🚀 [BATCH] Training probe: attention
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: full
Loaded probe from results/spam_gemma_9b/seed_46/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state.npz
Converted attention probe back to bfloat16 for training
Test activations: (536, 1167, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating test on '94_better_spam': Got unsupported ScalarType BFloat16
Loaded probe from results/spam_gemma_9b/seed_46/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 5157
[DEBUG] Processing 1282 full activations
[DEBUG] Estimated memory requirement: 0.60 GB
[WARNING] Large memory requirement (0.60 GB).
[DEBUG] Max sequence length: 163
[DEBUG] Shape of first activation: (1, 26, 3584)
[DEBUG] Final array shape: (1282, 163, 3584)
[DEBUG] Returning (1282, 163, 3584) activations
[DEBUG] test activations shape: (1282, 163, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (1282, 163, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating on '87_is_spam': Got unsupported ScalarType BFloat16

=== Group 37/700 — ('94_better_spam', 20, 'resid_post', 48, '{"class_counts": {"0": 1750, "1": 5}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-567.000000, 51.500000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 409.76it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 395.73it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 398.62it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6585820895522388, 'auc': 0.9403402762307864, 'precision': 0.9775280898876404, 'recall': 0.3246268656716418, 'fpr': 0.007462686567164179}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 418.55it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 398.55it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 392.28it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6585820895522388, 'auc': 0.9403402762307864, 'precision': 0.9775280898876404, 'recall': 0.3246268656716418, 'fpr': 0.007462686567164179}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1505.000000, 110.875000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 446.32it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 425.64it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 414.23it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6669266770670826, 'auc': 0.8573504250622443, 'precision': 0.9458333333333333, 'recall': 0.3541341653666147, 'fpr': 0.0202808112324493}
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 1755 full activations
[DEBUG] Estimated memory requirement: 3.28 GB
[WARNING] Large memory requirement (3.28 GB).
[DEBUG] Max sequence length: 511
[DEBUG] Shape of first activation: (1, 178, 3584)
[DEBUG] Final array shape: (1755, 511, 3584)
[DEBUG] Returning (1755, 511, 3584) activations
[DEBUG] train activations shape: (1755, 511, 3584), dtype: float16
[DEBUG] train activations range: [-1674.000000, 237.000000]
Train activations: (1755, 511, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: attention
  [DEBUG] Creating attention probe
  [DEBUG] Fitting attention probe normally...
Epoch 10/100, Loss: 0.1852
Epoch 20/100, Loss: 0.0731
Epoch 30/100, Loss: 0.0331
Epoch 40/100, Loss: 0.0140
Epoch 50/100, Loss: 0.0048
Epoch 60/100, Loss: 0.0020
Early stopping at epoch 63
Saved probe to results/spam_gemma_9b/seed_47/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state.npz
Saved training log to results/spam_gemma_9b/seed_47/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state_train_log.json
  - 🔥 Probe state saved to train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1755, 3584)
[DEBUG] Returning (1755, 3584) activations
[DEBUG] train activations shape: (1755, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_max
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.87GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 337.16it/s]
[DEBUG] Final encoded shape: (1755, 16384)
Encoded feature matrix shape: (1755, 16384)
[DEBUG] After encoding. Memory: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 16384), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1755, 16384)
[DEBUG] Memory after feature matrix build: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-106.125000, 237.500000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 419.30it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 394.20it/s]
Loaded probe from results/spam_gemma_9b/seed_47/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 536 full activations
[DEBUG] Estimated memory requirement: 1.60 GB
[WARNING] Large memory requirement (1.60 GB).
[DEBUG] Max sequence length: 939
[DEBUG] Shape of first activation: (1, 335, 3584)
[DEBUG] Final array shape: (536, 939, 3584)
[DEBUG] Returning (536, 939, 3584) activations
[DEBUG] test activations shape: (536, 939, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (536, 939, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 376.31it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5652985074626866, 'auc': 0.9599019826241925, 'precision': 1.0, 'recall': 0.13059701492537312, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
    💀💀💀 ERROR evaluating validation on '94_better_spam': Got unsupported ScalarType BFloat16
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Loaded probe from results/spam_gemma_9b/seed_47/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state.npz
Converted attention probe back to bfloat16 for training
Test activations: (536, 939, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating test on '94_better_spam': Got unsupported ScalarType BFloat16
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 416.23it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 425.73it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 394.42it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5652985074626866, 'auc': 0.9599019826241925, 'precision': 1.0, 'recall': 0.13059701492537312, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1336.000000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 445.21it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 419.51it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 407.83it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.4984399375975039, 'auc': 0.7673121901475124, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0031201248049922}
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
Loaded probe from results/spam_gemma_9b/seed_47/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 5157
[DEBUG] Processing 1282 full activations
[DEBUG] Estimated memory requirement: 0.59 GB
[WARNING] Large memory requirement (0.59 GB).
[DEBUG] Max sequence length: 210
[DEBUG] Shape of first activation: (1, 20, 3584)
[DEBUG] Final array shape: (1282, 210, 3584)
[DEBUG] Returning (1282, 210, 3584) activations
[DEBUG] test activations shape: (1282, 210, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.500000]
Test activations: (1282, 210, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating on '87_is_spam': Got unsupported ScalarType BFloat16

=== Group 38/700 — ('94_better_spam', 20, 'resid_post', 49, '{"class_counts": {"0": 1750, "1": 5}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1755, 3584)
[DEBUG] Returning (1755, 3584) activations
[DEBUG] train activations shape: (1755, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 231.875000]
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.85GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 336.38it/s]
[DEBUG] Final encoded shape: (1755, 16384)
Encoded feature matrix shape: (1755, 16384)
[DEBUG] After encoding. Memory: RAM: 6.86GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 16384), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 6.86GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1755, 16384)
[DEBUG] Memory after feature matrix build: RAM: 6.86GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 6.86GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 6.86GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1755, 3584)
[DEBUG] Returning (1755, 3584) activations
[DEBUG] train activations shape: (1755, 3584), dtype: float16
[DEBUG] train activations range: [-1526.000000, 117.875000]
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_mean
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.72GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 336.07it/s]
[DEBUG] Final encoded shape: (1755, 16384)
Encoded feature matrix shape: (1755, 16384)
[DEBUG] After encoding. Memory: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 16384), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1755, 16384)
[DEBUG] Memory after feature matrix build: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-256.250000, 81.875000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 414.99it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 389.15it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 411.09it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8022388059701493, 'auc': 0.9612803519714859, 'precision': 0.9709302325581395, 'recall': 0.6231343283582089, 'fpr': 0.018656716417910446}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-813.000000, 51.906250]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 414.95it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 406.15it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 391.81it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7313432835820896, 'auc': 0.899379037647583, 'precision': 1.0, 'recall': 0.4626865671641791, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 415.11it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 400.83it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 388.76it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8022388059701493, 'auc': 0.9612803519714859, 'precision': 0.9709302325581395, 'recall': 0.6231343283582089, 'fpr': 0.018656716417910446}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1336.000000, 221.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 443.72it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 412.50it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 410.66it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.4789391575663027, 'auc': 0.5065323049739462, 'precision': 0.44398340248962653, 'recall': 0.1669266770670827, 'fpr': 0.20904836193447737}
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 409.68it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 415.65it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 379.95it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7313432835820896, 'auc': 0.899379037647583, 'precision': 1.0, 'recall': 0.4626865671641791, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1053.000000, 69.625000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 445.68it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 426.06it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 405.15it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5772230889235569, 'auc': 0.8645422883998043, 'precision': 0.9626168224299065, 'recall': 0.1606864274570983, 'fpr': 0.0062402496099844}
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1755, 3584)
[DEBUG] Returning (1755, 3584) activations
[DEBUG] train activations shape: (1755, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.86GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 338.03it/s]
[DEBUG] Final encoded shape: (1755, 16384)
Encoded feature matrix shape: (1755, 16384)
[DEBUG] After encoding. Memory: RAM: 6.86GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 16384), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 6.86GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1755, 16384)
[DEBUG] Memory after feature matrix build: RAM: 6.86GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 6.86GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 6.86GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1755, 3584)
[DEBUG] Returning (1755, 3584) activations
[DEBUG] train activations shape: (1755, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_max
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.72GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 337.04it/s]
[DEBUG] Final encoded shape: (1755, 16384)
Encoded feature matrix shape: (1755, 16384)
[DEBUG] After encoding. Memory: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 16384), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1755, 16384)
[DEBUG] Memory after feature matrix build: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-106.500000, 237.500000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 410.56it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 427.38it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 391.81it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5578358208955224, 'auc': 0.9672672087324571, 'precision': 1.0, 'recall': 0.11567164179104478, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-119.687500, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 413.03it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 378.58it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 388.04it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.539179104477612, 'auc': 0.9650673869458676, 'precision': 1.0, 'recall': 0.07835820895522388, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 412.58it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 418.30it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 395.91it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5578358208955224, 'auc': 0.9672672087324571, 'precision': 1.0, 'recall': 0.11567164179104478, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1336.000000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 441.09it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 411.85it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 405.58it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.7384011429099908, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 416.85it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 417.97it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 389.19it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.539179104477612, 'auc': 0.9650673869458676, 'precision': 1.0, 'recall': 0.07835820895522388, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-177.375000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 444.62it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 425.73it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 401.39it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.8366972432407437, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1755, 3584)
[DEBUG] Returning (1755, 3584) activations
[DEBUG] train activations shape: (1755, 3584), dtype: float16
[DEBUG] train activations range: [-1526.000000, 117.875000]
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_mean
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.89GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.58it/s]
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1755, 3584)
[DEBUG] Returning (1755, 3584) activations
[DEBUG] train activations shape: (1755, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 231.875000]
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.72GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 330.72it/s]
[DEBUG] Final encoded shape: (1755, 16384)
Encoded feature matrix shape: (1755, 16384)
[DEBUG] After encoding. Memory: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 16384), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1755, 16384)
[DEBUG] Memory after feature matrix build: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Final encoded shape: (1755, 262144)
Encoded feature matrix shape: (1755, 262144)
[DEBUG] After encoding. Memory: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 262144), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1755, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-210.625000, 80.875000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 414.33it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 381.99it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 420.78it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7667910447761194, 'auc': 0.9431527066161729, 'precision': 0.9863945578231292, 'recall': 0.5410447761194029, 'fpr': 0.007462686567164179}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.62it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.06it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.83it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6753731343283582, 'auc': 0.8759606816662953, 'precision': 1.0, 'recall': 0.35074626865671643, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 412.91it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 383.60it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 386.61it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7667910447761194, 'auc': 0.9431527066161729, 'precision': 0.9863945578231292, 'recall': 0.5410447761194029, 'fpr': 0.007462686567164179}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-263.750000, 76.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 445.42it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 421.43it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 421.58it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.4711388455538221, 'auc': 0.5878490365823681, 'precision': 0.23943661971830985, 'recall': 0.0265210608424337, 'fpr': 0.0842433697347894}
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.62it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.11it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.85it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6753731343283582, 'auc': 0.8759606816662953, 'precision': 1.0, 'recall': 0.35074626865671643, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1755, 3584)
[DEBUG] Returning (1755, 3584) activations
[DEBUG] train activations shape: (1755, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.72GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 335.57it/s]
[DEBUG] Final encoded shape: (1755, 16384)
Encoded feature matrix shape: (1755, 16384)
[DEBUG] After encoding. Memory: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 16384), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1755, 16384)
[DEBUG] Memory after feature matrix build: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.14it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.66it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.65it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.500780031201248, 'auc': 0.7734039782808162, 'precision': 1.0, 'recall': 0.0015600624024961, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-119.750000, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 414.42it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 386.39it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 388.51it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5503731343283582, 'auc': 0.9659306081532636, 'precision': 1.0, 'recall': 0.10074626865671642, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_max
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.89GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.70it/s]
[DEBUG] Final encoded shape: (1755, 262144)
Encoded feature matrix shape: (1755, 262144)
[DEBUG] After encoding. Memory: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 262144), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1755, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 413.72it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 382.87it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 387.97it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5503731343283582, 'auc': 0.9659306081532636, 'precision': 1.0, 'recall': 0.10074626865671642, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-177.375000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 440.53it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 406.09it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 403.40it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.49921996879875197, 'auc': 0.7572314125014299, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0015600624024961}
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.64it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.82it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.11it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.582089552238806, 'auc': 0.9753285809757185, 'precision': 1.0, 'recall': 0.16417910447761194, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_mean
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.72GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.92it/s]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Final encoded shape: (1755, 262144)
Encoded feature matrix shape: (1755, 262144)
[DEBUG] After encoding. Memory: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 262144), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1755, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.64it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.75it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.83it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.582089552238806, 'auc': 0.9753285809757185, 'precision': 1.0, 'recall': 0.16417910447761194, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.09it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.59it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.61it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.8158955999425624, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.02it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.95it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.64it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5615671641791045, 'auc': 0.9256655157050568, 'precision': 1.0, 'recall': 0.12313432835820895, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.89GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.70it/s]
[DEBUG] Final encoded shape: (1755, 262144)
Encoded feature matrix shape: (1755, 262144)
[DEBUG] After encoding. Memory: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 262144), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1755, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.00it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.71it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.96it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5615671641791045, 'auc': 0.9256655157050568, 'precision': 1.0, 'recall': 0.12313432835820895, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.43it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.19it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.19it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.500780031201248, 'auc': 0.40555294598679426, 'precision': 1.0, 'recall': 0.0015600624024961, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.63it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.06it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.18it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8152985074626866, 'auc': 0.9290905546892405, 'precision': 0.9617486338797814, 'recall': 0.6567164179104478, 'fpr': 0.026119402985074626}
    [BATCH] Evaluating on 94_better_spam test set
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_max
  [DEBUG] Fitting SAE probe normally...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.72GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.91it/s]
[DEBUG] Final encoded shape: (1755, 262144)
Encoded feature matrix shape: (1755, 262144)
[DEBUG] After encoding. Memory: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 262144), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1755, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.67it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.92it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.92it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8152985074626866, 'auc': 0.9290905546892405, 'precision': 0.9617486338797814, 'recall': 0.6567164179104478, 'fpr': 0.026119402985074626}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.11it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.61it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.60it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5031201248049922, 'auc': 0.5359897391215461, 'precision': 0.5060240963855421, 'recall': 0.2620904836193448, 'fpr': 0.25585023400936036}
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.96it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.49it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5615671641791045, 'auc': 0.9738805970149255, 'precision': 1.0, 'recall': 0.12313432835820895, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.88GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.58it/s]
[DEBUG] Final encoded shape: (1755, 262144)
Encoded feature matrix shape: (1755, 262144)
[DEBUG] After encoding. Memory: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 262144), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1755, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.01it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.93it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.75it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5615671641791045, 'auc': 0.9738805970149255, 'precision': 1.0, 'recall': 0.12313432835820895, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.41it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.06it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.04it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.8264996434490766, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.62it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.87it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.56it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5764925373134329, 'auc': 0.9761221875696147, 'precision': 1.0, 'recall': 0.15298507462686567, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_last
  [DEBUG] Fitting SAE probe normally...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.71GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.89it/s]
[DEBUG] Final encoded shape: (1755, 262144)
Encoded feature matrix shape: (1755, 262144)
[DEBUG] After encoding. Memory: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 262144), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1755, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.67it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.16it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.87it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5764925373134329, 'auc': 0.9761221875696147, 'precision': 1.0, 'recall': 0.15298507462686567, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.09it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.66it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.59it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.7792913276593467, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: attention
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: full
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.06it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.73it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.72it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7817164179104478, 'auc': 0.8888950768545333, 'precision': 0.9631901840490797, 'recall': 0.585820895522388, 'fpr': 0.022388059701492536}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.05it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.00it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.26it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7817164179104478, 'auc': 0.8888950768545333, 'precision': 0.9631901840490797, 'recall': 0.585820895522388, 'fpr': 0.022388059701492536}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.45it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.12it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.27it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5561622464898596, 'auc': 0.6738958481896219, 'precision': 0.7068965517241379, 'recall': 0.1918876755070203, 'fpr': 0.07956318252730109}
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 1755 full activations
[DEBUG] Estimated memory requirement: 3.25 GB
[WARNING] Large memory requirement (3.25 GB).
[DEBUG] Max sequence length: 520
[DEBUG] Shape of first activation: (1, 174, 3584)
[DEBUG] Final array shape: (1755, 520, 3584)
[DEBUG] Returning (1755, 520, 3584) activations
[DEBUG] train activations shape: (1755, 520, 3584), dtype: float16
[DEBUG] train activations range: [-1674.000000, 237.000000]
Train activations: (1755, 520, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: attention
  [DEBUG] Creating attention probe
  [DEBUG] Fitting attention probe normally...
Epoch 10/100, Loss: 0.1969
Epoch 20/100, Loss: 0.0733
Epoch 30/100, Loss: 0.0382
Epoch 40/100, Loss: 0.0207
Epoch 50/100, Loss: 0.0127
Epoch 60/100, Loss: 0.0077
Early stopping at epoch 66
Saved probe to results/spam_gemma_9b/seed_48/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state.npz
Saved training log to results/spam_gemma_9b/seed_48/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state_train_log.json
  - 🔥 Probe state saved to train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.71GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.92it/s]
[DEBUG] Final encoded shape: (1755, 262144)
Encoded feature matrix shape: (1755, 262144)
[DEBUG] After encoding. Memory: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 262144), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1755, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Loaded probe from results/spam_gemma_9b/seed_48/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 536 full activations
[DEBUG] Estimated memory requirement: 1.63 GB
[WARNING] Large memory requirement (1.63 GB).
[DEBUG] Max sequence length: 939
[DEBUG] Shape of first activation: (1, 289, 3584)
[DEBUG] Final array shape: (536, 939, 3584)
[DEBUG] Returning (536, 939, 3584) activations
[DEBUG] test activations shape: (536, 939, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.500000]
Test activations: (536, 939, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating validation on '94_better_spam': Got unsupported ScalarType BFloat16
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.01it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.63it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.95it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5522388059701493, 'auc': 0.9776676319893072, 'precision': 1.0, 'recall': 0.1044776119402985, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Loaded probe from results/spam_gemma_9b/seed_48/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state.npz
Converted attention probe back to bfloat16 for training
Test activations: (536, 939, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating test on '94_better_spam': Got unsupported ScalarType BFloat16
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Loaded probe from results/spam_gemma_9b/seed_48/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 5157
[DEBUG] Processing 1282 full activations
[DEBUG] Estimated memory requirement: 0.60 GB
[WARNING] Large memory requirement (0.60 GB).
[DEBUG] Max sequence length: 210
[DEBUG] Shape of first activation: (1, 17, 3584)
[DEBUG] Final array shape: (1282, 210, 3584)
[DEBUG] Returning (1282, 210, 3584) activations
[DEBUG] test activations shape: (1282, 210, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (1282, 210, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating on '87_is_spam': Got unsupported ScalarType BFloat16

=== Group 39/700 — ('94_better_spam', 20, 'resid_post', 50, '{"class_counts": {"0": 1750, "1": 5}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.04it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.71it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.99it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5522388059701493, 'auc': 0.9776676319893072, 'precision': 1.0, 'recall': 0.1044776119402985, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.43it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.19it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.19it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.8449137341468698, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: attention
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: full
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1755, 3584)
[DEBUG] Returning (1755, 3584) activations
[DEBUG] train activations shape: (1755, 3584), dtype: float16
[DEBUG] train activations range: [-1526.000000, 117.875000]
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_mean
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.86GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 336.59it/s]
[DEBUG] Final encoded shape: (1755, 16384)
Encoded feature matrix shape: (1755, 16384)
[DEBUG] After encoding. Memory: RAM: 6.86GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 16384), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 6.86GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1755, 16384)
[DEBUG] Memory after feature matrix build: RAM: 6.86GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 6.86GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 6.86GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-1067.000000, 71.062500]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 420.52it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 388.94it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 390.97it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7201492537313433, 'auc': 0.9170193807083983, 'precision': 0.9916666666666667, 'recall': 0.44402985074626866, 'fpr': 0.0037313432835820895}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 412.22it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 395.73it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 401.91it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7201492537313433, 'auc': 0.9170193807083983, 'precision': 0.9916666666666667, 'recall': 0.44402985074626866, 'fpr': 0.0037313432835820895}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1045.000000, 76.437500]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 446.18it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 420.04it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 434.73it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.609204368174727, 'auc': 0.7477785538878654, 'precision': 0.972972972972973, 'recall': 0.22464898595943839, 'fpr': 0.0062402496099844}
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 1755 full activations
[DEBUG] Estimated memory requirement: 3.30 GB
[WARNING] Large memory requirement (3.30 GB).
[DEBUG] Max sequence length: 520
[DEBUG] Shape of first activation: (1, 100, 3584)
[DEBUG] Final array shape: (1755, 520, 3584)
[DEBUG] Returning (1755, 520, 3584) activations
[DEBUG] train activations shape: (1755, 520, 3584), dtype: float16
[DEBUG] train activations range: [-1674.000000, 237.000000]
Train activations: (1755, 520, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: attention
  [DEBUG] Creating attention probe
  [DEBUG] Fitting attention probe normally...
Epoch 10/100, Loss: 0.1396
Epoch 20/100, Loss: 0.0556
Epoch 30/100, Loss: 0.0223
Epoch 40/100, Loss: 0.0118
Early stopping at epoch 49
Saved probe to results/spam_gemma_9b/seed_49/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state.npz
Saved training log to results/spam_gemma_9b/seed_49/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state_train_log.json
  - 🔥 Probe state saved to train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1755, 3584)
[DEBUG] Returning (1755, 3584) activations
[DEBUG] train activations shape: (1755, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_max
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.88GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 335.32it/s]
[DEBUG] Final encoded shape: (1755, 16384)
Encoded feature matrix shape: (1755, 16384)
[DEBUG] After encoding. Memory: RAM: 6.88GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 16384), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 6.88GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1755, 16384)
[DEBUG] Memory after feature matrix build: RAM: 6.88GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 6.88GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 6.88GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-157.625000, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 410.92it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 417.97it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 432.89it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5410447761194029, 'auc': 0.9671279795054578, 'precision': 1.0, 'recall': 0.08208955223880597, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Loaded probe from results/spam_gemma_9b/seed_49/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 536 full activations
[DEBUG] Estimated memory requirement: 1.55 GB
[WARNING] Large memory requirement (1.55 GB).
[DEBUG] Max sequence length: 1167
[DEBUG] Shape of first activation: (1, 171, 3584)
[DEBUG] Final array shape: (536, 1167, 3584)
[DEBUG] Returning (536, 1167, 3584) activations
[DEBUG] test activations shape: (536, 1167, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (536, 1167, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating validation on '94_better_spam': Got unsupported ScalarType BFloat16
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 416.76it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 415.24it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 417.18it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5410447761194029, 'auc': 0.9671279795054578, 'precision': 1.0, 'recall': 0.08208955223880597, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-163.875000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 442.97it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 426.38it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 428.41it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5054602184087363, 'auc': 0.8693587681104747, 'precision': 1.0, 'recall': 0.0109204368174727, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
Loaded probe from results/spam_gemma_9b/seed_49/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state.npz
Converted attention probe back to bfloat16 for training
Test activations: (536, 1167, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating test on '94_better_spam': Got unsupported ScalarType BFloat16
Loaded probe from results/spam_gemma_9b/seed_49/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 5157
[DEBUG] Processing 1282 full activations
[DEBUG] Estimated memory requirement: 0.60 GB
[WARNING] Large memory requirement (0.60 GB).
[DEBUG] Max sequence length: 215
[DEBUG] Shape of first activation: (1, 33, 3584)
[DEBUG] Final array shape: (1282, 215, 3584)
[DEBUG] Returning (1282, 215, 3584) activations
[DEBUG] test activations shape: (1282, 215, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (1282, 215, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating on '87_is_spam': Got unsupported ScalarType BFloat16

=== Group 40/700 — ('94_better_spam', 20, 'resid_post', 51, '{"class_counts": {"0": 1750, "1": 5}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1755, 3584)
[DEBUG] Returning (1755, 3584) activations
[DEBUG] train activations shape: (1755, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 231.875000]
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.88GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 338.80it/s]
[DEBUG] Final encoded shape: (1755, 16384)
Encoded feature matrix shape: (1755, 16384)
[DEBUG] After encoding. Memory: RAM: 6.88GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 16384), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 6.88GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1755, 16384)
[DEBUG] Memory after feature matrix build: RAM: 6.88GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 6.88GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 6.88GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1755, 3584)
[DEBUG] Returning (1755, 3584) activations
[DEBUG] train activations shape: (1755, 3584), dtype: float16
[DEBUG] train activations range: [-1526.000000, 117.875000]
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_mean
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.73GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 335.93it/s]
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-256.250000, 90.750000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 408.84it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 389.33it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 381.13it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.75, 'auc': 0.936233014034306, 'precision': 0.9785714285714285, 'recall': 0.5111940298507462, 'fpr': 0.011194029850746268}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Final encoded shape: (1755, 16384)
Encoded feature matrix shape: (1755, 16384)
[DEBUG] After encoding. Memory: RAM: 9.73GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 16384), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 9.73GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1755, 16384)
[DEBUG] Memory after feature matrix build: RAM: 9.73GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 9.73GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 9.73GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 410.00it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 393.94it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 425.95it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.75, 'auc': 0.936233014034306, 'precision': 0.9785714285714285, 'recall': 0.5111940298507462, 'fpr': 0.011194029850746268}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-782.000000, 52.031250]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 411.61it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 389.08it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 427.47it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6735074626865671, 'auc': 0.879984406326576, 'precision': 1.0, 'recall': 0.34701492537313433, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-271.250000, 76.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 445.70it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 437.52it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 431.14it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5413416536661466, 'auc': 0.7099890235859044, 'precision': 0.7086614173228346, 'recall': 0.14040561622464898, 'fpr': 0.057722308892355696}
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 414.05it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 380.64it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 393.17it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6735074626865671, 'auc': 0.879984406326576, 'precision': 1.0, 'recall': 0.34701492537313433, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1522.000000, 113.625000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 444.29it/s]
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1755, 3584)
[DEBUG] Returning (1755, 3584) activations
[DEBUG] train activations shape: (1755, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_softmax
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 437.34it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 412.60it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.641185647425897, 'auc': 0.6868192980449328, 'precision': 0.9022222222222223, 'recall': 0.3166926677067083, 'fpr': 0.0343213728549142}
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.88GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 336.70it/s]
[DEBUG] Final encoded shape: (1755, 16384)
Encoded feature matrix shape: (1755, 16384)
[DEBUG] After encoding. Memory: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 16384), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1755, 16384)
[DEBUG] Memory after feature matrix build: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1755, 3584)
[DEBUG] Returning (1755, 3584) activations
[DEBUG] train activations shape: (1755, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_max
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.74GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 338.32it/s]
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-157.625000, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Final encoded shape: (1755, 16384)
Encoded feature matrix shape: (1755, 16384)
[DEBUG] After encoding. Memory: RAM: 9.74GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 16384), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 9.74GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1755, 16384)
[DEBUG] Memory after feature matrix build: RAM: 9.74GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 9.74GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 9.74GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 391.26it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 413.07it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 419.26it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5485074626865671, 'auc': 0.9755652706616174, 'precision': 1.0, 'recall': 0.09701492537313433, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-118.875000, 237.500000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 419.98it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 375.93it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 386.43it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5354477611940298, 'auc': 0.9755931165070172, 'precision': 1.0, 'recall': 0.0708955223880597, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 414.62it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 407.77it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 407.21it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5485074626865671, 'auc': 0.9755652706616174, 'precision': 1.0, 'recall': 0.09701492537313433, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-163.875000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 439.49it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 428.45it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 376.00it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.49921996879875197, 'auc': 0.8124298762902155, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0015600624024961}
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 405.60it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 406.90it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 402.79it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5354477611940298, 'auc': 0.9755931165070172, 'precision': 1.0, 'recall': 0.0708955223880597, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1371.000000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 434.80it/s]
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_mean
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 409.80it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 409.38it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5117004680187207, 'auc': 0.8299580657173246, 'precision': 1.0, 'recall': 0.0234009360374415, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.88GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.58it/s]
[DEBUG] Final encoded shape: (1755, 262144)
Encoded feature matrix shape: (1755, 262144)
[DEBUG] After encoding. Memory: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 262144), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1755, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1755, 3584)
[DEBUG] Returning (1755, 3584) activations
[DEBUG] train activations shape: (1755, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 231.875000]
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.75GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 335.33it/s]
[DEBUG] Final encoded shape: (1755, 16384)
Encoded feature matrix shape: (1755, 16384)
[DEBUG] After encoding. Memory: RAM: 9.75GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 16384), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 9.75GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1755, 16384)
[DEBUG] Memory after feature matrix build: RAM: 9.75GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 9.75GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 9.75GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-210.750000, 80.687500]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 412.66it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 383.57it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 288.57it/s]
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s][DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6791044776119403, 'auc': 0.8396914680329695, 'precision': 0.9444444444444444, 'recall': 0.3805970149253731, 'fpr': 0.022388059701492536}
    [BATCH] Evaluating on 94_better_spam test set
Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.55it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.74it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.84it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7332089552238806, 'auc': 0.9291740922254401, 'precision': 0.9921259842519685, 'recall': 0.4701492537313433, 'fpr': 0.0037313432835820895}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 417.26it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 376.54it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 403.10it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6791044776119403, 'auc': 0.8396914680329695, 'precision': 0.9444444444444444, 'recall': 0.3805970149253731, 'fpr': 0.022388059701492536}
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1371.000000, 229.375000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 447.44it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 413.93it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 407.10it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.609204368174727, 'auc': 0.6839206485576115, 'precision': 0.6794871794871795, 'recall': 0.41341653666146644, 'fpr': 0.19500780031201248}
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.59it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.99it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.77it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7332089552238806, 'auc': 0.9291740922254401, 'precision': 0.9921259842519685, 'recall': 0.4701492537313433, 'fpr': 0.0037313432835820895}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.10it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.60it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.68it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.49375975039001563, 'auc': 0.43640859518936137, 'precision': 0.3181818181818182, 'recall': 0.0109204368174727, 'fpr': 0.0234009360374415}
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1755, 3584)
[DEBUG] Returning (1755, 3584) activations
[DEBUG] train activations shape: (1755, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.75GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 338.48it/s]
[DEBUG] Final encoded shape: (1755, 16384)
Encoded feature matrix shape: (1755, 16384)
[DEBUG] After encoding. Memory: RAM: 9.75GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 16384), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 9.75GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1755, 16384)
[DEBUG] Memory after feature matrix build: RAM: 9.75GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 9.75GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 9.75GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_max
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-119.187500, 237.500000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 412.74it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 422.09it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 408.92it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.542910447761194, 'auc': 0.9710542437068389, 'precision': 1.0, 'recall': 0.08582089552238806, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.89GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.72it/s]
[DEBUG] Final encoded shape: (1755, 262144)
Encoded feature matrix shape: (1755, 262144)
[DEBUG] After encoding. Memory: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 262144), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1755, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 419.22it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 386.64it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 428.34it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.542910447761194, 'auc': 0.9710542437068389, 'precision': 1.0, 'recall': 0.08582089552238806, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1371.000000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 445.21it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 430.36it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 410.76it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.4641185647425897, 'auc': 0.7190500412528202, 'precision': 0.23255813953488372, 'recall': 0.031201248049921998, 'fpr': 0.1029641185647426}
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.64it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.88it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.09it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5354477611940298, 'auc': 0.9771524838494097, 'precision': 1.0, 'recall': 0.0708955223880597, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_mean
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.75GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.90it/s]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Final encoded shape: (1755, 262144)
Encoded feature matrix shape: (1755, 262144)
[DEBUG] After encoding. Memory: RAM: 11.46GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 262144), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 11.46GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1755, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.46GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.46GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.46GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.66it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.30it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.32it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5354477611940298, 'auc': 0.9771524838494097, 'precision': 1.0, 'recall': 0.0708955223880597, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.11it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.63it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.60it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.500780031201248, 'auc': 0.934131780247809, 'precision': 1.0, 'recall': 0.0015600624024961, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.05it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.68it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5615671641791045, 'auc': 0.8722293383827133, 'precision': 1.0, 'recall': 0.12313432835820895, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.89GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.70it/s]
[DEBUG] Final encoded shape: (1755, 262144)
Encoded feature matrix shape: (1755, 262144)
[DEBUG] After encoding. Memory: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 262144), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1755, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.06it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.06it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.75it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5615671641791045, 'auc': 0.8722293383827133, 'precision': 1.0, 'recall': 0.12313432835820895, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.42it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.99it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.06it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5280811232449298, 'auc': 0.41106062339217436, 'precision': 0.6698113207547169, 'recall': 0.11076443057722309, 'fpr': 0.054602184087363496}
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.62it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.07it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.76it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.792910447761194, 'auc': 0.9083175540209399, 'precision': 0.9757575757575757, 'recall': 0.6007462686567164, 'fpr': 0.014925373134328358}
    [BATCH] Evaluating on 94_better_spam test set
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_max
  [DEBUG] Fitting SAE probe normally...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.75GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.91it/s]
[DEBUG] Final encoded shape: (1755, 262144)
Encoded feature matrix shape: (1755, 262144)
[DEBUG] After encoding. Memory: RAM: 11.46GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 262144), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 11.46GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1755, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.46GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.47GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.47GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.69it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.08it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.21it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.792910447761194, 'auc': 0.9083175540209399, 'precision': 0.9757575757575757, 'recall': 0.6007462686567164, 'fpr': 0.014925373134328358}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.08it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.62it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.57it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7129485179407177, 'auc': 0.8395180112976748, 'precision': 0.8337408312958435, 'recall': 0.53198127925117, 'fpr': 0.1060842433697348}
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.01it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.73it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5354477611940298, 'auc': 0.9773891735353085, 'precision': 1.0, 'recall': 0.0708955223880597, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.89GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.70it/s]
[DEBUG] Final encoded shape: (1755, 262144)
Encoded feature matrix shape: (1755, 262144)
[DEBUG] After encoding. Memory: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 262144), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1755, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.02it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.94it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.71it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5354477611940298, 'auc': 0.9773891735353085, 'precision': 1.0, 'recall': 0.0708955223880597, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.41it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.23it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.03it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.500780031201248, 'auc': 0.8622739917397008, 'precision': 1.0, 'recall': 0.0015600624024961, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.64it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.14it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.39it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5466417910447762, 'auc': 0.9819698151035865, 'precision': 1.0, 'recall': 0.09328358208955224, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_last
  [DEBUG] Fitting SAE probe normally...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.75GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.90it/s]
[DEBUG] Final encoded shape: (1755, 262144)
Encoded feature matrix shape: (1755, 262144)
[DEBUG] After encoding. Memory: RAM: 11.46GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 262144), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 11.46GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1755, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.46GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.46GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.46GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.64it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.94it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.04it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5466417910447762, 'auc': 0.9819698151035865, 'precision': 1.0, 'recall': 0.09328358208955224, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.11it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.61it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.63it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.500780031201248, 'auc': 0.9292860950007423, 'precision': 1.0, 'recall': 0.0015600624024961, 'fpr': 0.0}
  🚀 [BATCH] Training probe: attention
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: full
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.02it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.70it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.05it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7238805970149254, 'auc': 0.8085876587213187, 'precision': 0.9347826086956522, 'recall': 0.48134328358208955, 'fpr': 0.033582089552238806}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.04it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.05it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.76it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7238805970149254, 'auc': 0.8085876587213187, 'precision': 0.9347826086956522, 'recall': 0.48134328358208955, 'fpr': 0.033582089552238806}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.46it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.17it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.23it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.672386895475819, 'auc': 0.7942396947047929, 'precision': 0.7688564476885644, 'recall': 0.49297971918876754, 'fpr': 0.1482059282371295}
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 1755 full activations
[DEBUG] Estimated memory requirement: 3.30 GB
[WARNING] Large memory requirement (3.30 GB).
[DEBUG] Max sequence length: 503
[DEBUG] Shape of first activation: (1, 77, 3584)
[DEBUG] Final array shape: (1755, 503, 3584)
[DEBUG] Returning (1755, 503, 3584) activations
[DEBUG] train activations shape: (1755, 503, 3584), dtype: float16
[DEBUG] train activations range: [-1674.000000, 237.000000]
Train activations: (1755, 503, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: attention
  [DEBUG] Creating attention probe
  [DEBUG] Fitting attention probe normally...
Epoch 10/100, Loss: 0.1911
Epoch 20/100, Loss: 0.0759
Epoch 30/100, Loss: 0.0389
Epoch 40/100, Loss: 0.0203
Epoch 50/100, Loss: 0.0149
Early stopping at epoch 52
Saved probe to results/spam_gemma_9b/seed_50/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state.npz
Saved training log to results/spam_gemma_9b/seed_50/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state_train_log.json
  - 🔥 Probe state saved to train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.74GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.91it/s]
[DEBUG] Final encoded shape: (1755, 262144)
Encoded feature matrix shape: (1755, 262144)
[DEBUG] After encoding. Memory: RAM: 11.45GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 262144), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 11.45GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1755, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.45GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.46GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.46GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Loaded probe from results/spam_gemma_9b/seed_50/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 536 full activations
[DEBUG] Estimated memory requirement: 1.58 GB
[WARNING] Large memory requirement (1.58 GB).
[DEBUG] Max sequence length: 1167
[DEBUG] Shape of first activation: (1, 106, 3584)
[DEBUG] Final array shape: (536, 1167, 3584)
[DEBUG] Returning (536, 1167, 3584) activations
[DEBUG] test activations shape: (536, 1167, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (536, 1167, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating validation on '94_better_spam': Got unsupported ScalarType BFloat16
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.01it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.65it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.87it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.539179104477612, 'auc': 0.9879705947872577, 'precision': 1.0, 'recall': 0.07835820895522388, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Loaded probe from results/spam_gemma_9b/seed_50/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state.npz
Converted attention probe back to bfloat16 for training
Test activations: (536, 1167, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating test on '94_better_spam': Got unsupported ScalarType BFloat16
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Loaded probe from results/spam_gemma_9b/seed_50/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 5157
[DEBUG] Processing 1282 full activations
[DEBUG] Estimated memory requirement: 0.60 GB
[WARNING] Large memory requirement (0.60 GB).
[DEBUG] Max sequence length: 210
[DEBUG] Shape of first activation: (1, 19, 3584)
[DEBUG] Final array shape: (1282, 210, 3584)
[DEBUG] Returning (1282, 210, 3584) activations
[DEBUG] test activations shape: (1282, 210, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (1282, 210, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating on '87_is_spam': Got unsupported ScalarType BFloat16

=== Group 41/700 — ('94_better_spam', 20, 'resid_post', 42, '{"class_counts": {"0": 1750, "1": 10}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.05it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.89it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.93it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.539179104477612, 'auc': 0.9879705947872577, 'precision': 1.0, 'recall': 0.07835820895522388, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.41it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.15it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.19it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.8536534909134275, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: attention
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: full
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-1337.000000, 237.500000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 415.32it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 387.36it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 405.64it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6902985074626866, 'auc': 0.9930942303408332, 'precision': 1.0, 'recall': 0.3805970149253731, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 418.68it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 424.91it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 425.43it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6902985074626866, 'auc': 0.9930942303408332, 'precision': 1.0, 'recall': 0.3805970149253731, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-150.125000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 423.95it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 398.83it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 415.85it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5429017160686428, 'auc': 0.9364706569542033, 'precision': 0.9661016949152542, 'recall': 0.08892355694227769, 'fpr': 0.0031201248049922}
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1760, 3584)
[DEBUG] Returning (1760, 3584) activations
[DEBUG] train activations shape: (1760, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 231.875000]
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.87GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 328.23it/s]
[DEBUG] Final encoded shape: (1760, 16384)
Encoded feature matrix shape: (1760, 16384)
[DEBUG] After encoding. Memory: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 16384), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1760, 16384)
[DEBUG] Memory after feature matrix build: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 1755 full activations
[DEBUG] Estimated memory requirement: 3.32 GB
[WARNING] Large memory requirement (3.32 GB).
[DEBUG] Max sequence length: 520
[DEBUG] Shape of first activation: (1, 87, 3584)
[DEBUG] Final array shape: (1755, 520, 3584)
[DEBUG] Returning (1755, 520, 3584) activations
[DEBUG] train activations shape: (1755, 520, 3584), dtype: float16
[DEBUG] train activations range: [-1674.000000, 237.000000]
Train activations: (1755, 520, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: attention
  [DEBUG] Creating attention probe
  [DEBUG] Fitting attention probe normally...
Epoch 10/100, Loss: 0.1871
Epoch 20/100, Loss: 0.0530
Epoch 30/100, Loss: 0.0215
Epoch 40/100, Loss: 0.0081
Epoch 50/100, Loss: 0.0037
Early stopping at epoch 53
Saved probe to results/spam_gemma_9b/seed_51/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state.npz
Saved training log to results/spam_gemma_9b/seed_51/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state_train_log.json
  - 🔥 Probe state saved to train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-1337.000000, 219.750000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 414.70it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 387.46it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 420.52it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8488805970149254, 'auc': 0.9589413009578971, 'precision': 0.9794871794871794, 'recall': 0.7126865671641791, 'fpr': 0.014925373134328358}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 412.34it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 387.97it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 364.09it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8488805970149254, 'auc': 0.9589413009578971, 'precision': 0.9794871794871794, 'recall': 0.7126865671641791, 'fpr': 0.014925373134328358}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-259.000000, 76.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 438.62it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 408.50it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 420.63it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.609204368174727, 'auc': 0.7423828310386705, 'precision': 0.7941176470588235, 'recall': 0.2948517940717629, 'fpr': 0.07644305772230889}
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
Loaded probe from results/spam_gemma_9b/seed_51/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 536 full activations
[DEBUG] Estimated memory requirement: 1.54 GB
[WARNING] Large memory requirement (1.54 GB).
[DEBUG] Max sequence length: 1167
[DEBUG] Shape of first activation: (1, 121, 3584)
[DEBUG] Final array shape: (536, 1167, 3584)
[DEBUG] Returning (536, 1167, 3584) activations
[DEBUG] test activations shape: (536, 1167, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.500000]
Test activations: (536, 1167, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating validation on '94_better_spam': Got unsupported ScalarType BFloat16
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1760, 3584)
[DEBUG] Returning (1760, 3584) activations
[DEBUG] train activations shape: (1760, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.89GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 336.65it/s]
[DEBUG] Final encoded shape: (1760, 16384)
Encoded feature matrix shape: (1760, 16384)
[DEBUG] After encoding. Memory: RAM: 6.89GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 16384), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 6.89GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1760, 16384)
[DEBUG] Memory after feature matrix build: RAM: 6.89GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 6.89GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 6.89GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Loaded probe from results/spam_gemma_9b/seed_51/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state.npz
Converted attention probe back to bfloat16 for training
Test activations: (536, 1167, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating test on '94_better_spam': Got unsupported ScalarType BFloat16
Loaded probe from results/spam_gemma_9b/seed_51/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_5_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 5157
[DEBUG] Processing 1282 full activations
[DEBUG] Estimated memory requirement: 0.60 GB
[WARNING] Large memory requirement (0.60 GB).
[DEBUG] Max sequence length: 99
[DEBUG] Shape of first activation: (1, 38, 3584)
[DEBUG] Final array shape: (1282, 99, 3584)
[DEBUG] Returning (1282, 99, 3584) activations
[DEBUG] test activations shape: (1282, 99, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (1282, 99, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating on '87_is_spam': Got unsupported ScalarType BFloat16

=== Group 42/700 — ('94_better_spam', 20, 'resid_post', 43, '{"class_counts": {"0": 1750, "1": 10}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-1337.000000, 237.500000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 412.70it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 418.38it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 385.54it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7276119402985075, 'auc': 0.9908526397861439, 'precision': 1.0, 'recall': 0.4552238805970149, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-1506.000000, 112.125000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 413.56it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 423.67it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 390.60it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8395522388059702, 'auc': 0.9683114279349521, 'precision': 1.0, 'recall': 0.6791044776119403, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 415.73it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 395.09it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 422.17it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7276119402985075, 'auc': 0.9908526397861439, 'precision': 1.0, 'recall': 0.4552238805970149, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-150.125000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 445.85it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 436.97it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 435.23it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5514820592823713, 'auc': 0.896274103694257, 'precision': 0.9714285714285714, 'recall': 0.1060842433697348, 'fpr': 0.0031201248049922}
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 409.32it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 422.13it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 419.14it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8395522388059702, 'auc': 0.9683114279349521, 'precision': 1.0, 'recall': 0.6791044776119403, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1063.000000, 70.687500]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 442.20it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 411.43it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 432.02it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6770670826833073, 'auc': 0.7205395236090255, 'precision': 0.9382239382239382, 'recall': 0.3790951638065523, 'fpr': 0.0249609984399376}
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1760, 3584)
[DEBUG] Returning (1760, 3584) activations
[DEBUG] train activations shape: (1760, 3584), dtype: float16
[DEBUG] train activations range: [-1526.000000, 117.875000]
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_mean
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1760, 3584)
[DEBUG] Returning (1760, 3584) activations
[DEBUG] train activations shape: (1760, 3584), dtype: float16
[DEBUG] train activations range: [-162.375000, 237.000000]
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_max
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.88GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.70it/s]

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.72GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 328.77it/s]
[DEBUG] Final encoded shape: (1760, 16384)
Encoded feature matrix shape: (1760, 16384)
[DEBUG] After encoding. Memory: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 16384), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1760, 16384)
[DEBUG] Memory after feature matrix build: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Final encoded shape: (1760, 262144)
Encoded feature matrix shape: (1760, 262144)
[DEBUG] After encoding. Memory: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 262144), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1760, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-1337.000000, 237.500000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 418.47it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 384.41it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 423.92it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.746268656716418, 'auc': 0.9880402094007574, 'precision': 1.0, 'recall': 0.4925373134328358, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-1506.000000, 112.125000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.61it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.13it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.06it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7835820895522388, 'auc': 0.9327522833593227, 'precision': 1.0, 'recall': 0.5671641791044776, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 411.17it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 422.26it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 417.72it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.746268656716418, 'auc': 0.9880402094007574, 'precision': 1.0, 'recall': 0.4925373134328358, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-147.875000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 439.03it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 426.38it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 407.95it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5741029641185648, 'auc': 0.9201812690292324, 'precision': 0.9896907216494846, 'recall': 0.1497659906396256, 'fpr': 0.0015600624024961}
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.64it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.15it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.09it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7835820895522388, 'auc': 0.9327522833593227, 'precision': 1.0, 'recall': 0.5671641791044776, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1760, 3584)
[DEBUG] Returning (1760, 3584) activations
[DEBUG] train activations shape: (1760, 3584), dtype: float16
[DEBUG] train activations range: [-256.250000, 92.687500]
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.72GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 337.01it/s]
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1063.000000, 70.687500]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Final encoded shape: (1760, 16384)
Encoded feature matrix shape: (1760, 16384)
[DEBUG] After encoding. Memory: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 16384), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1760, 16384)
[DEBUG] Memory after feature matrix build: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.09it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.74it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.54it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5561622464898596, 'auc': 0.7113470810283269, 'precision': 0.9186046511627907, 'recall': 0.12324492979719189, 'fpr': 0.0109204368174727}
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-1337.000000, 219.750000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 416.51it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 421.11it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 390.49it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8936567164179104, 'auc': 0.9784890844286032, 'precision': 0.9861751152073732, 'recall': 0.7985074626865671, 'fpr': 0.011194029850746268}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1760, 3584)
[DEBUG] Returning (1760, 3584) activations
[DEBUG] train activations shape: (1760, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_max
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.88GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.67it/s]
[DEBUG] Final encoded shape: (1760, 262144)
Encoded feature matrix shape: (1760, 262144)
[DEBUG] After encoding. Memory: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 262144), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1760, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 413.35it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 416.23it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 407.37it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8936567164179104, 'auc': 0.9784890844286032, 'precision': 0.9861751152073732, 'recall': 0.7985074626865671, 'fpr': 0.011194029850746268}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-252.250000, 76.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 444.01it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 427.79it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 426.90it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6045241809672387, 'auc': 0.7021205653218328, 'precision': 0.6763157894736842, 'recall': 0.40093603744149764, 'fpr': 0.1918876755070203}
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1760, 3584)
[DEBUG] Returning (1760, 3584) activations
[DEBUG] train activations shape: (1760, 3584), dtype: float16
[DEBUG] train activations range: [-162.375000, 237.000000]
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.73GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 335.75it/s]
[DEBUG] Final encoded shape: (1760, 16384)
Encoded feature matrix shape: (1760, 16384)
[DEBUG] After encoding. Memory: RAM: 9.73GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 16384), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 9.73GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1760, 16384)
[DEBUG] Memory after feature matrix build: RAM: 9.73GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 9.73GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 9.73GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.71it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.90it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.84it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7164179104477612, 'auc': 0.991882936065939, 'precision': 1.0, 'recall': 0.43283582089552236, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-1337.000000, 237.500000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 411.73it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 420.14it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 386.00it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7276119402985075, 'auc': 0.9922170862107373, 'precision': 1.0, 'recall': 0.4552238805970149, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.58it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.79it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.42it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7164179104477612, 'auc': 0.991882936065939, 'precision': 1.0, 'recall': 0.43283582089552236, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 416.43it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 422.77it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 416.31it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7276119402985075, 'auc': 0.9922170862107373, 'precision': 1.0, 'recall': 0.4552238805970149, 'fpr': 0.0}
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.07it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.66it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.67it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5889235569422777, 'auc': 0.9525020626410079, 'precision': 1.0, 'recall': 0.17784711388455537, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-147.875000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 446.18it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 414.70it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 407.45it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5709828393135725, 'auc': 0.9219725419281982, 'precision': 0.9789473684210527, 'recall': 0.14508580343213728, 'fpr': 0.0031201248049922}
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_last
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1760, 3584)
[DEBUG] Returning (1760, 3584) activations
[DEBUG] train activations shape: (1760, 3584), dtype: float16
[DEBUG] train activations range: [-1067.000000, 73.875000]
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_mean
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.88GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.69it/s]
[DEBUG] Final encoded shape: (1760, 262144)
Encoded feature matrix shape: (1760, 262144)
[DEBUG] After encoding. Memory: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 262144), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1760, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.73GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.86it/s]
[DEBUG] Final encoded shape: (1760, 262144)
Encoded feature matrix shape: (1760, 262144)
[DEBUG] After encoding. Memory: RAM: 11.45GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 262144), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 11.45GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1760, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.45GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.45GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.45GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.62it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.86it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.93it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.871268656716418, 'auc': 0.905421586099354, 'precision': 0.9627906976744186, 'recall': 0.7723880597014925, 'fpr': 0.029850746268656716}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.96it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.06it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7164179104477612, 'auc': 0.9614613499665849, 'precision': 0.9833333333333333, 'recall': 0.44029850746268656, 'fpr': 0.007462686567164179}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.63it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.06it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.13it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.871268656716418, 'auc': 0.905421586099354, 'precision': 0.9627906976744186, 'recall': 0.7723880597014925, 'fpr': 0.029850746268656716}
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.06it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.77it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.91it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7164179104477612, 'auc': 0.9614613499665849, 'precision': 0.9833333333333333, 'recall': 0.44029850746268656, 'fpr': 0.007462686567164179}
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.13it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.56it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.61it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7152886115444618, 'auc': 0.8102151231135049, 'precision': 0.8382352941176471, 'recall': 0.5335413416536662, 'fpr': 0.1029641185647426}
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.40it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.01it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.05it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5842433697347894, 'auc': 0.7218586403362531, 'precision': 1.0, 'recall': 0.1684867394695788, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.88GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.57it/s]
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_max
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Final encoded shape: (1760, 262144)
Encoded feature matrix shape: (1760, 262144)
[DEBUG] After encoding. Memory: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 262144), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1760, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.72GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.88it/s]
[DEBUG] Final encoded shape: (1760, 262144)
Encoded feature matrix shape: (1760, 262144)
[DEBUG] After encoding. Memory: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 262144), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1760, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.62it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.87it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.07it/s]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6884328358208955, 'auc': 0.9937346847850301, 'precision': 1.0, 'recall': 0.376865671641791, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.68it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.01it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7481343283582089, 'auc': 0.9797421474715973, 'precision': 1.0, 'recall': 0.4962686567164179, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.59it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.82it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.57it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6884328358208955, 'auc': 0.9937346847850301, 'precision': 1.0, 'recall': 0.376865671641791, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.05it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.70it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.60it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7481343283582089, 'auc': 0.9797421474715973, 'precision': 1.0, 'recall': 0.4962686567164179, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.10it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.66it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.74it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5452418096723869, 'auc': 0.937268941615699, 'precision': 1.0, 'recall': 0.0904836193447738, 'fpr': 0.0}
  🚀 [BATCH] Training probe: attention
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: full
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.42it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.14it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.20it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.641965678627145, 'auc': 0.938928789600882, 'precision': 1.0, 'recall': 0.2839313572542902, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.72GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.91it/s]
[DEBUG] Final encoded shape: (1760, 262144)
Encoded feature matrix shape: (1760, 262144)
[DEBUG] After encoding. Memory: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 262144), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1760, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 1760 full activations
[DEBUG] Estimated memory requirement: 3.32 GB
[WARNING] Large memory requirement (3.32 GB).
[DEBUG] Max sequence length: 520
[DEBUG] Shape of first activation: (1, 149, 3584)
[DEBUG] Final array shape: (1760, 520, 3584)
[DEBUG] Returning (1760, 520, 3584) activations
[DEBUG] train activations shape: (1760, 520, 3584), dtype: float16
[DEBUG] train activations range: [-1674.000000, 237.000000]
Train activations: (1760, 520, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: attention
  [DEBUG] Creating attention probe
  [DEBUG] Fitting attention probe normally...
Epoch 10/100, Loss: 0.2252
Epoch 20/100, Loss: 0.0931
Epoch 30/100, Loss: 0.0389
Epoch 40/100, Loss: 0.0128
Epoch 50/100, Loss: 0.0052
Early stopping at epoch 51
Saved probe to results/spam_gemma_9b/seed_42/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state.npz
Saved training log to results/spam_gemma_9b/seed_42/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state_train_log.json
  - 🔥 Probe state saved to train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.79it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.01it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9347014925373134, 'auc': 0.9628814880819783, 'precision': 0.9678714859437751, 'recall': 0.8992537313432836, 'fpr': 0.029850746268656716}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.99it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.79it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9347014925373134, 'auc': 0.9628814880819783, 'precision': 0.9678714859437751, 'recall': 0.8992537313432836, 'fpr': 0.029850746268656716}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
Loaded probe from results/spam_gemma_9b/seed_42/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 536 full activations
[DEBUG] Estimated memory requirement: 1.58 GB
[WARNING] Large memory requirement (1.58 GB).
[DEBUG] Max sequence length: 939
[DEBUG] Shape of first activation: (1, 151, 3584)
[DEBUG] Final array shape: (536, 939, 3584)
[DEBUG] Returning (536, 939, 3584) activations
[DEBUG] test activations shape: (536, 939, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.500000]
Test activations: (536, 939, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating validation on '94_better_spam': Got unsupported ScalarType BFloat16
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.41it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.21it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.08it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6638065522620905, 'auc': 0.7647688746863448, 'precision': 0.7868852459016393, 'recall': 0.44929797191887677, 'fpr': 0.12168486739469579}
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
Loaded probe from results/spam_gemma_9b/seed_42/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state.npz
Converted attention probe back to bfloat16 for training
Test activations: (536, 939, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating test on '94_better_spam': Got unsupported ScalarType BFloat16
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.71GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.90it/s]
Loaded probe from results/spam_gemma_9b/seed_42/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 5157
[DEBUG] Processing 1282 full activations
[DEBUG] Estimated memory requirement: 0.60 GB
[WARNING] Large memory requirement (0.60 GB).
[DEBUG] Max sequence length: 210
[DEBUG] Shape of first activation: (1, 54, 3584)
[DEBUG] Final array shape: (1282, 210, 3584)
[DEBUG] Returning (1282, 210, 3584) activations
[DEBUG] test activations shape: (1282, 210, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (1282, 210, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating on '87_is_spam': Got unsupported ScalarType BFloat16

=== Group 43/700 — ('94_better_spam', 20, 'resid_post', 44, '{"class_counts": {"0": 1750, "1": 10}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
[DEBUG] Final encoded shape: (1760, 262144)
Encoded feature matrix shape: (1760, 262144)
[DEBUG] After encoding. Memory: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 262144), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1760, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1760, 3584)
[DEBUG] Returning (1760, 3584) activations
[DEBUG] train activations shape: (1760, 3584), dtype: float16
[DEBUG] train activations range: [-1526.000000, 117.875000]
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_mean
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.88GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 338.17it/s]
[DEBUG] Final encoded shape: (1760, 16384)
Encoded feature matrix shape: (1760, 16384)
[DEBUG] After encoding. Memory: RAM: 6.88GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 16384), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 6.88GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1760, 16384)
[DEBUG] Memory after feature matrix build: RAM: 6.88GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 6.88GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 6.88GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.04it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.75it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.73it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7294776119402985, 'auc': 0.9848797059478727, 'precision': 1.0, 'recall': 0.458955223880597, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-593.000000, 51.843750]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 415.98it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 415.20it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 387.57it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.753731343283582, 'auc': 0.9422337937179772, 'precision': 0.9927536231884058, 'recall': 0.5111940298507462, 'fpr': 0.0037313432835820895}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.02it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.00it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.63it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7294776119402985, 'auc': 0.9848797059478727, 'precision': 1.0, 'recall': 0.458955223880597, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 415.24it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 421.92it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 409.96it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.753731343283582, 'auc': 0.9422337937179772, 'precision': 0.9927536231884058, 'recall': 0.5111940298507462, 'fpr': 0.0037313432835820895}
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.45it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.03it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.06it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5795631825273011, 'auc': 0.9437988127949455, 'precision': 1.0, 'recall': 0.15912636505460218, 'fpr': 0.0}
  🚀 [BATCH] Training probe: attention
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: full
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1053.000000, 69.625000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 440.53it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 411.25it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 418.45it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6060842433697348, 'auc': 0.6979466074118784, 'precision': 0.8908045977011494, 'recall': 0.24180967238689546, 'fpr': 0.029641185647425898}
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1760, 3584)
[DEBUG] Returning (1760, 3584) activations
[DEBUG] train activations shape: (1760, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_max
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.88GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 338.07it/s]
[DEBUG] Final encoded shape: (1760, 16384)
Encoded feature matrix shape: (1760, 16384)
[DEBUG] After encoding. Memory: RAM: 6.88GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 16384), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 6.88GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1760, 16384)
[DEBUG] Memory after feature matrix build: RAM: 6.88GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 6.88GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 6.88GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-109.562500, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 416.35it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 436.09it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 434.60it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.667910447761194, 'auc': 0.9910197148585431, 'precision': 1.0, 'recall': 0.3358208955223881, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 1760 full activations
[DEBUG] Estimated memory requirement: 3.22 GB
[WARNING] Large memory requirement (3.22 GB).
[DEBUG] Max sequence length: 520
[DEBUG] Shape of first activation: (1, 78, 3584)
[DEBUG] Final array shape: (1760, 520, 3584)
[DEBUG] Returning (1760, 520, 3584) activations
[DEBUG] train activations shape: (1760, 520, 3584), dtype: float16
[DEBUG] train activations range: [-1674.000000, 237.000000]
Train activations: (1760, 520, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: attention
  [DEBUG] Creating attention probe
  [DEBUG] Fitting attention probe normally...
Epoch 10/100, Loss: 0.2043
Epoch 20/100, Loss: 0.0760
Epoch 30/100, Loss: 0.0267
Epoch 40/100, Loss: 0.0072
Epoch 50/100, Loss: 0.0043
Early stopping at epoch 53
Saved probe to results/spam_gemma_9b/seed_43/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state.npz
Saved training log to results/spam_gemma_9b/seed_43/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state_train_log.json
  - 🔥 Probe state saved to train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 410.40it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 423.58it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 414.78it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.667910447761194, 'auc': 0.9910197148585431, 'precision': 1.0, 'recall': 0.3358208955223881, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-143.875000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 440.19it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 406.11it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 426.08it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.500780031201248, 'auc': 0.7169375074534963, 'precision': 1.0, 'recall': 0.0015600624024961, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1760, 3584)
[DEBUG] Returning (1760, 3584) activations
[DEBUG] train activations shape: (1760, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 231.875000]
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.87GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 338.99it/s]
[DEBUG] Final encoded shape: (1760, 16384)
Encoded feature matrix shape: (1760, 16384)
[DEBUG] After encoding. Memory: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 16384), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1760, 16384)
[DEBUG] Memory after feature matrix build: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Loaded probe from results/spam_gemma_9b/seed_43/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 536 full activations
[DEBUG] Estimated memory requirement: 1.61 GB
[WARNING] Large memory requirement (1.61 GB).
[DEBUG] Max sequence length: 939
[DEBUG] Shape of first activation: (1, 62, 3584)
[DEBUG] Final array shape: (536, 939, 3584)
[DEBUG] Returning (536, 939, 3584) activations
[DEBUG] test activations shape: (536, 939, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.500000]
Test activations: (536, 939, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating validation on '94_better_spam': Got unsupported ScalarType BFloat16
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-238.250000, 80.812500]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 416.64it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 383.46it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 416.18it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8264925373134329, 'auc': 0.9411617286700824, 'precision': 0.9679144385026738, 'recall': 0.6753731343283582, 'fpr': 0.022388059701492536}
    [BATCH] Evaluating on 94_better_spam test set
Loaded probe from results/spam_gemma_9b/seed_43/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state.npz
Converted attention probe back to bfloat16 for training
Test activations: (536, 939, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating test on '94_better_spam': Got unsupported ScalarType BFloat16
Loaded probe from results/spam_gemma_9b/seed_43/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 5157
[DEBUG] Processing 1282 full activations
[DEBUG] Estimated memory requirement: 0.60 GB
[WARNING] Large memory requirement (0.60 GB).
[DEBUG] Max sequence length: 119
[DEBUG] Shape of first activation: (1, 8, 3584)
[DEBUG] Final array shape: (1282, 119, 3584)
[DEBUG] Returning (1282, 119, 3584) activations
[DEBUG] test activations shape: (1282, 119, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (1282, 119, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating on '87_is_spam': Got unsupported ScalarType BFloat16

=== Group 44/700 — ('94_better_spam', 20, 'resid_post', 45, '{"class_counts": {"0": 1750, "1": 10}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 411.65it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 423.28it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 386.71it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8264925373134329, 'auc': 0.9411617286700824, 'precision': 0.9679144385026738, 'recall': 0.6753731343283582, 'fpr': 0.022388059701492536}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-274.000000, 76.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 440.30it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 435.52it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 409.04it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5413416536661466, 'auc': 0.5972313151496419, 'precision': 0.5548654244306418, 'recall': 0.41809672386895474, 'fpr': 0.33541341653666146}
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-677.000000, 50.625000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 415.20it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 384.09it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 395.28it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7425373134328358, 'auc': 0.9043216752060592, 'precision': 0.9779411764705882, 'recall': 0.4962686567164179, 'fpr': 0.011194029850746268}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1505.000000, 110.875000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 440.60it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 404.23it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 406.88it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6879875195007801, 'auc': 0.8505382336978347, 'precision': 0.889967637540453, 'recall': 0.4290171606864275, 'fpr': 0.0530421216848674}
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1760, 3584)
[DEBUG] Returning (1760, 3584) activations
[DEBUG] train activations shape: (1760, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.86GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 337.22it/s]
[DEBUG] Final encoded shape: (1760, 16384)
Encoded feature matrix shape: (1760, 16384)
[DEBUG] After encoding. Memory: RAM: 6.90GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 16384), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 6.90GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1760, 16384)
[DEBUG] Memory after feature matrix build: RAM: 6.90GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 6.90GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 6.90GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1760, 3584)
[DEBUG] Returning (1760, 3584) activations
[DEBUG] train activations shape: (1760, 3584), dtype: float16
[DEBUG] train activations range: [-1337.000000, 237.000000]
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_max
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.73GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 337.31it/s]
[DEBUG] Final encoded shape: (1760, 16384)
Encoded feature matrix shape: (1760, 16384)
[DEBUG] After encoding. Memory: RAM: 9.73GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 16384), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 9.73GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1760, 16384)
[DEBUG] Memory after feature matrix build: RAM: 9.73GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 9.73GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 9.73GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-109.625000, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 411.73it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 393.20it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 387.18it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6585820895522388, 'auc': 0.9809255959010915, 'precision': 1.0, 'recall': 0.31716417910447764, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-120.437500, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 415.77it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 373.39it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 388.94it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6399253731343284, 'auc': 0.9815938961906884, 'precision': 1.0, 'recall': 0.2798507462686567, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 418.13it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 403.69it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 383.18it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6585820895522388, 'auc': 0.9809255959010915, 'precision': 1.0, 'recall': 0.31716417910447764, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-143.875000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 442.53it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 435.70it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 398.02it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.6217420615701383, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 416.14it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 402.14it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 413.15it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6399253731343284, 'auc': 0.9815938961906884, 'precision': 1.0, 'recall': 0.2798507462686567, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1336.000000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 445.49it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 404.04it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 411.85it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5179407176287052, 'auc': 0.9041036212431337, 'precision': 0.96, 'recall': 0.0374414976599064, 'fpr': 0.0015600624024961}
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_mean
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.87GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.54it/s]
[DEBUG] Final encoded shape: (1760, 262144)
Encoded feature matrix shape: (1760, 262144)
[DEBUG] After encoding. Memory: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 262144), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1760, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1760, 3584)
[DEBUG] Returning (1760, 3584) activations
[DEBUG] train activations shape: (1760, 3584), dtype: float16
[DEBUG] train activations range: [-1337.000000, 219.750000]
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.72GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 336.93it/s]
[DEBUG] Final encoded shape: (1760, 16384)
Encoded feature matrix shape: (1760, 16384)
[DEBUG] After encoding. Memory: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 16384), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1760, 16384)
[DEBUG] Memory after feature matrix build: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-227.500000, 81.187500]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 416.89it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 408.60it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 387.39it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7873134328358209, 'auc': 0.9328218979728224, 'precision': 0.9529411764705882, 'recall': 0.6044776119402985, 'fpr': 0.029850746268656716}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.63it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.80it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.88it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7313432835820896, 'auc': 0.9036672978391624, 'precision': 0.9920634920634921, 'recall': 0.4664179104477612, 'fpr': 0.0037313432835820895}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 418.13it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 388.33it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 382.55it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7873134328358209, 'auc': 0.9328218979728224, 'precision': 0.9529411764705882, 'recall': 0.6044776119402985, 'fpr': 0.029850746268656716}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1336.000000, 221.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 443.09it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 426.53it/s]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 428.32it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5499219968798752, 'auc': 0.700979115607682, 'precision': 0.6454545454545455, 'recall': 0.22152886115444617, 'fpr': 0.12168486739469579}
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.61it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.82it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.72it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7313432835820896, 'auc': 0.9036672978391624, 'precision': 0.9920634920634921, 'recall': 0.4664179104477612, 'fpr': 0.0037313432835820895}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.09it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.61it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s][DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1760, 3584)
[DEBUG] Returning (1760, 3584) activations
[DEBUG] train activations shape: (1760, 3584), dtype: float16
[DEBUG] train activations range: [-1337.000000, 237.000000]
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_softmax
  [DEBUG] Fitting SAE probe normally...
Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.75it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5257410296411856, 'auc': 0.49653305945030307, 'precision': 0.7704918032786885, 'recall': 0.07332293291731669, 'fpr': 0.0218408736349454}
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.72GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 335.83it/s]
[DEBUG] Final encoded shape: (1760, 16384)
Encoded feature matrix shape: (1760, 16384)
[DEBUG] After encoding. Memory: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 16384), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1760, 16384)
[DEBUG] Memory after feature matrix build: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_max
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-120.625000, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 414.13it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 390.60it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 388.33it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6231343283582089, 'auc': 0.9830279572287816, 'precision': 1.0, 'recall': 0.2462686567164179, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.87GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.70it/s]
[DEBUG] Final encoded shape: (1760, 262144)
Encoded feature matrix shape: (1760, 262144)
[DEBUG] After encoding. Memory: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 262144), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1760, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 418.01it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 382.80it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 393.09it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6231343283582089, 'auc': 0.9830279572287816, 'precision': 1.0, 'recall': 0.2462686567164179, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1336.000000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 442.16it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 404.62it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 400.83it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5179407176287052, 'auc': 0.8727977200211253, 'precision': 0.896551724137931, 'recall': 0.0405616224648986, 'fpr': 0.0046801872074883}
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.62it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.81it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.06it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6958955223880597, 'auc': 0.9888338159946536, 'precision': 1.0, 'recall': 0.3917910447761194, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1760, 3584)
[DEBUG] Returning (1760, 3584) activations
[DEBUG] train activations shape: (1760, 3584), dtype: float16
[DEBUG] train activations range: [-1506.000000, 112.125000]
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_mean
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.73GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.88it/s]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Final encoded shape: (1760, 262144)
Encoded feature matrix shape: (1760, 262144)
[DEBUG] After encoding. Memory: RAM: 11.45GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 262144), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 11.45GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1760, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.45GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.45GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.45GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.67it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.22it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.16it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6958955223880597, 'auc': 0.9888338159946536, 'precision': 1.0, 'recall': 0.3917910447761194, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.09it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.76it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.60it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5023400936037441, 'auc': 0.7543205940406104, 'precision': 1.0, 'recall': 0.0046801872074883, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.68it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.88it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7164179104477612, 'auc': 0.9094731566050346, 'precision': 0.9603174603174603, 'recall': 0.45149253731343286, 'fpr': 0.018656716417910446}
    [BATCH] Evaluating on 94_better_spam test set
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.86GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.57it/s]
[DEBUG] Final encoded shape: (1760, 262144)
Encoded feature matrix shape: (1760, 262144)
[DEBUG] After encoding. Memory: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 262144), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1760, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.04it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.72it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.64it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7164179104477612, 'auc': 0.9094731566050346, 'precision': 0.9603174603174603, 'recall': 0.45149253731343286, 'fpr': 0.018656716417910446}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.43it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.04it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.22it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5959438377535101, 'auc': 0.8053061592042464, 'precision': 0.9361702127659575, 'recall': 0.2059282371294852, 'fpr': 0.014040561622464899}
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.64it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.11it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.17it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.878731343283582, 'auc': 0.9534695923368233, 'precision': 0.9393939393939394, 'recall': 0.8097014925373134, 'fpr': 0.05223880597014925}
    [BATCH] Evaluating on 94_better_spam test set
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_max
  [DEBUG] Fitting SAE probe normally...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.72GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.86it/s]
[DEBUG] Final encoded shape: (1760, 262144)
Encoded feature matrix shape: (1760, 262144)
[DEBUG] After encoding. Memory: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 262144), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1760, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.64it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.21it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.07it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.878731343283582, 'auc': 0.9534695923368233, 'precision': 0.9393939393939394, 'recall': 0.8097014925373134, 'fpr': 0.05223880597014925}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.09it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.61it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.59it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6661466458658346, 'auc': 0.71392446961529, 'precision': 0.6754530477759473, 'recall': 0.6396255850234009, 'fpr': 0.3073322932917317}
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.02it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.74it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.72it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6847014925373134, 'auc': 0.9855758520828692, 'precision': 1.0, 'recall': 0.3694029850746269, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.86GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.55it/s]
[DEBUG] Final encoded shape: (1760, 262144)
Encoded feature matrix shape: (1760, 262144)
[DEBUG] After encoding. Memory: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 262144), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1760, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.00it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.68it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.52it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6847014925373134, 'auc': 0.9855758520828692, 'precision': 1.0, 'recall': 0.3694029850746269, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.42it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.19it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.04it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.515600624024961, 'auc': 0.8555956590837737, 'precision': 1.0, 'recall': 0.031201248049921998, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.64it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.86it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.71it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6791044776119403, 'auc': 0.9833203386054801, 'precision': 1.0, 'recall': 0.3582089552238806, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_last
  [DEBUG] Fitting SAE probe normally...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.72GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.87it/s]
[DEBUG] Final encoded shape: (1760, 262144)
Encoded feature matrix shape: (1760, 262144)
[DEBUG] After encoding. Memory: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 262144), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1760, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.61it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.14it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.88it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6791044776119403, 'auc': 0.9833203386054801, 'precision': 1.0, 'recall': 0.3582089552238806, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.09it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.59it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.60it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.6943275546934513, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: attention
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: full
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.02it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.71it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.93it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8246268656716418, 'auc': 0.8984740476720874, 'precision': 0.935, 'recall': 0.6977611940298507, 'fpr': 0.048507462686567165}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.02it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.07it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.82it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8246268656716418, 'auc': 0.8984740476720874, 'precision': 0.935, 'recall': 0.6977611940298507, 'fpr': 0.048507462686567165}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.44it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.16it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.15it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7730109204368175, 'auc': 0.8537776144431113, 'precision': 0.8600823045267489, 'recall': 0.6521060842433697, 'fpr': 0.1060842433697348}
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.72GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.91it/s]
[DEBUG] Final encoded shape: (1760, 262144)
Encoded feature matrix shape: (1760, 262144)
[DEBUG] After encoding. Memory: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 262144), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1760, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 1760 full activations
[DEBUG] Estimated memory requirement: 3.38 GB
[WARNING] Large memory requirement (3.38 GB).
[DEBUG] Max sequence length: 1167
[DEBUG] Shape of first activation: (1, 251, 3584)
[DEBUG] Final array shape: (1760, 1167, 3584)
[DEBUG] Returning (1760, 1167, 3584) activations
[DEBUG] train activations shape: (1760, 1167, 3584), dtype: float16
[DEBUG] train activations range: [-1674.000000, 237.000000]
Train activations: (1760, 1167, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: attention
  [DEBUG] Creating attention probe
  [DEBUG] Fitting attention probe normally...
Epoch 10/100, Loss: 0.3394
Epoch 20/100, Loss: 0.1626
Epoch 30/100, Loss: 0.0700
Epoch 40/100, Loss: 0.0272
Epoch 50/100, Loss: 0.0084
Epoch 60/100, Loss: 0.0043
Early stopping at epoch 66
Saved probe to results/spam_gemma_9b/seed_44/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state.npz
Saved training log to results/spam_gemma_9b/seed_44/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state_train_log.json
  - 🔥 Probe state saved to train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.00it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.87it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6455223880597015, 'auc': 0.9787953887280019, 'precision': 1.0, 'recall': 0.291044776119403, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Loaded probe from results/spam_gemma_9b/seed_44/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 536 full activations
[DEBUG] Estimated memory requirement: 1.59 GB
[WARNING] Large memory requirement (1.59 GB).
[DEBUG] Max sequence length: 935
[DEBUG] Shape of first activation: (1, 56, 3584)
[DEBUG] Final array shape: (536, 935, 3584)
[DEBUG] Returning (536, 935, 3584) activations
[DEBUG] test activations shape: (536, 935, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (536, 935, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating validation on '94_better_spam': Got unsupported ScalarType BFloat16
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.02it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.08it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6455223880597015, 'auc': 0.9787953887280019, 'precision': 1.0, 'recall': 0.291044776119403, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.39it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.07it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.18it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5039001560062403, 'auc': 0.8263706523299933, 'precision': 0.8571428571428571, 'recall': 0.0093603744149766, 'fpr': 0.0015600624024961}
  🚀 [BATCH] Training probe: attention
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: full
Loaded probe from results/spam_gemma_9b/seed_44/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state.npz
Converted attention probe back to bfloat16 for training
Test activations: (536, 935, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating test on '94_better_spam': Got unsupported ScalarType BFloat16
Loaded probe from results/spam_gemma_9b/seed_44/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 5157
[DEBUG] Processing 1282 full activations
[DEBUG] Estimated memory requirement: 0.60 GB
[WARNING] Large memory requirement (0.60 GB).
[DEBUG] Max sequence length: 163
[DEBUG] Shape of first activation: (1, 45, 3584)
[DEBUG] Final array shape: (1282, 163, 3584)
[DEBUG] Returning (1282, 163, 3584) activations
[DEBUG] test activations shape: (1282, 163, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (1282, 163, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating on '87_is_spam': Got unsupported ScalarType BFloat16

=== Group 45/700 — ('94_better_spam', 20, 'resid_post', 46, '{"class_counts": {"0": 1750, "1": 10}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1760, 3584)
[DEBUG] Returning (1760, 3584) activations
[DEBUG] train activations shape: (1760, 3584), dtype: float16
[DEBUG] train activations range: [-1506.000000, 112.125000]
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_mean
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.85GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 336.43it/s]
[DEBUG] Final encoded shape: (1760, 16384)
Encoded feature matrix shape: (1760, 16384)
[DEBUG] After encoding. Memory: RAM: 6.85GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 16384), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 6.85GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1760, 16384)
[DEBUG] Memory after feature matrix build: RAM: 6.85GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 6.85GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 6.85GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 1760 full activations
[DEBUG] Estimated memory requirement: 3.29 GB
[WARNING] Large memory requirement (3.29 GB).
[DEBUG] Max sequence length: 501
[DEBUG] Shape of first activation: (1, 108, 3584)
[DEBUG] Final array shape: (1760, 501, 3584)
[DEBUG] Returning (1760, 501, 3584) activations
[DEBUG] train activations shape: (1760, 501, 3584), dtype: float16
[DEBUG] train activations range: [-1674.000000, 237.000000]
Train activations: (1760, 501, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: attention
  [DEBUG] Creating attention probe
  [DEBUG] Fitting attention probe normally...
Epoch 10/100, Loss: 0.2935
Epoch 20/100, Loss: 0.1176
Epoch 30/100, Loss: 0.0548
Epoch 40/100, Loss: 0.0290
Epoch 50/100, Loss: 0.0152
Epoch 60/100, Loss: 0.0103
Epoch 70/100, Loss: 0.0053
Early stopping at epoch 74
Saved probe to results/spam_gemma_9b/seed_45/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state.npz
Saved training log to results/spam_gemma_9b/seed_45/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state_train_log.json
  - 🔥 Probe state saved to train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-1053.000000, 68.750000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 413.35it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 387.43it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 387.18it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7593283582089553, 'auc': 0.943097014925373, 'precision': 0.986013986013986, 'recall': 0.5261194029850746, 'fpr': 0.007462686567164179}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 412.05it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 394.98it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 400.87it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7593283582089553, 'auc': 0.943097014925373, 'precision': 0.986013986013986, 'recall': 0.5261194029850746, 'fpr': 0.007462686567164179}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1490.000000, 111.750000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 445.18it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 423.77it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 427.31it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6918876755070202, 'auc': 0.741002869443951, 'precision': 0.9659090909090909, 'recall': 0.39781591263650545, 'fpr': 0.014040561622464899}
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
Loaded probe from results/spam_gemma_9b/seed_45/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 536 full activations
[DEBUG] Estimated memory requirement: 1.55 GB
[WARNING] Large memory requirement (1.55 GB).
[DEBUG] Max sequence length: 939
[DEBUG] Shape of first activation: (1, 61, 3584)
[DEBUG] Final array shape: (536, 939, 3584)
[DEBUG] Returning (536, 939, 3584) activations
[DEBUG] test activations shape: (536, 939, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (536, 939, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating validation on '94_better_spam': Got unsupported ScalarType BFloat16
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1760, 3584)
[DEBUG] Returning (1760, 3584) activations
[DEBUG] train activations shape: (1760, 3584), dtype: float16
[DEBUG] train activations range: [-1337.000000, 237.000000]
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_max
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.89GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 336.64it/s]
[DEBUG] Final encoded shape: (1760, 16384)
Encoded feature matrix shape: (1760, 16384)
[DEBUG] After encoding. Memory: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 16384), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1760, 16384)
[DEBUG] Memory after feature matrix build: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Loaded probe from results/spam_gemma_9b/seed_45/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state.npz
Converted attention probe back to bfloat16 for training
Test activations: (536, 939, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating test on '94_better_spam': Got unsupported ScalarType BFloat16
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-162.375000, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 418.84it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 395.47it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 418.55it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7388059701492538, 'auc': 0.9872466028068612, 'precision': 1.0, 'recall': 0.47761194029850745, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Loaded probe from results/spam_gemma_9b/seed_45/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 5157
[DEBUG] Processing 1282 full activations
[DEBUG] Estimated memory requirement: 0.60 GB
[WARNING] Large memory requirement (0.60 GB).
[DEBUG] Max sequence length: 210
[DEBUG] Shape of first activation: (1, 65, 3584)
[DEBUG] Final array shape: (1282, 210, 3584)
[DEBUG] Returning (1282, 210, 3584) activations
[DEBUG] test activations shape: (1282, 210, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (1282, 210, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating on '87_is_spam': Got unsupported ScalarType BFloat16

=== Group 46/700 — ('94_better_spam', 20, 'resid_post', 47, '{"class_counts": {"0": 1750, "1": 10}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 418.09it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 411.85it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 383.29it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7388059701492538, 'auc': 0.9872466028068612, 'precision': 1.0, 'recall': 0.47761194029850745, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-570.000000, 51.906250]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 413.68it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 391.30it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 396.74it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7835820895522388, 'auc': 0.9400478948540879, 'precision': 1.0, 'recall': 0.5671641791044776, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1306.000000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 444.64it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 431.53it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 411.15it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5678627145085804, 'auc': 0.939349836083927, 'precision': 1.0, 'recall': 0.1357254290171607, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1049.000000, 65.937500]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 441.16it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 414.56it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 412.18it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7550702028081123, 'auc': 0.9109085112234442, 'precision': 0.9455040871934605, 'recall': 0.5413416536661466, 'fpr': 0.031201248049921998}
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1760, 3584)
[DEBUG] Returning (1760, 3584) activations
[DEBUG] train activations shape: (1760, 3584), dtype: float16
[DEBUG] train activations range: [-1337.000000, 219.750000]
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.87GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 338.13it/s]
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1760, 3584)
[DEBUG] Returning (1760, 3584) activations
[DEBUG] train activations shape: (1760, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_max
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.71GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 331.37it/s]
[DEBUG] Final encoded shape: (1760, 16384)
Encoded feature matrix shape: (1760, 16384)
[DEBUG] After encoding. Memory: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 16384), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1760, 16384)
[DEBUG] Memory after feature matrix build: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Final encoded shape: (1760, 16384)
Encoded feature matrix shape: (1760, 16384)
[DEBUG] After encoding. Memory: RAM: 9.71GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 16384), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 9.71GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1760, 16384)
[DEBUG] Memory after feature matrix build: RAM: 9.71GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 9.71GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 9.71GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-231.750000, 90.750000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 393.76it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 409.76it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 404.23it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.878731343283582, 'auc': 0.9624220316328804, 'precision': 0.9903381642512077, 'recall': 0.7649253731343284, 'fpr': 0.007462686567164179}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-106.125000, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 417.84it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 384.87it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 375.97it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.707089552238806, 'auc': 0.9686038093116508, 'precision': 1.0, 'recall': 0.4141791044776119, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 414.42it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 386.75it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 389.59it/s]
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 391.88it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.878731343283582, 'auc': 0.9624220316328804, 'precision': 0.9903381642512077, 'recall': 0.7649253731343284, 'fpr': 0.007462686567164179}
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 183.38it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 273.08it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.707089552238806, 'auc': 0.9686038093116508, 'precision': 1.0, 'recall': 0.4141791044776119, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1306.000000, 211.500000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-177.375000, 237.500000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 447.39it/s]
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 445.37it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 440.16it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 253.66it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 274.00it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 236.98it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5998439937597504, 'auc': 0.7451025966155651, 'precision': 0.7206896551724138, 'recall': 0.32605304212168484, 'fpr': 0.12636505460218408}
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5187207488299532, 'auc': 0.9059119307049972, 'precision': 0.9615384615384616, 'recall': 0.0390015600624025, 'fpr': 0.0015600624024961}
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1760, 3584)
[DEBUG] Returning (1760, 3584) activations
[DEBUG] train activations shape: (1760, 3584), dtype: float16
[DEBUG] train activations range: [-1337.000000, 237.000000]
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_softmax
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1760, 3584)
[DEBUG] Returning (1760, 3584) activations
[DEBUG] train activations shape: (1760, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 231.875000]
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.89GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 337.01it/s]

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.71GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 336.82it/s]
[DEBUG] Final encoded shape: (1760, 16384)
Encoded feature matrix shape: (1760, 16384)
[DEBUG] After encoding. Memory: RAM: 9.71GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 16384), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 9.71GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1760, 16384)
[DEBUG] Memory after feature matrix build: RAM: 9.71GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 9.71GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 9.71GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Final encoded shape: (1760, 16384)
Encoded feature matrix shape: (1760, 16384)
[DEBUG] After encoding. Memory: RAM: 6.89GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 16384), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 6.89GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1760, 16384)
[DEBUG] Memory after feature matrix build: RAM: 6.89GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 6.89GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 6.89GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-237.375000, 80.312500]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 416.23it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 409.60it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 418.26it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.835820895522388, 'auc': 0.9605842058364892, 'precision': 0.9639175257731959, 'recall': 0.6977611940298507, 'fpr': 0.026119402985074626}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-162.375000, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 416.51it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 390.39it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 395.06it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7723880597014925, 'auc': 0.9904906437959456, 'precision': 1.0, 'recall': 0.5447761194029851, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 415.94it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 384.62it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 421.11it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.835820895522388, 'auc': 0.9605842058364892, 'precision': 0.9639175257731959, 'recall': 0.6977611940298507, 'fpr': 0.026119402985074626}
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 413.15it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 406.54it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 408.32it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7723880597014925, 'auc': 0.9904906437959456, 'precision': 1.0, 'recall': 0.5447761194029851, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-268.250000, 76.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 444.62it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 419.56it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 435.50it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5639625585023401, 'auc': 0.6418865802994055, 'precision': 0.5907079646017699, 'recall': 0.4165366614664587, 'fpr': 0.28861154446177845}
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1306.000000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 444.48it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 406.13it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 434.82it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5483619344773791, 'auc': 0.901584643728963, 'precision': 1.0, 'recall': 0.0967238689547582, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1760, 3584)
[DEBUG] Returning (1760, 3584) activations
[DEBUG] train activations shape: (1760, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.72GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 338.59it/s]
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_mean
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Final encoded shape: (1760, 16384)
Encoded feature matrix shape: (1760, 16384)
[DEBUG] After encoding. Memory: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 16384), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1760, 16384)
[DEBUG] Memory after feature matrix build: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 9.72GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.86GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.68it/s]
[DEBUG] Final encoded shape: (1760, 262144)
Encoded feature matrix shape: (1760, 262144)
[DEBUG] After encoding. Memory: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 262144), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1760, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-106.500000, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 417.30it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 383.64it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 409.04it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7369402985074627, 'auc': 0.9749805079082201, 'precision': 1.0, 'recall': 0.47388059701492535, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 413.35it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 408.88it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 418.55it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7369402985074627, 'auc': 0.9749805079082201, 'precision': 1.0, 'recall': 0.47388059701492535, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-177.375000, 237.500000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.59it/s]
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 445.00it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.95it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 391.17it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s][DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 130.90it/s]
Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 30.41it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5288611544461779, 'auc': 0.8188623956814746, 'precision': 0.9743589743589743, 'recall': 0.059282371294851796, 'fpr': 0.0015600624024961}
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6958955223880597, 'auc': 0.901244709289374, 'precision': 0.9906542056074766, 'recall': 0.39552238805970147, 'fpr': 0.0037313432835820895}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1760, 3584)
[DEBUG] Returning (1760, 3584) activations
[DEBUG] train activations shape: (1760, 3584), dtype: float16
[DEBUG] train activations range: [-1526.000000, 117.875000]
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_mean
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.62it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.94it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.97it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6958955223880597, 'auc': 0.901244709289374, 'precision': 0.9906542056074766, 'recall': 0.39552238805970147, 'fpr': 0.0037313432835820895}

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.72GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.90it/s]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Final encoded shape: (1760, 262144)
Encoded feature matrix shape: (1760, 262144)
[DEBUG] After encoding. Memory: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 262144), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1760, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.23it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.84it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.74it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6076443057722309, 'auc': 0.5130731282293414, 'precision': 0.9791666666666666, 'recall': 0.21996879875195008, 'fpr': 0.0046801872074883}
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_max
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.76it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.81it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8171641791044776, 'auc': 0.9540404321675207, 'precision': 0.9941860465116279, 'recall': 0.6380597014925373, 'fpr': 0.0037313432835820895}
    [BATCH] Evaluating on 94_better_spam test set

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.86GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.70it/s]
[DEBUG] Final encoded shape: (1760, 262144)
Encoded feature matrix shape: (1760, 262144)
[DEBUG] After encoding. Memory: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 262144), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1760, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.04it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.02it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.96it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8171641791044776, 'auc': 0.9540404321675207, 'precision': 0.9941860465116279, 'recall': 0.6380597014925373, 'fpr': 0.0037313432835820895}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.64it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.85it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.87it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.792910447761194, 'auc': 0.9915627088438406, 'precision': 0.9937106918238994, 'recall': 0.5895522388059702, 'fpr': 0.0037313432835820895}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.40it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.03it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.04it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5046801872074883, 'auc': 0.47954030485712407, 'precision': 0.5652173913043478, 'recall': 0.0405616224648986, 'fpr': 0.031201248049921998}
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_max
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.61it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.87it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.78it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.792910447761194, 'auc': 0.9915627088438406, 'precision': 0.9937106918238994, 'recall': 0.5895522388059702, 'fpr': 0.0037313432835820895}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.72GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.88it/s]
[DEBUG] Final encoded shape: (1760, 262144)
Encoded feature matrix shape: (1760, 262144)
[DEBUG] After encoding. Memory: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 262144), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1760, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.21it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.76it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.80it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6123244929797191, 'auc': 0.9518425042773941, 'precision': 1.0, 'recall': 0.22464898595943839, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_last
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.57it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.01it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7182835820895522, 'auc': 0.9852556248607708, 'precision': 1.0, 'recall': 0.43656716417910446, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.86GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.70it/s]
[DEBUG] Final encoded shape: (1760, 262144)
Encoded feature matrix shape: (1760, 262144)
[DEBUG] After encoding. Memory: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 262144), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1760, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.02it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.70it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.02it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7182835820895522, 'auc': 0.9852556248607708, 'precision': 1.0, 'recall': 0.43656716417910446, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.63it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.92it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.88it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8880597014925373, 'auc': 0.9582033860548006, 'precision': 0.9642857142857143, 'recall': 0.8059701492537313, 'fpr': 0.029850746268656716}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.40it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.08it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.05it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5351014040561622, 'auc': 0.901324227696097, 'precision': 0.9787234042553191, 'recall': 0.0717628705148206, 'fpr': 0.0015600624024961}
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_last
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.62it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.75it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.85it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8880597014925373, 'auc': 0.9582033860548006, 'precision': 0.9642857142857143, 'recall': 0.8059701492537313, 'fpr': 0.029850746268656716}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.72GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.89it/s]
[DEBUG] Final encoded shape: (1760, 262144)
Encoded feature matrix shape: (1760, 262144)
[DEBUG] After encoding. Memory: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 262144), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1760, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.07it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.68it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.66it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6864274570982839, 'auc': 0.8120769760587615, 'precision': 0.8153034300791556, 'recall': 0.48205928237129486, 'fpr': 0.10920436817472699}
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_softmax
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.05it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.84it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.67it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8861940298507462, 'auc': 0.9312207618623303, 'precision': 0.9726027397260274, 'recall': 0.7947761194029851, 'fpr': 0.022388059701492536}
    [BATCH] Evaluating on 94_better_spam test set

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.86GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.71it/s]
[DEBUG] Final encoded shape: (1760, 262144)
Encoded feature matrix shape: (1760, 262144)
[DEBUG] After encoding. Memory: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 262144), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1760, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.05it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.02it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.75it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8861940298507462, 'auc': 0.9312207618623303, 'precision': 0.9726027397260274, 'recall': 0.7947761194029851, 'fpr': 0.022388059701492536}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.64it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.86it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.16it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7742537313432836, 'auc': 0.993943528625529, 'precision': 1.0, 'recall': 0.5485074626865671, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.41it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.04it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.17it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6934477379095164, 'auc': 0.7899951567485476, 'precision': 0.7616033755274262, 'recall': 0.5631825273010921, 'fpr': 0.17628705148205928}
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_softmax
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.62it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.91it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.89it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7742537313432836, 'auc': 0.993943528625529, 'precision': 1.0, 'recall': 0.5485074626865671, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.72GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.90it/s]
[DEBUG] Final encoded shape: (1760, 262144)
Encoded feature matrix shape: (1760, 262144)
[DEBUG] After encoding. Memory: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 262144), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1760, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.14it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.60it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.62it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5772230889235569, 'auc': 0.9573039395834804, 'precision': 1.0, 'recall': 0.1544461778471139, 'fpr': 0.0}
  🚀 [BATCH] Training probe: attention
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: full
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.82it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.77it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7033582089552238, 'auc': 0.9789206950323012, 'precision': 1.0, 'recall': 0.40671641791044777, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.07it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.98it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.34it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7033582089552238, 'auc': 0.9789206950323012, 'precision': 1.0, 'recall': 0.40671641791044777, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 1760 full activations
[DEBUG] Estimated memory requirement: 3.34 GB
[WARNING] Large memory requirement (3.34 GB).
[DEBUG] Max sequence length: 520
[DEBUG] Shape of first activation: (1, 92, 3584)
[DEBUG] Final array shape: (1760, 520, 3584)
[DEBUG] Returning (1760, 520, 3584) activations
[DEBUG] train activations shape: (1760, 520, 3584), dtype: float16
[DEBUG] train activations range: [-1674.000000, 237.000000]
Train activations: (1760, 520, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: attention
  [DEBUG] Creating attention probe
  [DEBUG] Fitting attention probe normally...
Epoch 10/100, Loss: 0.2081
Epoch 20/100, Loss: 0.0732
Epoch 30/100, Loss: 0.0249
Epoch 40/100, Loss: 0.0126
Epoch 50/100, Loss: 0.0052
Early stopping at epoch 51
Saved probe to results/spam_gemma_9b/seed_46/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state.npz
Saved training log to results/spam_gemma_9b/seed_46/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state_train_log.json
  - 🔥 Probe state saved to train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.41it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.06it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.09it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5132605304212169, 'auc': 0.917114687707633, 'precision': 0.9473684210526315, 'recall': 0.028081123244929798, 'fpr': 0.0015600624024961}
  🚀 [BATCH] Training probe: attention
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: full
Loaded probe from results/spam_gemma_9b/seed_46/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 536 full activations
[DEBUG] Estimated memory requirement: 1.60 GB
[WARNING] Large memory requirement (1.60 GB).
[DEBUG] Max sequence length: 1167
[DEBUG] Shape of first activation: (1, 225, 3584)
[DEBUG] Final array shape: (536, 1167, 3584)
[DEBUG] Returning (536, 1167, 3584) activations
[DEBUG] test activations shape: (536, 1167, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (536, 1167, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating validation on '94_better_spam': Got unsupported ScalarType BFloat16
    [BATCH] Evaluating on 94_better_spam test set
Loaded probe from results/spam_gemma_9b/seed_46/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state.npz
Converted attention probe back to bfloat16 for training
Test activations: (536, 1167, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating test on '94_better_spam': Got unsupported ScalarType BFloat16
Loaded probe from results/spam_gemma_9b/seed_46/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 5157
[DEBUG] Processing 1282 full activations
[DEBUG] Estimated memory requirement: 0.60 GB
[WARNING] Large memory requirement (0.60 GB).
[DEBUG] Max sequence length: 163
[DEBUG] Shape of first activation: (1, 26, 3584)
[DEBUG] Final array shape: (1282, 163, 3584)
[DEBUG] Returning (1282, 163, 3584) activations
[DEBUG] test activations shape: (1282, 163, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (1282, 163, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating on '87_is_spam': Got unsupported ScalarType BFloat16

=== Group 47/700 — ('94_better_spam', 20, 'resid_post', 48, '{"class_counts": {"0": 1750, "1": 10}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1760, 3584)
[DEBUG] Returning (1760, 3584) activations
[DEBUG] train activations shape: (1760, 3584), dtype: float16
[DEBUG] train activations range: [-1526.000000, 117.875000]
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_mean
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.86GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 336.68it/s]
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 1760 full activations
[DEBUG] Estimated memory requirement: 3.31 GB
[WARNING] Large memory requirement (3.31 GB).
[DEBUG] Max sequence length: 1167
[DEBUG] Shape of first activation: (1, 109, 3584)
[DEBUG] Final array shape: (1760, 1167, 3584)
[DEBUG] Returning (1760, 1167, 3584) activations
[DEBUG] train activations shape: (1760, 1167, 3584), dtype: float16
[DEBUG] train activations range: [-1674.000000, 237.000000]
Train activations: (1760, 1167, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: attention
  [DEBUG] Creating attention probe
  [DEBUG] Fitting attention probe normally...
[DEBUG] Final encoded shape: (1760, 16384)
Encoded feature matrix shape: (1760, 16384)
[DEBUG] After encoding. Memory: RAM: 6.86GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 16384), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 6.86GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1760, 16384)
[DEBUG] Memory after feature matrix build: RAM: 6.86GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 6.86GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 6.86GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Epoch 10/100, Loss: 0.4104
Epoch 20/100, Loss: 0.2197
Epoch 30/100, Loss: 0.1045
Epoch 40/100, Loss: 0.0358
Epoch 50/100, Loss: 0.0109
Epoch 60/100, Loss: 0.0058
Early stopping at epoch 62
Saved probe to results/spam_gemma_9b/seed_47/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state.npz
Saved training log to results/spam_gemma_9b/seed_47/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state_train_log.json
  - 🔥 Probe state saved to train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-567.000000, 51.500000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 413.19it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 393.06it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 387.61it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7089552238805971, 'auc': 0.9633827132991758, 'precision': 0.9745762711864406, 'recall': 0.4291044776119403, 'fpr': 0.011194029850746268}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 411.97it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 419.05it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 428.47it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7089552238805971, 'auc': 0.9633827132991758, 'precision': 0.9745762711864406, 'recall': 0.4291044776119403, 'fpr': 0.011194029850746268}
Loaded probe from results/spam_gemma_9b/seed_47/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 536 full activations
[DEBUG] Estimated memory requirement: 1.60 GB
[WARNING] Large memory requirement (1.60 GB).
[DEBUG] Max sequence length: 939
[DEBUG] Shape of first activation: (1, 335, 3584)
[DEBUG] Final array shape: (536, 939, 3584)
[DEBUG] Returning (536, 939, 3584) activations
[DEBUG] test activations shape: (536, 939, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (536, 939, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating validation on '94_better_spam': Got unsupported ScalarType BFloat16
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1505.000000, 110.875000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 439.17it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 437.29it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 431.56it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6809672386895476, 'auc': 0.7471092603454528, 'precision': 0.8580246913580247, 'recall': 0.43369734789391573, 'fpr': 0.0717628705148206}
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
Loaded probe from results/spam_gemma_9b/seed_47/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state.npz
Converted attention probe back to bfloat16 for training
Test activations: (536, 939, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating test on '94_better_spam': Got unsupported ScalarType BFloat16
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1760, 3584)
[DEBUG] Returning (1760, 3584) activations
[DEBUG] train activations shape: (1760, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_max
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.86GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 336.58it/s]
[DEBUG] Final encoded shape: (1760, 16384)
Encoded feature matrix shape: (1760, 16384)
[DEBUG] After encoding. Memory: RAM: 6.86GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 16384), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 6.86GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1760, 16384)
[DEBUG] Memory after feature matrix build: RAM: 6.86GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 6.86GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 6.86GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Loaded probe from results/spam_gemma_9b/seed_47/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 5157
[DEBUG] Processing 1282 full activations
[DEBUG] Estimated memory requirement: 0.59 GB
[WARNING] Large memory requirement (0.59 GB).
[DEBUG] Max sequence length: 210
[DEBUG] Shape of first activation: (1, 20, 3584)
[DEBUG] Final array shape: (1282, 210, 3584)
[DEBUG] Returning (1282, 210, 3584) activations
[DEBUG] test activations shape: (1282, 210, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.500000]
Test activations: (1282, 210, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating on '87_is_spam': Got unsupported ScalarType BFloat16

=== Group 48/700 — ('94_better_spam', 20, 'resid_post', 49, '{"class_counts": {"0": 1750, "1": 10}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-106.125000, 237.500000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 412.66it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 415.24it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 392.69it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6791044776119403, 'auc': 0.9861466919135664, 'precision': 1.0, 'recall': 0.3582089552238806, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-813.000000, 51.906250]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 416.35it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 386.50it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 389.33it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.832089552238806, 'auc': 0.9489724883047449, 'precision': 1.0, 'recall': 0.664179104477612, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 407.45it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 388.72it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 386.36it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6791044776119403, 'auc': 0.9861466919135664, 'precision': 1.0, 'recall': 0.3582089552238806, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1336.000000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 442.48it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 414.29it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 418.74it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5351014040561622, 'auc': 0.8819341853237312, 'precision': 1.0, 'recall': 0.07020280811232449, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 417.63it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 381.20it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 410.32it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.832089552238806, 'auc': 0.9489724883047449, 'precision': 1.0, 'recall': 0.664179104477612, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1053.000000, 69.625000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 441.60it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 410.16it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 404.93it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.624804992199688, 'auc': 0.8569001730428032, 'precision': 0.9597701149425287, 'recall': 0.26053042121684866, 'fpr': 0.0109204368174727}
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1760, 3584)
[DEBUG] Returning (1760, 3584) activations
[DEBUG] train activations shape: (1760, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 231.875000]
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.86GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 336.64it/s]
[DEBUG] Final encoded shape: (1760, 16384)
Encoded feature matrix shape: (1760, 16384)
[DEBUG] After encoding. Memory: RAM: 6.86GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 16384), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 6.86GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1760, 16384)
[DEBUG] Memory after feature matrix build: RAM: 6.86GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 6.86GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 6.86GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1760, 3584)
[DEBUG] Returning (1760, 3584) activations
[DEBUG] train activations shape: (1760, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_max
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.70GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 336.51it/s]
[DEBUG] Final encoded shape: (1760, 16384)
Encoded feature matrix shape: (1760, 16384)
[DEBUG] After encoding. Memory: RAM: 9.70GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 16384), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 9.70GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1760, 16384)
[DEBUG] Memory after feature matrix build: RAM: 9.70GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 9.70GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 9.70GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-256.250000, 81.875000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 413.80it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 390.71it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 389.19it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8600746268656716, 'auc': 0.9717503898418356, 'precision': 0.9617224880382775, 'recall': 0.75, 'fpr': 0.029850746268656716}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-119.687500, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 414.83it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 385.26it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 386.89it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7201492537313433, 'auc': 0.9902678770327467, 'precision': 1.0, 'recall': 0.44029850746268656, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 412.42it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 383.04it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 391.88it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8600746268656716, 'auc': 0.9717503898418356, 'precision': 0.9617224880382775, 'recall': 0.75, 'fpr': 0.029850746268656716}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1336.000000, 221.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 440.07it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 433.01it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 409.98it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5546021840873635, 'auc': 0.6114884845003785, 'precision': 0.5817757009345794, 'recall': 0.38845553822152884, 'fpr': 0.2792511700468019}
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 414.17it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 414.13it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 389.33it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7201492537313433, 'auc': 0.9902678770327467, 'precision': 1.0, 'recall': 0.44029850746268656, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-177.375000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 443.94it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 429.70it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 431.53it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5078003120124805, 'auc': 0.8659709258885175, 'precision': 1.0, 'recall': 0.015600624024960999, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1760, 3584)
[DEBUG] Returning (1760, 3584) activations
[DEBUG] train activations shape: (1760, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.87GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 336.78it/s]
[DEBUG] Final encoded shape: (1760, 16384)
Encoded feature matrix shape: (1760, 16384)
[DEBUG] After encoding. Memory: RAM: 6.95GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 16384), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 6.95GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1760, 16384)
[DEBUG] Memory after feature matrix build: RAM: 6.95GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 6.96GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 6.96GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1760, 3584)
[DEBUG] Returning (1760, 3584) activations
[DEBUG] train activations shape: (1760, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 231.875000]
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.70GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 334.77it/s]
[DEBUG] Final encoded shape: (1760, 16384)
Encoded feature matrix shape: (1760, 16384)
[DEBUG] After encoding. Memory: RAM: 9.70GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 16384), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 9.70GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1760, 16384)
[DEBUG] Memory after feature matrix build: RAM: 9.70GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 9.70GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 9.70GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-106.500000, 237.500000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 416.85it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 392.47it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 390.02it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7126865671641791, 'auc': 0.9870099131209624, 'precision': 1.0, 'recall': 0.4253731343283582, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-210.625000, 80.875000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 415.94it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 384.98it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 391.77it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8843283582089553, 'auc': 0.9674482067275562, 'precision': 0.9724770642201835, 'recall': 0.7910447761194029, 'fpr': 0.022388059701492536}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 413.88it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 416.31it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 390.79it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7126865671641791, 'auc': 0.9870099131209624, 'precision': 1.0, 'recall': 0.4253731343283582, 'fpr': 0.0}
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1336.000000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 414.33it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 377.83it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 405.80it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8843283582089553, 'auc': 0.9674482067275562, 'precision': 0.9724770642201835, 'recall': 0.7910447761194029, 'fpr': 0.022388059701492536}
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 444.57it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 414.56it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 412.38it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5366614664586583, 'auc': 0.8829198721771024, 'precision': 0.9433962264150944, 'recall': 0.078003120124805, 'fpr': 0.0046801872074883}
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-263.750000, 76.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 444.92it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 410.98it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 400.99it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5249609984399376, 'auc': 0.6062558258960624, 'precision': 0.5481927710843374, 'recall': 0.2839313572542902, 'fpr': 0.23400936037441497}
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_mean
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1760, 3584)
[DEBUG] Returning (1760, 3584) activations
[DEBUG] train activations shape: (1760, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.70GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 337.77it/s]
[DEBUG] Final encoded shape: (1760, 16384)
Encoded feature matrix shape: (1760, 16384)
[DEBUG] After encoding. Memory: RAM: 9.70GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 16384), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 9.70GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1760, 16384)
[DEBUG] Memory after feature matrix build: RAM: 9.70GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 9.70GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 9.70GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.85GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.57it/s]
[DEBUG] Final encoded shape: (1760, 262144)
Encoded feature matrix shape: (1760, 262144)
[DEBUG] After encoding. Memory: RAM: 8.57GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 262144), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 8.57GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1760, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.57GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.57GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.57GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-119.750000, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 416.72it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 405.76it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 405.99it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7425373134328358, 'auc': 0.9875529071062599, 'precision': 1.0, 'recall': 0.48507462686567165, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.58it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.84it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.95it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6958955223880597, 'auc': 0.8725356426821118, 'precision': 0.9906542056074766, 'recall': 0.39552238805970147, 'fpr': 0.0037313432835820895}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 417.01it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 385.08it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 414.33it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7425373134328358, 'auc': 0.9875529071062599, 'precision': 1.0, 'recall': 0.48507462686567165, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-177.375000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 442.25it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 424.35it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 406.90it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5070202808112324, 'auc': 0.8167741998291477, 'precision': 1.0, 'recall': 0.014040561622464899, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1760, 3584)
[DEBUG] Returning (1760, 3584) activations
[DEBUG] train activations shape: (1760, 3584), dtype: float16
[DEBUG] train activations range: [-1526.000000, 117.875000]
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_mean
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.66it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.85it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.83it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6958955223880597, 'auc': 0.8725356426821118, 'precision': 0.9906542056074766, 'recall': 0.39552238805970147, 'fpr': 0.0037313432835820895}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.71GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.89it/s]
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.19it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.66it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.55it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5132605304212169, 'auc': 0.4699487199456777, 'precision': 0.8695652173913043, 'recall': 0.031201248049921998, 'fpr': 0.0046801872074883}
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
[DEBUG] Final encoded shape: (1760, 262144)
Encoded feature matrix shape: (1760, 262144)
[DEBUG] After encoding. Memory: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 262144), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1760, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_max
  [DEBUG] Fitting SAE probe normally...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.86GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.60it/s]
[DEBUG] Final encoded shape: (1760, 262144)
Encoded feature matrix shape: (1760, 262144)
[DEBUG] After encoding. Memory: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 262144), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1760, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.71it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7444029850746269, 'auc': 0.9466334372911561, 'precision': 1.0, 'recall': 0.48880597014925375, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.66it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.96it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.93it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7052238805970149, 'auc': 0.986675762976164, 'precision': 0.9910714285714286, 'recall': 0.4141791044776119, 'fpr': 0.0037313432835820895}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.74it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.97it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7444029850746269, 'auc': 0.9466334372911561, 'precision': 1.0, 'recall': 0.48880597014925375, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.41it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.02it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.00it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.500780031201248, 'auc': 0.500979602366622, 'precision': 0.6666666666666666, 'recall': 0.0031201248049922, 'fpr': 0.0015600624024961}
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.61it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.68it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.93it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7052238805970149, 'auc': 0.986675762976164, 'precision': 0.9910714285714286, 'recall': 0.4141791044776119, 'fpr': 0.0037313432835820895}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_max
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.12it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.13it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.66it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5327613104524181, 'auc': 0.8949866262981252, 'precision': 1.0, 'recall': 0.0655226209048362, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.70GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.87it/s]
[DEBUG] Final encoded shape: (1760, 262144)
Encoded feature matrix shape: (1760, 262144)
[DEBUG] After encoding. Memory: RAM: 11.41GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 262144), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 11.41GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1760, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.41GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.41GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.41GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_last
  [DEBUG] Fitting SAE probe normally...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.86GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.57it/s]
[DEBUG] Final encoded shape: (1760, 262144)
Encoded feature matrix shape: (1760, 262144)
[DEBUG] After encoding. Memory: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 262144), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1760, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.79it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.69it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7705223880597015, 'auc': 0.9945143684562263, 'precision': 1.0, 'recall': 0.5410447761194029, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.65it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.96it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.23it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.871268656716418, 'auc': 0.9397415905546892, 'precision': 0.954337899543379, 'recall': 0.7798507462686567, 'fpr': 0.03731343283582089}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.98it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.67it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7705223880597015, 'auc': 0.9945143684562263, 'precision': 1.0, 'recall': 0.5410447761194029, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.41it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.02it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.03it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5117004680187207, 'auc': 0.8654379248492873, 'precision': 1.0, 'recall': 0.0234009360374415, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.59it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.82it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.02it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.871268656716418, 'auc': 0.9397415905546892, 'precision': 0.954337899543379, 'recall': 0.7798507462686567, 'fpr': 0.03731343283582089}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_last
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.12it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.65it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.65it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5717628705148206, 'auc': 0.6099211207137832, 'precision': 0.5905511811023622, 'recall': 0.46801872074882994, 'fpr': 0.3244929797191888}
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.68GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.90it/s]
[DEBUG] Final encoded shape: (1760, 262144)
Encoded feature matrix shape: (1760, 262144)
[DEBUG] After encoding. Memory: RAM: 11.40GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 262144), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 11.40GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1760, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.40GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.40GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.40GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_softmax
  [DEBUG] Fitting SAE probe normally...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.86GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.58it/s]
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.85it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.51it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8544776119402985, 'auc': 0.9200128090888839, 'precision': 0.9656862745098039, 'recall': 0.7350746268656716, 'fpr': 0.026119402985074626}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Final encoded shape: (1760, 262144)
Encoded feature matrix shape: (1760, 262144)
[DEBUG] After encoding. Memory: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 262144), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1760, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.73it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.95it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8544776119402985, 'auc': 0.9200128090888839, 'precision': 0.9656862745098039, 'recall': 0.7350746268656716, 'fpr': 0.026119402985074626}
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.65it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.85it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.85it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6865671641791045, 'auc': 0.9860213856092671, 'precision': 0.9901960784313726, 'recall': 0.376865671641791, 'fpr': 0.0037313432835820895}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.41it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.18it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.13it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.609204368174727, 'auc': 0.6636374035304625, 'precision': 0.6871657754010695, 'recall': 0.40093603744149764, 'fpr': 0.18252730109204368}
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.63it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.79it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.80it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6865671641791045, 'auc': 0.9860213856092671, 'precision': 0.9901960784313726, 'recall': 0.376865671641791, 'fpr': 0.0037313432835820895}
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_softmax
  [DEBUG] Fitting SAE probe normally...
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.68GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.88it/s]
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.15it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.69it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.61it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5101404056162246, 'auc': 0.8427184513277567, 'precision': 1.0, 'recall': 0.0202808112324493, 'fpr': 0.0}
  🚀 [BATCH] Training probe: attention
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: full
[DEBUG] Final encoded shape: (1760, 262144)
Encoded feature matrix shape: (1760, 262144)
[DEBUG] After encoding. Memory: RAM: 11.40GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 262144), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 11.40GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1760, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.40GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.40GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.40GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.12it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.86it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7294776119402985, 'auc': 0.9913260191579416, 'precision': 1.0, 'recall': 0.458955223880597, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.06it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.00it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.88it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7294776119402985, 'auc': 0.9913260191579416, 'precision': 1.0, 'recall': 0.458955223880597, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 1760 full activations
[DEBUG] Estimated memory requirement: 3.27 GB
[WARNING] Large memory requirement (3.27 GB).
[DEBUG] Max sequence length: 520
[DEBUG] Shape of first activation: (1, 73, 3584)
[DEBUG] Final array shape: (1760, 520, 3584)
[DEBUG] Returning (1760, 520, 3584) activations
[DEBUG] train activations shape: (1760, 520, 3584), dtype: float16
[DEBUG] train activations range: [-1674.000000, 237.000000]
Train activations: (1760, 520, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: attention
  [DEBUG] Creating attention probe
  [DEBUG] Fitting attention probe normally...
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
Epoch 10/100, Loss: 0.1785
Epoch 20/100, Loss: 0.0598
Epoch 30/100, Loss: 0.0183
Epoch 40/100, Loss: 0.0050
Epoch 50/100, Loss: 0.0029
Early stopping at epoch 50
Saved probe to results/spam_gemma_9b/seed_48/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state.npz
Saved training log to results/spam_gemma_9b/seed_48/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state_train_log.json
  - 🔥 Probe state saved to train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.39it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.23it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.04it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.517160686427457, 'auc': 0.8724983632730644, 'precision': 1.0, 'recall': 0.0343213728549142, 'fpr': 0.0}
  🚀 [BATCH] Training probe: attention
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: full
Loaded probe from results/spam_gemma_9b/seed_48/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 536 full activations
[DEBUG] Estimated memory requirement: 1.63 GB
[WARNING] Large memory requirement (1.63 GB).
[DEBUG] Max sequence length: 939
[DEBUG] Shape of first activation: (1, 289, 3584)
[DEBUG] Final array shape: (536, 939, 3584)
[DEBUG] Returning (536, 939, 3584) activations
[DEBUG] test activations shape: (536, 939, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.500000]
Test activations: (536, 939, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating validation on '94_better_spam': Got unsupported ScalarType BFloat16
    [BATCH] Evaluating on 94_better_spam test set
Loaded probe from results/spam_gemma_9b/seed_48/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state.npz
Converted attention probe back to bfloat16 for training
Test activations: (536, 939, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating test on '94_better_spam': Got unsupported ScalarType BFloat16
Loaded probe from results/spam_gemma_9b/seed_48/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 5157
[DEBUG] Processing 1282 full activations
[DEBUG] Estimated memory requirement: 0.60 GB
[WARNING] Large memory requirement (0.60 GB).
[DEBUG] Max sequence length: 210
[DEBUG] Shape of first activation: (1, 17, 3584)
[DEBUG] Final array shape: (1282, 210, 3584)
[DEBUG] Returning (1282, 210, 3584) activations
[DEBUG] test activations shape: (1282, 210, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (1282, 210, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating on '87_is_spam': Got unsupported ScalarType BFloat16

=== Group 49/700 — ('94_better_spam', 20, 'resid_post', 50, '{"class_counts": {"0": 1750, "1": 10}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 1760 full activations
[DEBUG] Estimated memory requirement: 3.33 GB
[WARNING] Large memory requirement (3.33 GB).
[DEBUG] Max sequence length: 660
[DEBUG] Shape of first activation: (1, 13, 3584)
[DEBUG] Final array shape: (1760, 660, 3584)
[DEBUG] Returning (1760, 660, 3584) activations
[DEBUG] train activations shape: (1760, 660, 3584), dtype: float16
[DEBUG] train activations range: [-1674.000000, 237.000000]
Train activations: (1760, 660, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: attention
  [DEBUG] Creating attention probe
  [DEBUG] Fitting attention probe normally...
Epoch 10/100, Loss: 0.2207
Epoch 20/100, Loss: 0.0823
Epoch 30/100, Loss: 0.0409
Epoch 40/100, Loss: 0.0204
Epoch 50/100, Loss: 0.0123
Early stopping at epoch 57
Saved probe to results/spam_gemma_9b/seed_49/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state.npz
Saved training log to results/spam_gemma_9b/seed_49/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state_train_log.json
  - 🔥 Probe state saved to train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1760, 3584)
[DEBUG] Returning (1760, 3584) activations
[DEBUG] train activations shape: (1760, 3584), dtype: float16
[DEBUG] train activations range: [-1526.000000, 117.875000]
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_mean
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.85GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 335.42it/s]
[DEBUG] Final encoded shape: (1760, 16384)
Encoded feature matrix shape: (1760, 16384)
[DEBUG] After encoding. Memory: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 16384), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1760, 16384)
[DEBUG] Memory after feature matrix build: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-1067.000000, 71.062500]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 415.11it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 428.60it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 420.65it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7817164179104478, 'auc': 0.9402149699264868, 'precision': 0.9808917197452229, 'recall': 0.5746268656716418, 'fpr': 0.011194029850746268}
    [BATCH] Evaluating on 94_better_spam test set
Loaded probe from results/spam_gemma_9b/seed_49/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 536 full activations
[DEBUG] Estimated memory requirement: 1.55 GB
[WARNING] Large memory requirement (1.55 GB).
[DEBUG] Max sequence length: 1167
[DEBUG] Shape of first activation: (1, 171, 3584)
[DEBUG] Final array shape: (536, 1167, 3584)
[DEBUG] Returning (536, 1167, 3584) activations
[DEBUG] test activations shape: (536, 1167, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (536, 1167, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating validation on '94_better_spam': Got unsupported ScalarType BFloat16
    [BATCH] Evaluating on 94_better_spam test set
Loaded probe from results/spam_gemma_9b/seed_49/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state.npz
Converted attention probe back to bfloat16 for training
Test activations: (536, 1167, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating test on '94_better_spam': Got unsupported ScalarType BFloat16
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 415.94it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 373.16it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 372.13it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7817164179104478, 'auc': 0.9402149699264868, 'precision': 0.9808917197452229, 'recall': 0.5746268656716418, 'fpr': 0.011194029850746268}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1045.000000, 76.437500]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 443.77it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 436.97it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 414.91it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7472698907956318, 'auc': 0.875961653130712, 'precision': 0.9594202898550724, 'recall': 0.516380655226209, 'fpr': 0.0218408736349454}
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
Loaded probe from results/spam_gemma_9b/seed_49/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 5157
[DEBUG] Processing 1282 full activations
[DEBUG] Estimated memory requirement: 0.60 GB
[WARNING] Large memory requirement (0.60 GB).
[DEBUG] Max sequence length: 215
[DEBUG] Shape of first activation: (1, 33, 3584)
[DEBUG] Final array shape: (1282, 215, 3584)
[DEBUG] Returning (1282, 215, 3584) activations
[DEBUG] test activations shape: (1282, 215, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (1282, 215, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating on '87_is_spam': Got unsupported ScalarType BFloat16

=== Group 50/700 — ('94_better_spam', 20, 'resid_post', 51, '{"class_counts": {"0": 1750, "1": 10}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1760, 3584)
[DEBUG] Returning (1760, 3584) activations
[DEBUG] train activations shape: (1760, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_max
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.84GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 333.87it/s]
[DEBUG] Final encoded shape: (1760, 16384)
Encoded feature matrix shape: (1760, 16384)
[DEBUG] After encoding. Memory: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 16384), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1760, 16384)
[DEBUG] Memory after feature matrix build: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1760, 3584)
[DEBUG] Returning (1760, 3584) activations
[DEBUG] train activations shape: (1760, 3584), dtype: float16
[DEBUG] train activations range: [-1526.000000, 117.875000]
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_mean
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.70GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 337.73it/s]
[DEBUG] Final encoded shape: (1760, 16384)
Encoded feature matrix shape: (1760, 16384)
[DEBUG] After encoding. Memory: RAM: 9.70GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 16384), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 9.70GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1760, 16384)
[DEBUG] Memory after feature matrix build: RAM: 9.70GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 9.70GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 9.70GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-157.625000, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 413.31it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 419.64it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 387.21it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7686567164179104, 'auc': 0.9863276899086656, 'precision': 1.0, 'recall': 0.5373134328358209, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-782.000000, 52.031250]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 415.77it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 390.90it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 426.90it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8227611940298507, 'auc': 0.961892960570283, 'precision': 0.9942857142857143, 'recall': 0.6492537313432836, 'fpr': 0.0037313432835820895}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 411.93it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 385.51it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 388.22it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7686567164179104, 'auc': 0.9863276899086656, 'precision': 1.0, 'recall': 0.5373134328358209, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-163.875000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 445.04it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 436.16it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 423.77it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5117004680187207, 'auc': 0.8941664374843324, 'precision': 0.7777777777777778, 'recall': 0.0327613104524181, 'fpr': 0.0093603744149766}
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 414.42it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 419.01it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 424.01it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8227611940298507, 'auc': 0.961892960570283, 'precision': 0.9942857142857143, 'recall': 0.6492537313432836, 'fpr': 0.0037313432835820895}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1522.000000, 113.625000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 436.79it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 416.54it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 427.34it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7051482059282371, 'auc': 0.8100082505640318, 'precision': 0.9148264984227129, 'recall': 0.45241809672386896, 'fpr': 0.0421216848673947}
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1760, 3584)
[DEBUG] Returning (1760, 3584) activations
[DEBUG] train activations shape: (1760, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 231.875000]
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.82GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 335.88it/s]
[DEBUG] Final encoded shape: (1760, 16384)
Encoded feature matrix shape: (1760, 16384)
[DEBUG] After encoding. Memory: RAM: 6.86GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 16384), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 6.86GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1760, 16384)
[DEBUG] Memory after feature matrix build: RAM: 6.86GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 6.86GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 6.86GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1760, 3584)
[DEBUG] Returning (1760, 3584) activations
[DEBUG] train activations shape: (1760, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_max
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.70GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 336.47it/s]
[DEBUG] Final encoded shape: (1760, 16384)
Encoded feature matrix shape: (1760, 16384)
[DEBUG] After encoding. Memory: RAM: 9.70GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 16384), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 9.70GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1760, 16384)
[DEBUG] Memory after feature matrix build: RAM: 9.70GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 9.70GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 9.70GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-256.250000, 90.750000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 411.41it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 382.80it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 394.05it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8414179104477612, 'auc': 0.9615866562708844, 'precision': 0.9740932642487047, 'recall': 0.7014925373134329, 'fpr': 0.018656716417910446}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-118.875000, 237.500000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 414.74it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 387.57it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 385.97it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6585820895522388, 'auc': 0.9934423034083315, 'precision': 1.0, 'recall': 0.31716417910447764, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 411.97it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 321.43it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 411.57it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8414179104477612, 'auc': 0.9615866562708844, 'precision': 0.9740932642487047, 'recall': 0.7014925373134329, 'fpr': 0.018656716417910446}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-271.250000, 76.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 442.06it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 436.61it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 411.47it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.624804992199688, 'auc': 0.7099963249700034, 'precision': 0.6990049751243781, 'recall': 0.43837753510140404, 'fpr': 0.18876755070202808}
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 410.64it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 385.51it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 378.72it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6585820895522388, 'auc': 0.9934423034083315, 'precision': 1.0, 'recall': 0.31716417910447764, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1371.000000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 442.86it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 432.51it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 405.91it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5429017160686428, 'auc': 0.9039259542300568, 'precision': 1.0, 'recall': 0.08580343213728549, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1760, 3584)
[DEBUG] Returning (1760, 3584) activations
[DEBUG] train activations shape: (1760, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.82GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 334.95it/s]
[DEBUG] Final encoded shape: (1760, 16384)
Encoded feature matrix shape: (1760, 16384)
[DEBUG] After encoding. Memory: RAM: 6.86GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 16384), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 6.86GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1760, 16384)
[DEBUG] Memory after feature matrix build: RAM: 6.86GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 6.86GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 6.86GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1760, 3584)
[DEBUG] Returning (1760, 3584) activations
[DEBUG] train activations shape: (1760, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 231.875000]
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.70GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 335.69it/s]
[DEBUG] Final encoded shape: (1760, 16384)
Encoded feature matrix shape: (1760, 16384)
[DEBUG] After encoding. Memory: RAM: 9.70GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 16384), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 9.70GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1760, 16384)
[DEBUG] Memory after feature matrix build: RAM: 9.70GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 9.70GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 9.70GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-157.625000, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 414.54it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 392.84it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 376.20it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.75, 'auc': 0.992272777901537, 'precision': 1.0, 'recall': 0.5, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-210.750000, 80.687500]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 418.13it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 382.20it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 386.46it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8208955223880597, 'auc': 0.9399922031632881, 'precision': 0.9479166666666666, 'recall': 0.6791044776119403, 'fpr': 0.03731343283582089}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 411.53it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 392.25it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 416.80it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.75, 'auc': 0.992272777901537, 'precision': 1.0, 'recall': 0.5, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-163.875000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 443.49it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 416.16it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 383.20it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.514820592823713, 'auc': 0.8343924396601448, 'precision': 0.7878787878787878, 'recall': 0.0405616224648986, 'fpr': 0.0109204368174727}
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 416.35it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 407.49it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 367.79it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8208955223880597, 'auc': 0.9399922031632881, 'precision': 0.9479166666666666, 'recall': 0.6791044776119403, 'fpr': 0.03731343283582089}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1371.000000, 229.375000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 441.00it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 427.36it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 426.92it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6123244929797191, 'auc': 0.6641801397484917, 'precision': 0.6188118811881188, 'recall': 0.5850234009360374, 'fpr': 0.36037441497659906}
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_mean
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.83GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.57it/s]
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1760, 3584)
[DEBUG] Returning (1760, 3584) activations
[DEBUG] train activations shape: (1760, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.70GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 329.97it/s]
[DEBUG] Final encoded shape: (1760, 16384)
Encoded feature matrix shape: (1760, 16384)
[DEBUG] After encoding. Memory: RAM: 9.70GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 16384), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 9.70GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1760, 16384)
[DEBUG] Memory after feature matrix build: RAM: 9.70GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 9.70GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 9.70GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Final encoded shape: (1760, 262144)
Encoded feature matrix shape: (1760, 262144)
[DEBUG] After encoding. Memory: RAM: 8.54GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 262144), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 8.54GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1760, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.54GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.54GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.54GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-119.187500, 237.500000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 416.31it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 385.05it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 393.65it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7276119402985075, 'auc': 0.9907830251726443, 'precision': 1.0, 'recall': 0.4552238805970149, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.63it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.66it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.00it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8097014925373134, 'auc': 0.9514229226999332, 'precision': 0.9940476190476191, 'recall': 0.6231343283582089, 'fpr': 0.0037313432835820895}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 415.81it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 416.93it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 382.52it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7276119402985075, 'auc': 0.9907830251726443, 'precision': 1.0, 'recall': 0.4552238805970149, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1371.000000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 442.44it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 411.67it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 408.03it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5499219968798752, 'auc': 0.8573601602410431, 'precision': 1.0, 'recall': 0.0998439937597504, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.60it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.14it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.11it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8097014925373134, 'auc': 0.9514229226999332, 'precision': 0.9940476190476191, 'recall': 0.6231343283582089, 'fpr': 0.0037313432835820895}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_mean
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.12it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.65it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.60it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.49765990639625585, 'auc': 0.4369635003808888, 'precision': 0.42857142857142855, 'recall': 0.014040561622464899, 'fpr': 0.0187207488299532}
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.70GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.89it/s]
[DEBUG] Final encoded shape: (1760, 262144)
Encoded feature matrix shape: (1760, 262144)
[DEBUG] After encoding. Memory: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 262144), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1760, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_max
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.83GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.56it/s]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Final encoded shape: (1760, 262144)
Encoded feature matrix shape: (1760, 262144)
[DEBUG] After encoding. Memory: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 262144), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1760, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.00it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.67it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.70it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.753731343283582, 'auc': 0.9354394074404099, 'precision': 0.9927536231884058, 'recall': 0.5111940298507462, 'fpr': 0.0037313432835820895}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.59it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.10it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.57it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7817164179104478, 'auc': 0.9936233014034306, 'precision': 1.0, 'recall': 0.5634328358208955, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.04it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.65it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.98it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.753731343283582, 'auc': 0.9354394074404099, 'precision': 0.9927536231884058, 'recall': 0.5111940298507462, 'fpr': 0.0037313432835820895}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.42it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.17it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.03it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5078003120124805, 'auc': 0.393274451726899, 'precision': 0.5595238095238095, 'recall': 0.07332293291731669, 'fpr': 0.057722308892355696}
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.60it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.89it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.02it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7817164179104478, 'auc': 0.9936233014034306, 'precision': 1.0, 'recall': 0.5634328358208955, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_max
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.07it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.62it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.59it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.53198127925117, 'auc': 0.947184221222203, 'precision': 1.0, 'recall': 0.06396255850234009, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.70GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.87it/s]
[DEBUG] Final encoded shape: (1760, 262144)
Encoded feature matrix shape: (1760, 262144)
[DEBUG] After encoding. Memory: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 262144), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1760, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.85GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.57it/s]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Final encoded shape: (1760, 262144)
Encoded feature matrix shape: (1760, 262144)
[DEBUG] After encoding. Memory: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 262144), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1760, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.98it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.72it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6977611940298507, 'auc': 0.9944586767654265, 'precision': 1.0, 'recall': 0.39552238805970147, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.59it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.08it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.84it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8563432835820896, 'auc': 0.9071480285141458, 'precision': 0.9282511210762332, 'recall': 0.7723880597014925, 'fpr': 0.05970149253731343}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.67it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.77it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6977611940298507, 'auc': 0.9944586767654265, 'precision': 1.0, 'recall': 0.39552238805970147, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.43it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.05it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.96it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5179407176287052, 'auc': 0.9131208306054551, 'precision': 1.0, 'recall': 0.0358814352574103, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.64it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.75it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.13it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8563432835820896, 'auc': 0.9071480285141458, 'precision': 0.9282511210762332, 'recall': 0.7723880597014925, 'fpr': 0.05970149253731343}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_last
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.09it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.61it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.56it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7870514820592823, 'auc': 0.8553729668687526, 'precision': 0.7875, 'recall': 0.7862714508580343, 'fpr': 0.2121684867394696}
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.70GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.88it/s]
[DEBUG] Final encoded shape: (1760, 262144)
Encoded feature matrix shape: (1760, 262144)
[DEBUG] After encoding. Memory: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 262144), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1760, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.84GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.57it/s]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Final encoded shape: (1760, 262144)
Encoded feature matrix shape: (1760, 262144)
[DEBUG] After encoding. Memory: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 262144), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1760, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.04it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.72it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.01it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8414179104477612, 'auc': 0.9184116729783917, 'precision': 0.9255813953488372, 'recall': 0.7425373134328358, 'fpr': 0.05970149253731343}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.61it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.88it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.88it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7611940298507462, 'auc': 0.9953079750501225, 'precision': 1.0, 'recall': 0.5223880597014925, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.04it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.75it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.73it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8414179104477612, 'auc': 0.9184116729783917, 'precision': 0.9255813953488372, 'recall': 0.7425373134328358, 'fpr': 0.05970149253731343}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.43it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.13it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.17it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6887675507020281, 'auc': 0.7637539822965773, 'precision': 0.6990131578947368, 'recall': 0.6630265210608425, 'fpr': 0.28549141965678626}
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.63it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.90it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.88it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7611940298507462, 'auc': 0.9953079750501225, 'precision': 1.0, 'recall': 0.5223880597014925, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
Train activations: (1760, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_softmax
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.10it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.62it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.57it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5078003120124805, 'auc': 0.8983160574472899, 'precision': 1.0, 'recall': 0.015600624024960999, 'fpr': 0.0}
  🚀 [BATCH] Training probe: attention
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: full

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1760, 3584)
Input y shape: (1760,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.70GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1760, 3584)
[DEBUG] Using pre-aggregated inputs: (1760, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.88it/s]
[DEBUG] Final encoded shape: (1760, 262144)
Encoded feature matrix shape: (1760, 262144)
[DEBUG] After encoding. Memory: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1760, 262144), y_train=(1760,)
[DEBUG] Memory at start of feature selection: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1760, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1760, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.04it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.96it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.99it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7014925373134329, 'auc': 0.993539763867231, 'precision': 1.0, 'recall': 0.40298507462686567, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 1760 full activations
[DEBUG] Estimated memory requirement: 3.33 GB
[WARNING] Large memory requirement (3.33 GB).
[DEBUG] Max sequence length: 541
[DEBUG] Shape of first activation: (1, 6, 3584)
[DEBUG] Final array shape: (1760, 541, 3584)
[DEBUG] Returning (1760, 541, 3584) activations
[DEBUG] train activations shape: (1760, 541, 3584), dtype: float16
[DEBUG] train activations range: [-1674.000000, 237.000000]
Train activations: (1760, 541, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: attention
  [DEBUG] Creating attention probe
  [DEBUG] Fitting attention probe normally...
Epoch 10/100, Loss: 0.2288
Epoch 20/100, Loss: 0.0848
Epoch 30/100, Loss: 0.0208
Epoch 40/100, Loss: 0.0065
Early stopping at epoch 47
Saved probe to results/spam_gemma_9b/seed_50/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state.npz
Saved training log to results/spam_gemma_9b/seed_50/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state_train_log.json
  - 🔥 Probe state saved to train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.95it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.00it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.55it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7014925373134329, 'auc': 0.993539763867231, 'precision': 1.0, 'recall': 0.40298507462686567, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.40it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.04it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.05it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.514820592823713, 'auc': 0.8832167951304637, 'precision': 1.0, 'recall': 0.029641185647425898, 'fpr': 0.0}
  🚀 [BATCH] Training probe: attention
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: full
Loaded probe from results/spam_gemma_9b/seed_50/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 536 full activations
[DEBUG] Estimated memory requirement: 1.58 GB
[WARNING] Large memory requirement (1.58 GB).
[DEBUG] Max sequence length: 1167
[DEBUG] Shape of first activation: (1, 106, 3584)
[DEBUG] Final array shape: (536, 1167, 3584)
[DEBUG] Returning (536, 1167, 3584) activations
[DEBUG] test activations shape: (536, 1167, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (536, 1167, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating validation on '94_better_spam': Got unsupported ScalarType BFloat16
    [BATCH] Evaluating on 94_better_spam test set
Loaded probe from results/spam_gemma_9b/seed_50/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state.npz
Converted attention probe back to bfloat16 for training
Test activations: (536, 1167, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating test on '94_better_spam': Got unsupported ScalarType BFloat16
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 1760 full activations
[DEBUG] Estimated memory requirement: 3.34 GB
[WARNING] Large memory requirement (3.34 GB).
[DEBUG] Max sequence length: 520
[DEBUG] Shape of first activation: (1, 170, 3584)
[DEBUG] Final array shape: (1760, 520, 3584)
[DEBUG] Returning (1760, 520, 3584) activations
[DEBUG] train activations shape: (1760, 520, 3584), dtype: float16
[DEBUG] train activations range: [-1674.000000, 237.000000]
Train activations: (1760, 520, 3584)
[DEBUG] Pre-fit sample counts — X: 1760, y: 1760
[DEBUG] y_train class distribution: {0: 1750, 1: 10}
  [DEBUG] Creating probe for architecture: attention
  [DEBUG] Creating attention probe
  [DEBUG] Fitting attention probe normally...
Epoch 10/100, Loss: 0.2182
Epoch 20/100, Loss: 0.0880
Epoch 30/100, Loss: 0.0349
Epoch 40/100, Loss: 0.0120
Epoch 50/100, Loss: 0.0086
Early stopping at epoch 56
Saved probe to results/spam_gemma_9b/seed_51/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state.npz
Saved training log to results/spam_gemma_9b/seed_51/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state_train_log.json
  - 🔥 Probe state saved to train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Loaded probe from results/spam_gemma_9b/seed_50/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 5157
[DEBUG] Processing 1282 full activations
[DEBUG] Estimated memory requirement: 0.60 GB
[WARNING] Large memory requirement (0.60 GB).
[DEBUG] Max sequence length: 210
[DEBUG] Shape of first activation: (1, 19, 3584)
[DEBUG] Final array shape: (1282, 210, 3584)
[DEBUG] Returning (1282, 210, 3584) activations
[DEBUG] test activations shape: (1282, 210, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (1282, 210, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating on '87_is_spam': Got unsupported ScalarType BFloat16

=== Group 51/700 — ('94_better_spam', 20, 'resid_post', 42, '{"class_counts": {"0": 1750, "1": 15}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1765, 3584)
[DEBUG] Returning (1765, 3584) activations
[DEBUG] train activations shape: (1765, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_max
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.83GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 334.13it/s]
[DEBUG] Final encoded shape: (1765, 16384)
Encoded feature matrix shape: (1765, 16384)
[DEBUG] After encoding. Memory: RAM: 6.85GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 16384), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 6.85GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1765, 16384)
[DEBUG] Memory after feature matrix build: RAM: 6.85GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 6.85GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 6.85GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Loaded probe from results/spam_gemma_9b/seed_51/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 536 full activations
[DEBUG] Estimated memory requirement: 1.54 GB
[WARNING] Large memory requirement (1.54 GB).
[DEBUG] Max sequence length: 1167
[DEBUG] Shape of first activation: (1, 121, 3584)
[DEBUG] Final array shape: (536, 1167, 3584)
[DEBUG] Returning (536, 1167, 3584) activations
[DEBUG] test activations shape: (536, 1167, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.500000]
Test activations: (536, 1167, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating validation on '94_better_spam': Got unsupported ScalarType BFloat16
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-1337.000000, 237.500000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 415.81it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 389.19it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 387.04it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7910447761194029, 'auc': 0.9928436177322343, 'precision': 1.0, 'recall': 0.582089552238806, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Loaded probe from results/spam_gemma_9b/seed_51/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state.npz
Converted attention probe back to bfloat16 for training
Test activations: (536, 1167, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating test on '94_better_spam': Got unsupported ScalarType BFloat16
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 412.42it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 383.74it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 396.21it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7910447761194029, 'auc': 0.9928436177322343, 'precision': 1.0, 'recall': 0.582089552238806, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-150.125000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 444.36it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 421.69it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 437.43it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6287051482059283, 'auc': 0.9446238691981377, 'precision': 1.0, 'recall': 0.2574102964118565, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
Loaded probe from results/spam_gemma_9b/seed_51/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_10_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 5157
[DEBUG] Processing 1282 full activations
[DEBUG] Estimated memory requirement: 0.60 GB
[WARNING] Large memory requirement (0.60 GB).
[DEBUG] Max sequence length: 99
[DEBUG] Shape of first activation: (1, 38, 3584)
[DEBUG] Final array shape: (1282, 99, 3584)
[DEBUG] Returning (1282, 99, 3584) activations
[DEBUG] test activations shape: (1282, 99, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (1282, 99, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating on '87_is_spam': Got unsupported ScalarType BFloat16

=== Group 52/700 — ('94_better_spam', 20, 'resid_post', 43, '{"class_counts": {"0": 1750, "1": 15}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1765, 3584)
[DEBUG] Returning (1765, 3584) activations
[DEBUG] train activations shape: (1765, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 231.875000]
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.85GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 337.62it/s]
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1765, 3584)
[DEBUG] Returning (1765, 3584) activations
[DEBUG] train activations shape: (1765, 3584), dtype: float16
[DEBUG] train activations range: [-1067.000000, 73.875000]
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_mean
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.68GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 329.03it/s]
[DEBUG] Final encoded shape: (1765, 16384)
Encoded feature matrix shape: (1765, 16384)
[DEBUG] After encoding. Memory: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 16384), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1765, 16384)
[DEBUG] Memory after feature matrix build: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Final encoded shape: (1765, 16384)
Encoded feature matrix shape: (1765, 16384)
[DEBUG] After encoding. Memory: RAM: 9.68GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 16384), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 9.68GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1765, 16384)
[DEBUG] Memory after feature matrix build: RAM: 9.68GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 9.68GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 9.68GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-1337.000000, 219.750000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 410.28it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 392.25it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 423.11it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9011194029850746, 'auc': 0.9705947872577413, 'precision': 0.9820627802690582, 'recall': 0.8171641791044776, 'fpr': 0.014925373134328358}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-1506.000000, 112.125000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 414.66it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 404.82it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 422.69it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8805970149253731, 'auc': 0.9728920695032302, 'precision': 1.0, 'recall': 0.7611940298507462, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 413.88it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 412.50it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 385.36it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9011194029850746, 'auc': 0.9705947872577413, 'precision': 0.9820627802690582, 'recall': 0.8171641791044776, 'fpr': 0.014925373134328358}
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-259.000000, 76.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 412.54it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 415.73it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 426.42it/s]
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 418.86it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8805970149253731, 'auc': 0.9728920695032302, 'precision': 1.0, 'recall': 0.7611940298507462, 'fpr': 0.0}
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 247.62it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 425.99it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5772230889235569, 'auc': 0.6311328584188609, 'precision': 0.6348773841961853, 'recall': 0.36349453978159124, 'fpr': 0.20904836193447737}
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1063.000000, 70.687500]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 443.07it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 408.84it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 406.98it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7051482059282371, 'auc': 0.7939354703673327, 'precision': 0.9201277955271565, 'recall': 0.44929797191887677, 'fpr': 0.0390015600624025}
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1765, 3584)
[DEBUG] Returning (1765, 3584) activations
[DEBUG] train activations shape: (1765, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.88GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 335.75it/s]
[DEBUG] Final encoded shape: (1765, 16384)
Encoded feature matrix shape: (1765, 16384)
[DEBUG] After encoding. Memory: RAM: 6.94GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 16384), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 6.94GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1765, 16384)
[DEBUG] Memory after feature matrix build: RAM: 6.94GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 6.94GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 6.94GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1765, 3584)
[DEBUG] Returning (1765, 3584) activations
[DEBUG] train activations shape: (1765, 3584), dtype: float16
[DEBUG] train activations range: [-162.375000, 237.000000]
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_max
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.68GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 333.04it/s]
[DEBUG] Final encoded shape: (1765, 16384)
Encoded feature matrix shape: (1765, 16384)
[DEBUG] After encoding. Memory: RAM: 9.68GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 16384), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 9.68GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1765, 16384)
[DEBUG] Memory after feature matrix build: RAM: 9.68GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 9.68GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 9.68GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-1337.000000, 237.500000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 416.18it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 408.88it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 390.75it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8059701492537313, 'auc': 0.9914095566941412, 'precision': 1.0, 'recall': 0.6119402985074627, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-1337.000000, 237.500000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 419.81it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 386.82it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 415.69it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8339552238805971, 'auc': 0.9937068389396302, 'precision': 1.0, 'recall': 0.667910447761194, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 410.20it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 390.90it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 382.69it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8059701492537313, 'auc': 0.9914095566941412, 'precision': 1.0, 'recall': 0.6119402985074627, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-150.125000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 446.23it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 302.09it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 409.62it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6146645865834633, 'auc': 0.932401352216335, 'precision': 0.9867549668874173, 'recall': 0.23244929797191888, 'fpr': 0.0031201248049922}
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 417.84it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 407.02it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 380.16it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8339552238805971, 'auc': 0.9937068389396302, 'precision': 1.0, 'recall': 0.667910447761194, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-147.875000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 441.48it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 417.28it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 404.43it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7277691107644306, 'auc': 0.9513289735957612, 'precision': 0.9932432432432432, 'recall': 0.45865834633385333, 'fpr': 0.0031201248049922}
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1765, 3584)
[DEBUG] Returning (1765, 3584) activations
[DEBUG] train activations shape: (1765, 3584), dtype: float16
[DEBUG] train activations range: [-1526.000000, 117.875000]
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_mean
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1765, 3584)
[DEBUG] Returning (1765, 3584) activations
[DEBUG] train activations shape: (1765, 3584), dtype: float16
[DEBUG] train activations range: [-256.250000, 92.687500]
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.68GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 336.30it/s]
[DEBUG] Final encoded shape: (1765, 16384)
Encoded feature matrix shape: (1765, 16384)
[DEBUG] After encoding. Memory: RAM: 9.68GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 16384), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 9.68GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1765, 16384)
[DEBUG] Memory after feature matrix build: RAM: 9.68GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 9.68GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 9.68GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.87GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.57it/s]
[DEBUG] Final encoded shape: (1765, 262144)
Encoded feature matrix shape: (1765, 262144)
[DEBUG] After encoding. Memory: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 262144), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1765, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-1337.000000, 219.750000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 413.07it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 380.37it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 389.99it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8992537313432836, 'auc': 0.9735325239474271, 'precision': 0.9776785714285714, 'recall': 0.8171641791044776, 'fpr': 0.018656716417910446}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-1506.000000, 112.125000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.61it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.79it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.77it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8078358208955224, 'auc': 0.9489446424593452, 'precision': 1.0, 'recall': 0.6156716417910447, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 413.76it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 384.62it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 391.11it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8992537313432836, 'auc': 0.9735325239474271, 'precision': 0.9776785714285714, 'recall': 0.8171641791044776, 'fpr': 0.018656716417910446}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-252.250000, 76.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 445.02it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 423.26it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 406.80it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.47737909516380655, 'auc': 0.4680235883382293, 'precision': 0.4774494556765163, 'recall': 0.4789391575663027, 'fpr': 0.5241809672386896}
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1765, 3584)
[DEBUG] Returning (1765, 3584) activations
[DEBUG] train activations shape: (1765, 3584), dtype: float16
[DEBUG] train activations range: [-162.375000, 237.000000]
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.69GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 335.73it/s]
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.74it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.11it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.90it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8078358208955224, 'auc': 0.9489446424593452, 'precision': 1.0, 'recall': 0.6156716417910447, 'fpr': 0.0}
[DEBUG] Final encoded shape: (1765, 16384)
Encoded feature matrix shape: (1765, 16384)
[DEBUG] After encoding. Memory: RAM: 9.69GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 16384), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 9.69GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1765, 16384)
[DEBUG] Memory after feature matrix build: RAM: 9.69GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 9.69GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 9.69GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1063.000000, 70.687500]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.09it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.71it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.57it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5741029641185648, 'auc': 0.7149563985679552, 'precision': 0.9203539823008849, 'recall': 0.1622464898595944, 'fpr': 0.014040561622464899}
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-1337.000000, 237.500000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 417.51it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 384.20it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 405.76it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8544776119402985, 'auc': 0.99374860770773, 'precision': 1.0, 'recall': 0.7089552238805971, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_max
  [DEBUG] Fitting SAE probe normally...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.87GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.54it/s]
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 412.91it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 351.43it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 379.95it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8544776119402985, 'auc': 0.99374860770773, 'precision': 1.0, 'recall': 0.7089552238805971, 'fpr': 0.0}
[DEBUG] Final encoded shape: (1765, 262144)
Encoded feature matrix shape: (1765, 262144)
[DEBUG] After encoding. Memory: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 262144), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1765, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-147.875000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 439.33it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 401.06it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 425.47it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6856474258970359, 'auc': 0.9374028003241813, 'precision': 0.976, 'recall': 0.38065522620904835, 'fpr': 0.0093603744149766}
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_mean
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.63it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.93it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.93it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8283582089552238, 'auc': 0.9929828469592338, 'precision': 1.0, 'recall': 0.6567164179104478, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.69GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.86it/s]
[DEBUG] Final encoded shape: (1765, 262144)
Encoded feature matrix shape: (1765, 262144)
[DEBUG] After encoding. Memory: RAM: 11.41GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 262144), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 11.41GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1765, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.41GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.41GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.41GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.61it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.18it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.11it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8283582089552238, 'auc': 0.9929828469592338, 'precision': 1.0, 'recall': 0.6567164179104478, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.11it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.63it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.65it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6692667706708268, 'auc': 0.9477829347183249, 'precision': 0.9954337899543378, 'recall': 0.34009360374414976, 'fpr': 0.0015600624024961}
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.04it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.69it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.96it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8451492537313433, 'auc': 0.9665292938293606, 'precision': 0.9894179894179894, 'recall': 0.6977611940298507, 'fpr': 0.007462686567164179}
    [BATCH] Evaluating on 94_better_spam test set
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_last
  [DEBUG] Fitting SAE probe normally...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.87GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.55it/s]
[DEBUG] Final encoded shape: (1765, 262144)
Encoded feature matrix shape: (1765, 262144)
[DEBUG] After encoding. Memory: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 262144), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1765, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.02it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.79it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.05it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8451492537313433, 'auc': 0.9665292938293606, 'precision': 0.9894179894179894, 'recall': 0.6977611940298507, 'fpr': 0.007462686567164179}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.41it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.22it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.16it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6294851794071763, 'auc': 0.7789554639907905, 'precision': 0.9662921348314607, 'recall': 0.26833073322932915, 'fpr': 0.0093603744149766}
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.60it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.09it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.65it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8899253731343284, 'auc': 0.9406465805301848, 'precision': 0.9644444444444444, 'recall': 0.8097014925373134, 'fpr': 0.029850746268656716}
    [BATCH] Evaluating on 94_better_spam test set
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_max
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.68GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.87it/s]
[DEBUG] Final encoded shape: (1765, 262144)
Encoded feature matrix shape: (1765, 262144)
[DEBUG] After encoding. Memory: RAM: 11.40GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 262144), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 11.40GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1765, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.40GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.41GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.41GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.63it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.11it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8899253731343284, 'auc': 0.9406465805301848, 'precision': 0.9644444444444444, 'recall': 0.8097014925373134, 'fpr': 0.029850746268656716}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.09it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.62it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.61it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7402496099843994, 'auc': 0.8252389377946412, 'precision': 0.8142857142857143, 'recall': 0.6224648985959438, 'fpr': 0.1419656786271451}
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.05it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.05it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.73it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8582089552238806, 'auc': 0.9911171753174426, 'precision': 1.0, 'recall': 0.7164179104477612, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_softmax
  [DEBUG] Fitting SAE probe normally...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.87GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 20.03it/s]
[DEBUG] Final encoded shape: (1765, 262144)
Encoded feature matrix shape: (1765, 262144)
[DEBUG] After encoding. Memory: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 262144), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1765, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.07it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.69it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8582089552238806, 'auc': 0.9911171753174426, 'precision': 1.0, 'recall': 0.7164179104477612, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.41it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.04it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.02it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7137285491419657, 'auc': 0.9621715289828442, 'precision': 1.0, 'recall': 0.42745709828393136, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.61it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.07it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.79it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8115671641791045, 'auc': 0.993539763867231, 'precision': 1.0, 'recall': 0.6231343283582089, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.68GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.86it/s]
[DEBUG] Final encoded shape: (1765, 262144)
Encoded feature matrix shape: (1765, 262144)
[DEBUG] After encoding. Memory: RAM: 11.40GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 262144), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 11.40GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1765, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.40GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.41GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.41GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.65it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.14it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.05it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8115671641791045, 'auc': 0.993539763867231, 'precision': 1.0, 'recall': 0.6231343283582089, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.12it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.62it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.56it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5865834633385335, 'auc': 0.9280302569357064, 'precision': 1.0, 'recall': 0.1731669266770671, 'fpr': 0.0}
  🚀 [BATCH] Training probe: attention
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: full
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.68it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.76it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9458955223880597, 'auc': 0.9757880374248162, 'precision': 0.9649805447470817, 'recall': 0.9253731343283582, 'fpr': 0.033582089552238806}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.04it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.79it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.78it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9458955223880597, 'auc': 0.9757880374248162, 'precision': 0.9649805447470817, 'recall': 0.9253731343283582, 'fpr': 0.033582089552238806}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.43it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.27it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.12it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6029641185647426, 'auc': 0.6294888300992257, 'precision': 0.6, 'recall': 0.6177847113884556, 'fpr': 0.4118564742589704}
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 1765 full activations
[DEBUG] Estimated memory requirement: 3.34 GB
[WARNING] Large memory requirement (3.34 GB).
[DEBUG] Max sequence length: 520
[DEBUG] Shape of first activation: (1, 57, 3584)
[DEBUG] Final array shape: (1765, 520, 3584)
[DEBUG] Returning (1765, 520, 3584) activations
[DEBUG] train activations shape: (1765, 520, 3584), dtype: float16
[DEBUG] train activations range: [-1674.000000, 237.000000]
Train activations: (1765, 520, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: attention
  [DEBUG] Creating attention probe
  [DEBUG] Fitting attention probe normally...
Epoch 10/100, Loss: 0.1898
Epoch 20/100, Loss: 0.0753
Epoch 30/100, Loss: 0.0364
Epoch 40/100, Loss: 0.0190
Epoch 50/100, Loss: 0.0142
Epoch 60/100, Loss: 0.0076
Early stopping at epoch 64
Saved probe to results/spam_gemma_9b/seed_42/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_15_state.npz
Saved training log to results/spam_gemma_9b/seed_42/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_15_state_train_log.json
  - 🔥 Probe state saved to train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.68GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.88it/s]
[DEBUG] Final encoded shape: (1765, 262144)
Encoded feature matrix shape: (1765, 262144)
[DEBUG] After encoding. Memory: RAM: 11.40GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 262144), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 11.40GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1765, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.40GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.41GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.41GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.04it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.01it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.98it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8414179104477612, 'auc': 0.9917715526843395, 'precision': 0.9945945945945946, 'recall': 0.6865671641791045, 'fpr': 0.0037313432835820895}
    [BATCH] Evaluating on 94_better_spam test set
Loaded probe from results/spam_gemma_9b/seed_42/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_15_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 536 full activations
[DEBUG] Estimated memory requirement: 1.58 GB
[WARNING] Large memory requirement (1.58 GB).
[DEBUG] Max sequence length: 939
[DEBUG] Shape of first activation: (1, 151, 3584)
[DEBUG] Final array shape: (536, 939, 3584)
[DEBUG] Returning (536, 939, 3584) activations
[DEBUG] test activations shape: (536, 939, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.500000]
Test activations: (536, 939, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating validation on '94_better_spam': Got unsupported ScalarType BFloat16
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Loaded probe from results/spam_gemma_9b/seed_42/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_15_state.npz
Converted attention probe back to bfloat16 for training
Test activations: (536, 939, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating test on '94_better_spam': Got unsupported ScalarType BFloat16
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.06it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.98it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.95it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8414179104477612, 'auc': 0.9917715526843395, 'precision': 0.9945945945945946, 'recall': 0.6865671641791045, 'fpr': 0.0037313432835820895}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
Loaded probe from results/spam_gemma_9b/seed_42/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_15_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 5157
[DEBUG] Processing 1282 full activations
[DEBUG] Estimated memory requirement: 0.60 GB
[WARNING] Large memory requirement (0.60 GB).
[DEBUG] Max sequence length: 210
[DEBUG] Shape of first activation: (1, 54, 3584)
[DEBUG] Final array shape: (1282, 210, 3584)
[DEBUG] Returning (1282, 210, 3584) activations
[DEBUG] test activations shape: (1282, 210, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (1282, 210, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating on '87_is_spam': Got unsupported ScalarType BFloat16

=== Group 53/700 — ('94_better_spam', 20, 'resid_post', 44, '{"class_counts": {"0": 1750, "1": 15}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.40it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.05it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.02it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6965678627145085, 'auc': 0.9447650292907193, 'precision': 0.9960629921259843, 'recall': 0.39469578783151327, 'fpr': 0.0015600624024961}
  🚀 [BATCH] Training probe: attention
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: full
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-593.000000, 51.843750]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 410.28it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 381.27it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 421.16it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8451492537313433, 'auc': 0.9668355981287592, 'precision': 1.0, 'recall': 0.6902985074626866, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 415.20it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 409.96it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 417.93it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8451492537313433, 'auc': 0.9668355981287592, 'precision': 1.0, 'recall': 0.6902985074626866, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1053.000000, 69.625000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 442.48it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 417.09it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 421.07it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6872074882995319, 'auc': 0.8849009810626435, 'precision': 0.9026845637583892, 'recall': 0.41965678627145087, 'fpr': 0.0452418096723869}
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1765, 3584)
[DEBUG] Returning (1765, 3584) activations
[DEBUG] train activations shape: (1765, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_max
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.86GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 337.28it/s]
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 1765 full activations
[DEBUG] Estimated memory requirement: 3.23 GB
[WARNING] Large memory requirement (3.23 GB).
[DEBUG] Max sequence length: 520
[DEBUG] Shape of first activation: (1, 23, 3584)
[DEBUG] Final array shape: (1765, 520, 3584)
[DEBUG] Returning (1765, 520, 3584) activations
[DEBUG] train activations shape: (1765, 520, 3584), dtype: float16
[DEBUG] train activations range: [-1674.000000, 237.000000]
Train activations: (1765, 520, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: attention
  [DEBUG] Creating attention probe
  [DEBUG] Fitting attention probe normally...
[DEBUG] Final encoded shape: (1765, 16384)
Encoded feature matrix shape: (1765, 16384)
[DEBUG] After encoding. Memory: RAM: 6.84GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 16384), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 6.84GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1765, 16384)
[DEBUG] Memory after feature matrix build: RAM: 6.84GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 6.84GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 6.84GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Epoch 10/100, Loss: 0.2216
Epoch 20/100, Loss: 0.0790
Epoch 30/100, Loss: 0.0267
Epoch 40/100, Loss: 0.0094
Epoch 50/100, Loss: 0.0051
Early stopping at epoch 57
Saved probe to results/spam_gemma_9b/seed_43/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_15_state.npz
Saved training log to results/spam_gemma_9b/seed_43/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_15_state_train_log.json
  - 🔥 Probe state saved to train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-109.562500, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 407.41it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 408.76it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 375.93it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8115671641791045, 'auc': 0.9934979950991312, 'precision': 1.0, 'recall': 0.6231343283582089, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Loaded probe from results/spam_gemma_9b/seed_43/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_15_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 536 full activations
[DEBUG] Estimated memory requirement: 1.61 GB
[WARNING] Large memory requirement (1.61 GB).
[DEBUG] Max sequence length: 939
[DEBUG] Shape of first activation: (1, 62, 3584)
[DEBUG] Final array shape: (536, 939, 3584)
[DEBUG] Returning (536, 939, 3584) activations
[DEBUG] test activations shape: (536, 939, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.500000]
Test activations: (536, 939, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating validation on '94_better_spam': Got unsupported ScalarType BFloat16
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 410.72it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 387.07it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 424.87it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8115671641791045, 'auc': 0.9934979950991312, 'precision': 1.0, 'recall': 0.6231343283582089, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-143.875000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 445.97it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 413.88it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 413.01it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5382215288611545, 'auc': 0.8329638021714316, 'precision': 0.9803921568627451, 'recall': 0.078003120124805, 'fpr': 0.0015600624024961}
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
Loaded probe from results/spam_gemma_9b/seed_43/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_15_state.npz
Converted attention probe back to bfloat16 for training
Test activations: (536, 939, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating test on '94_better_spam': Got unsupported ScalarType BFloat16
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1765, 3584)
[DEBUG] Returning (1765, 3584) activations
[DEBUG] train activations shape: (1765, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 231.875000]
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.85GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 304.21it/s]
[DEBUG] Final encoded shape: (1765, 16384)
Encoded feature matrix shape: (1765, 16384)
[DEBUG] After encoding. Memory: RAM: 6.94GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 16384), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 6.94GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1765, 16384)
[DEBUG] Memory after feature matrix build: RAM: 6.94GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 6.94GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 6.94GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Loaded probe from results/spam_gemma_9b/seed_43/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_15_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 5157
[DEBUG] Processing 1282 full activations
[DEBUG] Estimated memory requirement: 0.60 GB
[WARNING] Large memory requirement (0.60 GB).
[DEBUG] Max sequence length: 119
[DEBUG] Shape of first activation: (1, 8, 3584)
[DEBUG] Final array shape: (1282, 119, 3584)
[DEBUG] Returning (1282, 119, 3584) activations
[DEBUG] test activations shape: (1282, 119, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (1282, 119, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating on '87_is_spam': Got unsupported ScalarType BFloat16

=== Group 54/700 — ('94_better_spam', 20, 'resid_post', 45, '{"class_counts": {"0": 1750, "1": 15}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-238.250000, 80.812500]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 409.72it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 420.31it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 403.57it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8507462686567164, 'auc': 0.9451436845622633, 'precision': 0.9607843137254902, 'recall': 0.7313432835820896, 'fpr': 0.029850746268656716}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1765, 3584)
[DEBUG] Returning (1765, 3584) activations
[DEBUG] train activations shape: (1765, 3584), dtype: float16
[DEBUG] train activations range: [-1506.000000, 112.125000]
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_mean
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.70GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 337.43it/s]
[DEBUG] Final encoded shape: (1765, 16384)
Encoded feature matrix shape: (1765, 16384)
[DEBUG] After encoding. Memory: RAM: 9.70GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 16384), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 9.70GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1765, 16384)
[DEBUG] Memory after feature matrix build: RAM: 9.70GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 9.70GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 9.70GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 410.56it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 424.05it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 384.94it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8507462686567164, 'auc': 0.9451436845622633, 'precision': 0.9607843137254902, 'recall': 0.7313432835820896, 'fpr': 0.029850746268656716}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-274.000000, 76.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 442.90it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 411.41it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 223.00it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.49375975039001563, 'auc': 0.5044307232507709, 'precision': 0.4940119760479042, 'recall': 0.514820592823713, 'fpr': 0.5273010920436817}
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-677.000000, 50.625000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 418.26it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 385.36it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 382.73it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8638059701492538, 'auc': 0.9289234796168412, 'precision': 0.9802955665024631, 'recall': 0.7425373134328358, 'fpr': 0.014925373134328358}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1765, 3584)
[DEBUG] Returning (1765, 3584) activations
[DEBUG] train activations shape: (1765, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_softmax
  [DEBUG] Fitting SAE probe normally...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.85GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 336.85it/s]
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 413.88it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 383.57it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 377.83it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8638059701492538, 'auc': 0.9289234796168412, 'precision': 0.9802955665024631, 'recall': 0.7425373134328358, 'fpr': 0.014925373134328358}
[DEBUG] Final encoded shape: (1765, 16384)
Encoded feature matrix shape: (1765, 16384)
[DEBUG] After encoding. Memory: RAM: 6.94GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 16384), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 6.94GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1765, 16384)
[DEBUG] Memory after feature matrix build: RAM: 6.94GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 6.94GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 6.94GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1505.000000, 110.875000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 442.90it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 406.58it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 405.74it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7527301092043682, 'auc': 0.8603439925428531, 'precision': 0.8894230769230769, 'recall': 0.5772230889235569, 'fpr': 0.0717628705148206}
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-109.625000, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 412.83it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 388.87it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 383.71it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7854477611940298, 'auc': 0.990629873022945, 'precision': 1.0, 'recall': 0.5708955223880597, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1765, 3584)
[DEBUG] Returning (1765, 3584) activations
[DEBUG] train activations shape: (1765, 3584), dtype: float16
[DEBUG] train activations range: [-1337.000000, 237.500000]
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_max
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.70GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 336.95it/s]
[DEBUG] Final encoded shape: (1765, 16384)
Encoded feature matrix shape: (1765, 16384)
[DEBUG] After encoding. Memory: RAM: 9.70GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 16384), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 9.70GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1765, 16384)
[DEBUG] Memory after feature matrix build: RAM: 9.70GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 9.70GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 9.70GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 413.76it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 420.14it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 427.90it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7854477611940298, 'auc': 0.990629873022945, 'precision': 1.0, 'recall': 0.5708955223880597, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-143.875000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 439.10it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 412.95it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 409.86it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5031201248049922, 'auc': 0.7389779522538156, 'precision': 0.6, 'recall': 0.0187207488299532, 'fpr': 0.0124804992199688}
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-120.437500, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 418.05it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 386.96it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 389.73it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8115671641791045, 'auc': 0.9928853865003342, 'precision': 1.0, 'recall': 0.6231343283582089, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1765, 3584)
[DEBUG] Returning (1765, 3584) activations
[DEBUG] train activations shape: (1765, 3584), dtype: float16
[DEBUG] train activations range: [-1526.000000, 117.875000]
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_mean
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 418.05it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 387.25it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 424.48it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8115671641791045, 'auc': 0.9928853865003342, 'precision': 1.0, 'recall': 0.6231343283582089, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1336.000000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 443.84it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 433.59it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 406.11it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5670826833073322, 'auc': 0.9268693368639582, 'precision': 0.9886363636363636, 'recall': 0.1357254290171607, 'fpr': 0.0015600624024961}
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.87GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.53it/s]
[DEBUG] Final encoded shape: (1765, 262144)
Encoded feature matrix shape: (1765, 262144)
[DEBUG] After encoding. Memory: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 262144), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1765, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1765, 3584)
[DEBUG] Returning (1765, 3584) activations
[DEBUG] train activations shape: (1765, 3584), dtype: float16
[DEBUG] train activations range: [-1337.000000, 219.750000]
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.70GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 335.05it/s]
[DEBUG] Final encoded shape: (1765, 16384)
Encoded feature matrix shape: (1765, 16384)
[DEBUG] After encoding. Memory: RAM: 9.70GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 16384), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 9.70GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1765, 16384)
[DEBUG] Memory after feature matrix build: RAM: 9.70GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 9.70GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 9.70GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-227.500000, 81.187500]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.61it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.87it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s][DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.41it/s]
Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 364.69it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 187.53it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 368.34it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8115671641791045, 'auc': 0.9507824682557362, 'precision': 0.9883040935672515, 'recall': 0.6305970149253731, 'fpr': 0.007462686567164179}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8899253731343284, 'auc': 0.9596513700155936, 'precision': 0.960352422907489, 'recall': 0.8134328358208955, 'fpr': 0.033582089552238806}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 393.79it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 404.11it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 408.84it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8899253731343284, 'auc': 0.9596513700155936, 'precision': 0.960352422907489, 'recall': 0.8134328358208955, 'fpr': 0.033582089552238806}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1336.000000, 221.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 430.34it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 422.62it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 424.74it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6318252730109204, 'auc': 0.733557891457624, 'precision': 0.7055961070559611, 'recall': 0.45241809672386896, 'fpr': 0.18876755070202808}
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.62it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.76it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.75it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8115671641791045, 'auc': 0.9507824682557362, 'precision': 0.9883040935672515, 'recall': 0.6305970149253731, 'fpr': 0.007462686567164179}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.08it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.72it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.57it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5179407176287052, 'auc': 0.5285873525424636, 'precision': 0.5506607929515418, 'recall': 0.19500780031201248, 'fpr': 0.15912636505460218}
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1765, 3584)
[DEBUG] Returning (1765, 3584) activations
[DEBUG] train activations shape: (1765, 3584), dtype: float16
[DEBUG] train activations range: [-1337.000000, 237.500000]
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.71GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 335.56it/s]
[DEBUG] Final encoded shape: (1765, 16384)
Encoded feature matrix shape: (1765, 16384)
[DEBUG] After encoding. Memory: RAM: 9.71GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 16384), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 9.71GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1765, 16384)
[DEBUG] Memory after feature matrix build: RAM: 9.71GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 9.71GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 9.71GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_max
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-120.625000, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 389.73it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 401.02it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 390.79it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8022388059701493, 'auc': 0.9896691913566495, 'precision': 1.0, 'recall': 0.6044776119402985, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.87GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.53it/s]
[DEBUG] Final encoded shape: (1765, 262144)
Encoded feature matrix shape: (1765, 262144)
[DEBUG] After encoding. Memory: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 262144), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1765, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 390.79it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 403.49it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 392.06it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8022388059701493, 'auc': 0.9896691913566495, 'precision': 1.0, 'recall': 0.6044776119402985, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1336.000000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 431.22it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 408.30it/s]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 421.83it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5881435257410297, 'auc': 0.9133788128436213, 'precision': 0.991304347826087, 'recall': 0.17784711388455537, 'fpr': 0.0015600624024961}
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.63it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.90it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.72it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8339552238805971, 'auc': 0.9963800400980174, 'precision': 1.0, 'recall': 0.667910447761194, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_mean
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.71GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.89it/s]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.76it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s][DEBUG] Final encoded shape: (1765, 262144)
Encoded feature matrix shape: (1765, 262144)
[DEBUG] After encoding. Memory: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 262144), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1765, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.42it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.53it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8339552238805971, 'auc': 0.9963800400980174, 'precision': 1.0, 'recall': 0.667910447761194, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.09it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.58it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.59it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5522620904836193, 'auc': 0.8646031332672963, 'precision': 0.958904109589041, 'recall': 0.10920436817472699, 'fpr': 0.0046801872074883}
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.05it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.82it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.90it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8227611940298507, 'auc': 0.9236606148362665, 'precision': 0.9726775956284153, 'recall': 0.664179104477612, 'fpr': 0.018656716417910446}
    [BATCH] Evaluating on 94_better_spam test set
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.87GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.54it/s]
[DEBUG] Final encoded shape: (1765, 262144)
Encoded feature matrix shape: (1765, 262144)
[DEBUG] After encoding. Memory: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 262144), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1765, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.01it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.84it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.64it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8227611940298507, 'auc': 0.9236606148362665, 'precision': 0.9726775956284153, 'recall': 0.664179104477612, 'fpr': 0.018656716417910446}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.42it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.22it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.07it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5600624024960998, 'auc': 0.6364981588343097, 'precision': 0.8888888888888888, 'recall': 0.1372854914196568, 'fpr': 0.0171606864274571}
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.62it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.90it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.08it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8917910447761194, 'auc': 0.9587742258854979, 'precision': 0.9233870967741935, 'recall': 0.8544776119402985, 'fpr': 0.0708955223880597}
    [BATCH] Evaluating on 94_better_spam test set
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_max
  [DEBUG] Fitting SAE probe normally...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.71GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.87it/s]
[DEBUG] Final encoded shape: (1765, 262144)
Encoded feature matrix shape: (1765, 262144)
[DEBUG] After encoding. Memory: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 262144), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1765, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.69it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.22it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.48it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8917910447761194, 'auc': 0.9587742258854979, 'precision': 0.9233870967741935, 'recall': 0.8544776119402985, 'fpr': 0.0708955223880597}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.08it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.61it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.60it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6216848673946958, 'auc': 0.6614859290159438, 'precision': 0.6117478510028653, 'recall': 0.6661466458658346, 'fpr': 0.42277691107644305}
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.02it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.69it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.00it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8414179104477612, 'auc': 0.9957674314992204, 'precision': 1.0, 'recall': 0.6828358208955224, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.87GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.55it/s]
[DEBUG] Final encoded shape: (1765, 262144)
Encoded feature matrix shape: (1765, 262144)
[DEBUG] After encoding. Memory: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 262144), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1765, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.01it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.88it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.76it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8414179104477612, 'auc': 0.9957674314992204, 'precision': 1.0, 'recall': 0.6828358208955224, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.40it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.06it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.05it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5717628705148206, 'auc': 0.9095504537810218, 'precision': 0.9693877551020408, 'recall': 0.1482059282371295, 'fpr': 0.0046801872074883}
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.63it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.82it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.83it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7966417910447762, 'auc': 0.9917993985297394, 'precision': 1.0, 'recall': 0.5932835820895522, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_last
  [DEBUG] Fitting SAE probe normally...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.72GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.87it/s]
[DEBUG] Final encoded shape: (1765, 262144)
Encoded feature matrix shape: (1765, 262144)
[DEBUG] After encoding. Memory: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 262144), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1765, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.44GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.72it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.17it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.09it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7966417910447762, 'auc': 0.9917993985297394, 'precision': 1.0, 'recall': 0.5932835820895522, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.08it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.59it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.63it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5296411856474259, 'auc': 0.8265945614423641, 'precision': 0.975, 'recall': 0.060842433697347896, 'fpr': 0.0015600624024961}
  🚀 [BATCH] Training probe: attention
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: full
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.04it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.96it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.93it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8880597014925373, 'auc': 0.9504761639563376, 'precision': 0.940677966101695, 'recall': 0.8283582089552238, 'fpr': 0.05223880597014925}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.05it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.22it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.86it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8880597014925373, 'auc': 0.9504761639563376, 'precision': 0.940677966101695, 'recall': 0.8283582089552238, 'fpr': 0.05223880597014925}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.43it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.14it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.18it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7932917316692668, 'auc': 0.8641236757114591, 'precision': 0.8081967213114755, 'recall': 0.7691107644305772, 'fpr': 0.18252730109204368}
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.71GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.89it/s]
[DEBUG] Final encoded shape: (1765, 262144)
Encoded feature matrix shape: (1765, 262144)
[DEBUG] After encoding. Memory: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 262144), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1765, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 1765 full activations
[DEBUG] Estimated memory requirement: 3.41 GB
[WARNING] Large memory requirement (3.41 GB).
[DEBUG] Max sequence length: 1167
[DEBUG] Shape of first activation: (1, 267, 3584)
[DEBUG] Final array shape: (1765, 1167, 3584)
[DEBUG] Returning (1765, 1167, 3584) activations
[DEBUG] train activations shape: (1765, 1167, 3584), dtype: float16
[DEBUG] train activations range: [-1674.000000, 237.000000]
Train activations: (1765, 1167, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: attention
  [DEBUG] Creating attention probe
  [DEBUG] Fitting attention probe normally...
Epoch 10/100, Loss: 0.3226
Epoch 20/100, Loss: 0.1552
Epoch 30/100, Loss: 0.0757
Epoch 40/100, Loss: 0.0353
Epoch 50/100, Loss: 0.0211
Epoch 60/100, Loss: 0.0130
Early stopping at epoch 68
Saved probe to results/spam_gemma_9b/seed_44/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_15_state.npz
Saved training log to results/spam_gemma_9b/seed_44/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_15_state_train_log.json
  - 🔥 Probe state saved to train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.02it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.73it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.63it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7779850746268657, 'auc': 0.9914095566941412, 'precision': 1.0, 'recall': 0.5559701492537313, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Loaded probe from results/spam_gemma_9b/seed_44/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_15_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 536 full activations
[DEBUG] Estimated memory requirement: 1.59 GB
[WARNING] Large memory requirement (1.59 GB).
[DEBUG] Max sequence length: 935
[DEBUG] Shape of first activation: (1, 56, 3584)
[DEBUG] Final array shape: (536, 935, 3584)
[DEBUG] Returning (536, 935, 3584) activations
[DEBUG] test activations shape: (536, 935, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (536, 935, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating validation on '94_better_spam': Got unsupported ScalarType BFloat16
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.05it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.99it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.95it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7779850746268657, 'auc': 0.9914095566941412, 'precision': 1.0, 'recall': 0.5559701492537313, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.41it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.07it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.13it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5265210608424337, 'auc': 0.8498640725660229, 'precision': 0.9722222222222222, 'recall': 0.054602184087363496, 'fpr': 0.0015600624024961}
  🚀 [BATCH] Training probe: attention
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: full
Loaded probe from results/spam_gemma_9b/seed_44/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_15_state.npz
Converted attention probe back to bfloat16 for training
Test activations: (536, 935, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating test on '94_better_spam': Got unsupported ScalarType BFloat16
Loaded probe from results/spam_gemma_9b/seed_44/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_15_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 5157
[DEBUG] Processing 1282 full activations
[DEBUG] Estimated memory requirement: 0.60 GB
[WARNING] Large memory requirement (0.60 GB).
[DEBUG] Max sequence length: 163
[DEBUG] Shape of first activation: (1, 45, 3584)
[DEBUG] Final array shape: (1282, 163, 3584)
[DEBUG] Returning (1282, 163, 3584) activations
[DEBUG] test activations shape: (1282, 163, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (1282, 163, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating on '87_is_spam': Got unsupported ScalarType BFloat16

=== Group 55/700 — ('94_better_spam', 20, 'resid_post', 46, '{"class_counts": {"0": 1750, "1": 15}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1765, 3584)
[DEBUG] Returning (1765, 3584) activations
[DEBUG] train activations shape: (1765, 3584), dtype: float16
[DEBUG] train activations range: [-1337.000000, 237.000000]
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_max
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.84GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 337.24it/s]
[DEBUG] Final encoded shape: (1765, 16384)
Encoded feature matrix shape: (1765, 16384)
[DEBUG] After encoding. Memory: RAM: 6.83GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 16384), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 6.83GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1765, 16384)
[DEBUG] Memory after feature matrix build: RAM: 6.83GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 6.83GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 6.83GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-162.375000, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 416.97it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 397.56it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 423.20it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8563432835820896, 'auc': 0.9915487859211405, 'precision': 1.0, 'recall': 0.7126865671641791, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 1765 full activations
[DEBUG] Estimated memory requirement: 3.31 GB
[WARNING] Large memory requirement (3.31 GB).
[DEBUG] Max sequence length: 833
[DEBUG] Shape of first activation: (1, 240, 3584)
[DEBUG] Final array shape: (1765, 833, 3584)
[DEBUG] Returning (1765, 833, 3584) activations
[DEBUG] train activations shape: (1765, 833, 3584), dtype: float16
[DEBUG] train activations range: [-1674.000000, 237.500000]
Train activations: (1765, 833, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: attention
  [DEBUG] Creating attention probe
  [DEBUG] Fitting attention probe normally...
Epoch 10/100, Loss: 0.3131
Epoch 20/100, Loss: 0.1344
Epoch 30/100, Loss: 0.0626
Epoch 40/100, Loss: 0.0305
Epoch 50/100, Loss: 0.0127
Early stopping at epoch 59
Saved probe to results/spam_gemma_9b/seed_45/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_15_state.npz
Saved training log to results/spam_gemma_9b/seed_45/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_15_state_train_log.json
  - 🔥 Probe state saved to train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 416.97it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 388.87it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 415.11it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8563432835820896, 'auc': 0.9915487859211405, 'precision': 1.0, 'recall': 0.7126865671641791, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1306.000000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 438.46it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 413.78it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 410.90it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5912636505460218, 'auc': 0.9378019426549293, 'precision': 0.9915966386554622, 'recall': 0.18408736349453977, 'fpr': 0.0015600624024961}
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1765, 3584)
[DEBUG] Returning (1765, 3584) activations
[DEBUG] train activations shape: (1765, 3584), dtype: float16
[DEBUG] train activations range: [-1337.000000, 219.750000]
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.86GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 337.03it/s]
[DEBUG] Final encoded shape: (1765, 16384)
Encoded feature matrix shape: (1765, 16384)
[DEBUG] After encoding. Memory: RAM: 6.85GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 16384), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 6.85GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1765, 16384)
[DEBUG] Memory after feature matrix build: RAM: 6.85GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 6.85GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 6.85GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Loaded probe from results/spam_gemma_9b/seed_45/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_15_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 536 full activations
[DEBUG] Estimated memory requirement: 1.55 GB
[WARNING] Large memory requirement (1.55 GB).
[DEBUG] Max sequence length: 939
[DEBUG] Shape of first activation: (1, 61, 3584)
[DEBUG] Final array shape: (536, 939, 3584)
[DEBUG] Returning (536, 939, 3584) activations
[DEBUG] test activations shape: (536, 939, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (536, 939, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating validation on '94_better_spam': Got unsupported ScalarType BFloat16
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-231.750000, 90.750000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 410.16it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 388.65it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 389.05it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9291044776119403, 'auc': 0.9783637781243039, 'precision': 0.9831932773109243, 'recall': 0.8731343283582089, 'fpr': 0.014925373134328358}
    [BATCH] Evaluating on 94_better_spam test set
Loaded probe from results/spam_gemma_9b/seed_45/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_15_state.npz
Converted attention probe back to bfloat16 for training
Test activations: (536, 939, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating test on '94_better_spam': Got unsupported ScalarType BFloat16
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 407.49it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 422.73it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 394.39it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9291044776119403, 'auc': 0.9783637781243039, 'precision': 0.9831932773109243, 'recall': 0.8731343283582089, 'fpr': 0.014925373134328358}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1306.000000, 211.500000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
Loaded probe from results/spam_gemma_9b/seed_45/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_15_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 5157
[DEBUG] Processing 1282 full activations
[DEBUG] Estimated memory requirement: 0.60 GB
[WARNING] Large memory requirement (0.60 GB).
[DEBUG] Max sequence length: 210
[DEBUG] Shape of first activation: (1, 65, 3584)
[DEBUG] Final array shape: (1282, 210, 3584)
[DEBUG] Returning (1282, 210, 3584) activations
[DEBUG] test activations shape: (1282, 210, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (1282, 210, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating on '87_is_spam': Got unsupported ScalarType BFloat16

=== Group 56/700 — ('94_better_spam', 20, 'resid_post', 47, '{"class_counts": {"0": 1750, "1": 15}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 439.91it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 416.62it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 409.74it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.609204368174727, 'auc': 0.6840423382925956, 'precision': 0.6851851851851852, 'recall': 0.4040561622464899, 'fpr': 0.1856474258970359}
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1765, 3584)
[DEBUG] Returning (1765, 3584) activations
[DEBUG] train activations shape: (1765, 3584), dtype: float16
[DEBUG] train activations range: [-1526.000000, 117.875000]
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_mean
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.71GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 337.41it/s]
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1765, 3584)
[DEBUG] Returning (1765, 3584) activations
[DEBUG] train activations shape: (1765, 3584), dtype: float16
[DEBUG] train activations range: [-1337.000000, 237.000000]
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.85GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 328.67it/s]
[DEBUG] Final encoded shape: (1765, 16384)
Encoded feature matrix shape: (1765, 16384)
[DEBUG] After encoding. Memory: RAM: 6.84GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 16384), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 6.84GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1765, 16384)
[DEBUG] Memory after feature matrix build: RAM: 6.84GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 6.84GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 6.84GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Final encoded shape: (1765, 16384)
Encoded feature matrix shape: (1765, 16384)
[DEBUG] After encoding. Memory: RAM: 9.71GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 16384), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 9.71GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1765, 16384)
[DEBUG] Memory after feature matrix build: RAM: 9.71GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 9.71GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 9.71GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-162.375000, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-570.000000, 51.906250]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 413.56it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 423.20it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 392.14it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8656716417910447, 'auc': 0.9935954555580309, 'precision': 0.9949494949494949, 'recall': 0.7350746268656716, 'fpr': 0.0037313432835820895}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 416.10it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 384.27it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 387.07it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8078358208955224, 'auc': 0.9568528625529071, 'precision': 1.0, 'recall': 0.6156716417910447, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 380.57it/s]
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 428.56it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 379.47it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 169.66it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 172.57it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 294.50it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8078358208955224, 'auc': 0.9568528625529071, 'precision': 1.0, 'recall': 0.6156716417910447, 'fpr': 0.0}
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8656716417910447, 'auc': 0.9935954555580309, 'precision': 0.9949494949494949, 'recall': 0.7350746268656716, 'fpr': 0.0037313432835820895}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1049.000000, 65.937500]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1306.000000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 444.05it/s]
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s][DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 242.94it/s]
Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 255.64it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s][DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 184.00it/s]
Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 221.70it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s][DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7745709828393136, 'auc': 0.8948211282585469, 'precision': 0.8981900452488688, 'recall': 0.6193447737909517, 'fpr': 0.07020280811232449}
Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 269.16it/s]
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5686427457098284, 'auc': 0.867246234311151, 'precision': 0.9583333333333334, 'recall': 0.1435257410296412, 'fpr': 0.0062402496099844}
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1765, 3584)
[DEBUG] Returning (1765, 3584) activations
[DEBUG] train activations shape: (1765, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_max
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1765, 3584)
[DEBUG] Returning (1765, 3584) activations
[DEBUG] train activations shape: (1765, 3584), dtype: float16
[DEBUG] train activations range: [-1506.000000, 112.125000]
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_mean
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.71GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 338.00it/s]
[DEBUG] Final encoded shape: (1765, 16384)
Encoded feature matrix shape: (1765, 16384)
[DEBUG] After encoding. Memory: RAM: 9.71GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 16384), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 9.71GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1765, 16384)
[DEBUG] Memory after feature matrix build: RAM: 9.71GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 9.71GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 9.71GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.87GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.54it/s]
[DEBUG] Final encoded shape: (1765, 262144)
Encoded feature matrix shape: (1765, 262144)
[DEBUG] After encoding. Memory: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 262144), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1765, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-106.125000, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 411.93it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 385.54it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 388.54it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8115671641791045, 'auc': 0.983125417687681, 'precision': 1.0, 'recall': 0.6231343283582089, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-1053.000000, 68.750000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 415.61it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 386.50it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 416.85it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8115671641791045, 'auc': 0.983125417687681, 'precision': 1.0, 'recall': 0.6231343283582089, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-177.375000, 237.500000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.62it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.01it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.32it/s]
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 427.42it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 415.90it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7481343283582089, 'auc': 0.9080530184896414, 'precision': 0.9854014598540146, 'recall': 0.503731343283582, 'fpr': 0.007462686567164179}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 242.89it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5709828393135725, 'auc': 0.9236250885292822, 'precision': 0.989247311827957, 'recall': 0.1435257410296412, 'fpr': 0.0015600624024961}
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1765, 3584)
[DEBUG] Returning (1765, 3584) activations
[DEBUG] train activations shape: (1765, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 231.875000]
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.71GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 336.99it/s]
[DEBUG] Final encoded shape: (1765, 16384)
Encoded feature matrix shape: (1765, 16384)
[DEBUG] After encoding. Memory: RAM: 9.71GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 16384), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 9.71GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1765, 16384)
[DEBUG] Memory after feature matrix build: RAM: 9.71GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 9.71GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 9.71GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.64it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.93it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.87it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7481343283582089, 'auc': 0.9080530184896414, 'precision': 0.9854014598540146, 'recall': 0.503731343283582, 'fpr': 0.007462686567164179}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1490.000000, 111.750000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.10it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.59it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.55it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6045241809672387, 'auc': 0.6232923887938357, 'precision': 0.8850574712643678, 'recall': 0.24024960998439937, 'fpr': 0.031201248049921998}
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-237.375000, 80.312500]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 413.68it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 404.50it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 408.56it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9197761194029851, 'auc': 0.9802572956114948, 'precision': 0.966804979253112, 'recall': 0.8694029850746269, 'fpr': 0.029850746268656716}
    [BATCH] Evaluating on 94_better_spam test set
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_max
  [DEBUG] Fitting SAE probe normally...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 411.85it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 406.82it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 397.83it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9197761194029851, 'auc': 0.9802572956114948, 'precision': 0.966804979253112, 'recall': 0.8694029850746269, 'fpr': 0.029850746268656716}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-268.250000, 76.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 442.86it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 405.40it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 431.00it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6521060842433697, 'auc': 0.6938992068263073, 'precision': 0.6649746192893401, 'recall': 0.6131045241809673, 'fpr': 0.3088923556942278}
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.87GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.54it/s]
[DEBUG] Final encoded shape: (1765, 262144)
Encoded feature matrix shape: (1765, 262144)
[DEBUG] After encoding. Memory: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 262144), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1765, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1765, 3584)
[DEBUG] Returning (1765, 3584) activations
[DEBUG] train activations shape: (1765, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.71GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 335.14it/s]
[DEBUG] Final encoded shape: (1765, 16384)
Encoded feature matrix shape: (1765, 16384)
[DEBUG] After encoding. Memory: RAM: 9.71GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 16384), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 9.71GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1765, 16384)
[DEBUG] Memory after feature matrix build: RAM: 9.71GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 9.71GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 9.71GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.62it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.83it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.70it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.875, 'auc': 0.9957535085765203, 'precision': 1.0, 'recall': 0.75, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-106.500000, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 417.09it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 385.97it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 410.08it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7910447761194029, 'auc': 0.9821090443305859, 'precision': 0.9936708860759493, 'recall': 0.585820895522388, 'fpr': 0.0037313432835820895}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.64it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.88it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.17it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.875, 'auc': 0.9957535085765203, 'precision': 1.0, 'recall': 0.75, 'fpr': 0.0}
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 410.08it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 415.77it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 387.00it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7910447761194029, 'auc': 0.9821090443305859, 'precision': 0.9936708860759493, 'recall': 0.585820895522388, 'fpr': 0.0037313432835820895}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-177.375000, 237.500000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 440.67it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 434.46it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 430.83it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5733229329173167, 'auc': 0.8696775952161331, 'precision': 0.9795918367346939, 'recall': 0.1497659906396256, 'fpr': 0.0031201248049922}
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.08it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.59it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.59it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6544461778471139, 'auc': 0.9480847252610853, 'precision': 1.0, 'recall': 0.3088923556942278, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_mean
  [DEBUG] Fitting SAE probe normally...
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.71GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.88it/s]

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.87GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.65it/s]
[DEBUG] Final encoded shape: (1765, 262144)
Encoded feature matrix shape: (1765, 262144)
[DEBUG] After encoding. Memory: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 262144), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1765, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.59GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Final encoded shape: (1765, 262144)
Encoded feature matrix shape: (1765, 262144)
[DEBUG] After encoding. Memory: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 262144), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1765, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.63it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.97it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.90it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.917910447761194, 'auc': 0.9642041657384718, 'precision': 0.9590163934426229, 'recall': 0.8731343283582089, 'fpr': 0.03731343283582089}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.01it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.71it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.01it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.832089552238806, 'auc': 0.9494040989084428, 'precision': 0.978494623655914, 'recall': 0.6791044776119403, 'fpr': 0.014925373134328358}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.65it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.89it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.16it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.917910447761194, 'auc': 0.9642041657384718, 'precision': 0.9590163934426229, 'recall': 0.8731343283582089, 'fpr': 0.03731343283582089}
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.05it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.78it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.82it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.832089552238806, 'auc': 0.9494040989084428, 'precision': 0.978494623655914, 'recall': 0.6791044776119403, 'fpr': 0.014925373134328358}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.13it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.70it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.68it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6786271450858035, 'auc': 0.7750224517561046, 'precision': 0.75054704595186, 'recall': 0.5351014040561622, 'fpr': 0.17784711388455537}
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.42it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.21it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.03it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5280811232449298, 'auc': 0.5093299519812307, 'precision': 0.7903225806451613, 'recall': 0.07644305772230889, 'fpr': 0.0202808112324493}
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_softmax
  [DEBUG] Fitting SAE probe normally...
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_max
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.88GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.56it/s]

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.71GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.82it/s]
[DEBUG] Final encoded shape: (1765, 262144)
Encoded feature matrix shape: (1765, 262144)
[DEBUG] After encoding. Memory: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 262144), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1765, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.60GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Final encoded shape: (1765, 262144)
Encoded feature matrix shape: (1765, 262144)
[DEBUG] After encoding. Memory: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 262144), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1765, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.67it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.88it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.90it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8656716417910447, 'auc': 0.9929550011138338, 'precision': 1.0, 'recall': 0.7313432835820896, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.02it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.75it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.87it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.832089552238806, 'auc': 0.9945561372243261, 'precision': 1.0, 'recall': 0.664179104477612, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.63it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.94it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.84it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8656716417910447, 'auc': 0.9929550011138338, 'precision': 1.0, 'recall': 0.7313432835820896, 'fpr': 0.0}
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.05it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.82it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.04it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.832089552238806, 'auc': 0.9945561372243261, 'precision': 1.0, 'recall': 0.664179104477612, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.13it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.71it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.77it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6014040561622465, 'auc': 0.9355969246570175, 'precision': 1.0, 'recall': 0.20280811232449297, 'fpr': 0.0}
  🚀 [BATCH] Training probe: attention
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: full
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.44it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.19it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.13it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5858034321372855, 'auc': 0.9160632883973706, 'precision': 0.9910714285714286, 'recall': 0.1731669266770671, 'fpr': 0.0015600624024961}
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.71GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.88it/s]
[DEBUG] Final encoded shape: (1765, 262144)
Encoded feature matrix shape: (1765, 262144)
[DEBUG] After encoding. Memory: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 262144), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1765, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.05it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.94it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.99it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9328358208955224, 'auc': 0.967072287814658, 'precision': 0.964, 'recall': 0.8992537313432836, 'fpr': 0.033582089552238806}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 1765 full activations
[DEBUG] Estimated memory requirement: 3.36 GB
[WARNING] Large memory requirement (3.36 GB).
[DEBUG] Max sequence length: 520
[DEBUG] Shape of first activation: (1, 208, 3584)
[DEBUG] Final array shape: (1765, 520, 3584)
[DEBUG] Returning (1765, 520, 3584) activations
[DEBUG] train activations shape: (1765, 520, 3584), dtype: float16
[DEBUG] train activations range: [-1674.000000, 237.000000]
Train activations: (1765, 520, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: attention
  [DEBUG] Creating attention probe
  [DEBUG] Fitting attention probe normally...
Epoch 10/100, Loss: 0.1700
Epoch 20/100, Loss: 0.0603
Epoch 30/100, Loss: 0.0242
Epoch 40/100, Loss: 0.0095
Epoch 50/100, Loss: 0.0034
Early stopping at epoch 57
Saved probe to results/spam_gemma_9b/seed_46/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_15_state.npz
Saved training log to results/spam_gemma_9b/seed_46/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_15_state_train_log.json
  - 🔥 Probe state saved to train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.05it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.78it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.90it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9328358208955224, 'auc': 0.967072287814658, 'precision': 0.964, 'recall': 0.8992537313432836, 'fpr': 0.033582089552238806}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.45it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.16it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.12it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.719188767550702, 'auc': 0.7907204275690528, 'precision': 0.7337770382695508, 'recall': 0.6879875195007801, 'fpr': 0.24960998439937598}
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
Loaded probe from results/spam_gemma_9b/seed_46/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_15_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 536 full activations
[DEBUG] Estimated memory requirement: 1.60 GB
[WARNING] Large memory requirement (1.60 GB).
[DEBUG] Max sequence length: 1167
[DEBUG] Shape of first activation: (1, 225, 3584)
[DEBUG] Final array shape: (536, 1167, 3584)
[DEBUG] Returning (536, 1167, 3584) activations
[DEBUG] test activations shape: (536, 1167, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (536, 1167, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating validation on '94_better_spam': Got unsupported ScalarType BFloat16
    [BATCH] Evaluating on 94_better_spam test set
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_softmax
  [DEBUG] Fitting SAE probe normally...
Loaded probe from results/spam_gemma_9b/seed_46/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_15_state.npz
Converted attention probe back to bfloat16 for training
Test activations: (536, 1167, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating test on '94_better_spam': Got unsupported ScalarType BFloat16

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.71GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.88it/s]
[DEBUG] Final encoded shape: (1765, 262144)
Encoded feature matrix shape: (1765, 262144)
[DEBUG] After encoding. Memory: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 262144), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1765, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.42GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.43GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Loaded probe from results/spam_gemma_9b/seed_46/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_15_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 5157
[DEBUG] Processing 1282 full activations
[DEBUG] Estimated memory requirement: 0.60 GB
[WARNING] Large memory requirement (0.60 GB).
[DEBUG] Max sequence length: 163
[DEBUG] Shape of first activation: (1, 26, 3584)
[DEBUG] Final array shape: (1282, 163, 3584)
[DEBUG] Returning (1282, 163, 3584) activations
[DEBUG] test activations shape: (1282, 163, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (1282, 163, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating on '87_is_spam': Got unsupported ScalarType BFloat16

=== Group 57/700 — ('94_better_spam', 20, 'resid_post', 48, '{"class_counts": {"0": 1750, "1": 15}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1765, 3584)
[DEBUG] Returning (1765, 3584) activations
[DEBUG] train activations shape: (1765, 3584), dtype: float16
[DEBUG] train activations range: [-1526.000000, 117.875000]
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_mean
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.96it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.70it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]
=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.86GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 150.01it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 30.37it/s]

[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8246268656716418, 'auc': 0.9916462463800401, 'precision': 1.0, 'recall': 0.6492537313432836, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Final encoded shape: (1765, 16384)
Encoded feature matrix shape: (1765, 16384)
[DEBUG] After encoding. Memory: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 16384), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1765, 16384)
[DEBUG] Memory after feature matrix build: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 6.87GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.02it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.01it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.93it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8246268656716418, 'auc': 0.9916462463800401, 'precision': 1.0, 'recall': 0.6492537313432836, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-567.000000, 51.500000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 413.35it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 425.17it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 425.99it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7966417910447762, 'auc': 0.9773334818445089, 'precision': 0.9818181818181818, 'recall': 0.6044776119402985, 'fpr': 0.011194029850746268}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.43it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.00it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.99it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5748829953198128, 'auc': 0.9201204241617404, 'precision': 0.9897959183673469, 'recall': 0.15132605304212168, 'fpr': 0.0015600624024961}
  🚀 [BATCH] Training probe: attention
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: full
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 396.29it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 406.15it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 406.35it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7966417910447762, 'auc': 0.9773334818445089, 'precision': 0.9818181818181818, 'recall': 0.6044776119402985, 'fpr': 0.011194029850746268}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1505.000000, 110.875000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 428.25it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 426.03it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 421.31it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7519500780031201, 'auc': 0.8060728045346464, 'precision': 0.8891566265060241, 'recall': 0.5756630265210608, 'fpr': 0.0717628705148206}
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1765, 3584)
[DEBUG] Returning (1765, 3584) activations
[DEBUG] train activations shape: (1765, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_max
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.84GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 303.00it/s]
[DEBUG] Final encoded shape: (1765, 16384)
Encoded feature matrix shape: (1765, 16384)
[DEBUG] After encoding. Memory: RAM: 6.83GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 16384), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 6.83GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1765, 16384)
[DEBUG] Memory after feature matrix build: RAM: 6.83GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 6.83GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 6.83GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-106.125000, 237.500000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 397.00it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 417.39it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 414.70it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8059701492537313, 'auc': 0.9886388950768545, 'precision': 1.0, 'recall': 0.6119402985074627, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 415.28it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 397.90it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 394.24it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8059701492537313, 'auc': 0.9886388950768545, 'precision': 1.0, 'recall': 0.6119402985074627, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1336.000000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 442.46it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 416.87it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 421.41it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6115444617784711, 'auc': 0.9109133788128435, 'precision': 0.993103448275862, 'recall': 0.22464898595943839, 'fpr': 0.0015600624024961}
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 1765 full activations
[DEBUG] Estimated memory requirement: 3.34 GB
[WARNING] Large memory requirement (3.34 GB).
[DEBUG] Max sequence length: 1167
[DEBUG] Shape of first activation: (1, 384, 3584)
[DEBUG] Final array shape: (1765, 1167, 3584)
[DEBUG] Returning (1765, 1167, 3584) activations
[DEBUG] train activations shape: (1765, 1167, 3584), dtype: float16
[DEBUG] train activations range: [-1674.000000, 237.000000]
Train activations: (1765, 1167, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: attention
  [DEBUG] Creating attention probe
  [DEBUG] Fitting attention probe normally...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1765, 3584)
[DEBUG] Returning (1765, 3584) activations
[DEBUG] train activations shape: (1765, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 231.875000]
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.84GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 141.38it/s]
Epoch 10/100, Loss: 0.3010
Epoch 20/100, Loss: 0.1301
Epoch 30/100, Loss: 0.0610
Epoch 40/100, Loss: 0.0304
Epoch 50/100, Loss: 0.0206
Epoch 60/100, Loss: 0.0121
Epoch 70/100, Loss: 0.0070
Early stopping at epoch 74
Saved probe to results/spam_gemma_9b/seed_47/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_15_state.npz
Saved training log to results/spam_gemma_9b/seed_47/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_15_state_train_log.json
  - 🔥 Probe state saved to train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Final encoded shape: (1765, 16384)
Encoded feature matrix shape: (1765, 16384)
[DEBUG] After encoding. Memory: RAM: 6.93GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 16384), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 6.93GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1765, 16384)
[DEBUG] Memory after feature matrix build: RAM: 6.93GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 6.93GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 6.93GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-256.250000, 81.875000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 412.05it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 390.64it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 407.69it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9067164179104478, 'auc': 0.9755791935843172, 'precision': 0.9658119658119658, 'recall': 0.8432835820895522, 'fpr': 0.029850746268656716}
    [BATCH] Evaluating on 94_better_spam test set
Loaded probe from results/spam_gemma_9b/seed_47/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_15_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 536 full activations
[DEBUG] Estimated memory requirement: 1.60 GB
[WARNING] Large memory requirement (1.60 GB).
[DEBUG] Max sequence length: 939
[DEBUG] Shape of first activation: (1, 335, 3584)
[DEBUG] Final array shape: (536, 939, 3584)
[DEBUG] Returning (536, 939, 3584) activations
[DEBUG] test activations shape: (536, 939, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.000000]
Test activations: (536, 939, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating validation on '94_better_spam': Got unsupported ScalarType BFloat16
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 415.52it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 420.82it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 380.26it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9067164179104478, 'auc': 0.9755791935843172, 'precision': 0.9658119658119658, 'recall': 0.8432835820895522, 'fpr': 0.029850746268656716}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1336.000000, 221.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 438.80it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 432.49it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 381.58it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5764430577223089, 'auc': 0.6237231704556794, 'precision': 0.6074561403508771, 'recall': 0.43213728549141966, 'fpr': 0.2792511700468019}
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
Loaded probe from results/spam_gemma_9b/seed_47/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_15_state.npz
Converted attention probe back to bfloat16 for training
Test activations: (536, 939, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating test on '94_better_spam': Got unsupported ScalarType BFloat16
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1765, 3584)
[DEBUG] Returning (1765, 3584) activations
[DEBUG] train activations shape: (1765, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.84GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 339.02it/s]
[DEBUG] Final encoded shape: (1765, 16384)
Encoded feature matrix shape: (1765, 16384)
[DEBUG] After encoding. Memory: RAM: 6.92GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 16384), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 6.92GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1765, 16384)
[DEBUG] Memory after feature matrix build: RAM: 6.92GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 6.92GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 6.92GB, GPU_allocated: 0.50GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Loaded probe from results/spam_gemma_9b/seed_47/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_15_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 5157
[DEBUG] Processing 1282 full activations
[DEBUG] Estimated memory requirement: 0.59 GB
[WARNING] Large memory requirement (0.59 GB).
[DEBUG] Max sequence length: 210
[DEBUG] Shape of first activation: (1, 20, 3584)
[DEBUG] Final array shape: (1282, 210, 3584)
[DEBUG] Returning (1282, 210, 3584) activations
[DEBUG] test activations shape: (1282, 210, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.500000]
Test activations: (1282, 210, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating on '87_is_spam': Got unsupported ScalarType BFloat16

=== Group 58/700 — ('94_better_spam', 20, 'resid_post', 49, '{"class_counts": {"0": 1750, "1": 15}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-106.500000, 237.500000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 414.91it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 395.47it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 418.68it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8227611940298507, 'auc': 0.9847683225662731, 'precision': 1.0, 'recall': 0.6455223880597015, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1765, 3584)
[DEBUG] Returning (1765, 3584) activations
[DEBUG] train activations shape: (1765, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_max
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.73GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 337.80it/s]
[DEBUG] Final encoded shape: (1765, 16384)
Encoded feature matrix shape: (1765, 16384)
[DEBUG] After encoding. Memory: RAM: 9.73GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 16384), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 9.73GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1765, 16384)
[DEBUG] Memory after feature matrix build: RAM: 9.73GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 9.73GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 9.73GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 409.88it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 389.33it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 388.79it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8227611940298507, 'auc': 0.9847683225662731, 'precision': 1.0, 'recall': 0.6455223880597015, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1336.000000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 432.02it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 422.41it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 418.18it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6638065522620905, 'auc': 0.9264312538180153, 'precision': 0.9952830188679245, 'recall': 0.3291731669266771, 'fpr': 0.0015600624024961}
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-119.687500, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 411.81it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 383.92it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 390.90it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8731343283582089, 'auc': 0.9940827578525284, 'precision': 1.0, 'recall': 0.746268656716418, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_mean
  [DEBUG] Fitting SAE probe normally...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 407.97it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 410.20it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 411.13it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8731343283582089, 'auc': 0.9940827578525284, 'precision': 1.0, 'recall': 0.746268656716418, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-177.375000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 441.90it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 439.06it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 415.75it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6170046801872074, 'auc': 0.8922023651616893, 'precision': 0.993421052631579, 'recall': 0.23556942277691106, 'fpr': 0.0015600624024961}
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.83GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.55it/s]
[DEBUG] Final encoded shape: (1765, 262144)
Encoded feature matrix shape: (1765, 262144)
[DEBUG] After encoding. Memory: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 262144), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1765, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1765, 3584)
[DEBUG] Returning (1765, 3584) activations
[DEBUG] train activations shape: (1765, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 231.875000]
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_last
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.73GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 336.01it/s]
[DEBUG] Final encoded shape: (1765, 16384)
Encoded feature matrix shape: (1765, 16384)
[DEBUG] After encoding. Memory: RAM: 9.73GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 16384), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 9.73GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1765, 16384)
[DEBUG] Memory after feature matrix build: RAM: 9.73GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 9.73GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 9.73GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.62it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.78it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.05it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7555970149253731, 'auc': 0.936887391401203, 'precision': 0.9858156028368794, 'recall': 0.5186567164179104, 'fpr': 0.007462686567164179}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-210.625000, 80.875000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 416.35it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 382.66it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 383.11it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9029850746268657, 'auc': 0.9793244597905992, 'precision': 0.9655172413793104, 'recall': 0.835820895522388, 'fpr': 0.029850746268656716}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 418.13it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 385.47it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 386.39it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9029850746268657, 'auc': 0.9793244597905992, 'precision': 0.9655172413793104, 'recall': 0.835820895522388, 'fpr': 0.029850746268656716}
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.61it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.83it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.83it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7555970149253731, 'auc': 0.936887391401203, 'precision': 0.9858156028368794, 'recall': 0.5186567164179104, 'fpr': 0.007462686567164179}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-263.750000, 76.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 444.31it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 415.81it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 414.05it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6068642745709828, 'auc': 0.7002149040719821, 'precision': 0.648590021691974, 'recall': 0.4664586583463339, 'fpr': 0.25273010920436817}
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.07it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.60it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.59it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5678627145085804, 'auc': 0.5371871661137897, 'precision': 0.9223300970873787, 'recall': 0.1482059282371295, 'fpr': 0.0124804992199688}
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1765, 3584)
[DEBUG] Returning (1765, 3584) activations
[DEBUG] train activations shape: (1765, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.73GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 335.08it/s]
[DEBUG] Final encoded shape: (1765, 16384)
Encoded feature matrix shape: (1765, 16384)
[DEBUG] After encoding. Memory: RAM: 9.73GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 16384), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 9.73GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1765, 16384)
[DEBUG] Memory after feature matrix build: RAM: 9.73GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 9.73GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 9.73GB, GPU_allocated: 0.50GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_max
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.84GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.54it/s]
[DEBUG] Final encoded shape: (1765, 262144)
Encoded feature matrix shape: (1765, 262144)
[DEBUG] After encoding. Memory: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 262144), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1765, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.56GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-119.750000, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 409.68it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 381.79it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 414.62it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8619402985074627, 'auc': 0.9948485186010247, 'precision': 1.0, 'recall': 0.7238805970149254, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 415.57it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 380.61it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 388.43it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8619402985074627, 'auc': 0.9948485186010247, 'precision': 1.0, 'recall': 0.7238805970149254, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-177.375000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 440.05it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 411.17it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 403.59it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5959438377535101, 'auc': 0.8802329628286536, 'precision': 1.0, 'recall': 0.1918876755070203, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.62it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.81it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.63it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.789179104477612, 'auc': 0.9880123635553576, 'precision': 0.9936305732484076, 'recall': 0.582089552238806, 'fpr': 0.0037313432835820895}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1765, 3584)
[DEBUG] Returning (1765, 3584) activations
[DEBUG] train activations shape: (1765, 3584), dtype: float16
[DEBUG] train activations range: [-1526.000000, 117.875000]
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_mean
  [DEBUG] Fitting SAE probe normally...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.74GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.87it/s]
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.67it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.10it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.87it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.789179104477612, 'auc': 0.9880123635553576, 'precision': 0.9936305732484076, 'recall': 0.582089552238806, 'fpr': 0.0037313432835820895}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Final encoded shape: (1765, 262144)
Encoded feature matrix shape: (1765, 262144)
[DEBUG] After encoding. Memory: RAM: 11.46GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 262144), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 11.46GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1765, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.46GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.46GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.46GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.11it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.65it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.61it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.594383775351014, 'auc': 0.9358938476103786, 'precision': 0.9763779527559056, 'recall': 0.1934477379095164, 'fpr': 0.0046801872074883}
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-813.000000, 51.906250]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_last
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.02it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.65it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.66it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8041044776119403, 'auc': 0.9693974159055468, 'precision': 1.0, 'recall': 0.6082089552238806, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.85GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.54it/s]
[DEBUG] Final encoded shape: (1765, 262144)
Encoded feature matrix shape: (1765, 262144)
[DEBUG] After encoding. Memory: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 262144), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1765, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.04it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.73it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.98it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8041044776119403, 'auc': 0.9693974159055468, 'precision': 1.0, 'recall': 0.6082089552238806, 'fpr': 0.0}
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1053.000000, 69.625000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.63it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.76it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.89it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8955223880597015, 'auc': 0.9605981287591892, 'precision': 0.9416666666666667, 'recall': 0.8432835820895522, 'fpr': 0.05223880597014925}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.43it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.03it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.07it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5663026521060842, 'auc': 0.5727789798019378, 'precision': 0.967032967032967, 'recall': 0.1372854914196568, 'fpr': 0.0046801872074883}
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_max
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.62it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.93it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.13it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8955223880597015, 'auc': 0.9605981287591892, 'precision': 0.9416666666666667, 'recall': 0.8432835820895522, 'fpr': 0.05223880597014925}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.74GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.88it/s]
[DEBUG] Final encoded shape: (1765, 262144)
Encoded feature matrix shape: (1765, 262144)
[DEBUG] After encoding. Memory: RAM: 11.46GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 262144), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 11.46GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1765, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.46GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.46GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.46GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.11it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.61it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.65it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5873634945397815, 'auc': 0.6246723503885554, 'precision': 0.5982456140350877, 'recall': 0.53198127925117, 'fpr': 0.35725429017160687}
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_softmax
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.73it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.68it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8694029850746269, 'auc': 0.9970622633103141, 'precision': 1.0, 'recall': 0.7388059701492538, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 6.85GB, GPU_allocated: 0.06GB, GPU_reserved: 45.12GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.55it/s]
[DEBUG] Final encoded shape: (1765, 262144)
Encoded feature matrix shape: (1765, 262144)
[DEBUG] After encoding. Memory: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 262144), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Feature matrix for selection shape: (1765, 262144)
[DEBUG] Memory after feature matrix build: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 8.58GB, GPU_allocated: 7.06GB, GPU_reserved: 45.12GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.02it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.93it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.94it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8694029850746269, 'auc': 0.9970622633103141, 'precision': 1.0, 'recall': 0.7388059701492538, 'fpr': 0.0}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.63it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 26.22it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.78it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7686567164179104, 'auc': 0.9871491423479617, 'precision': 0.9931506849315068, 'recall': 0.5410447761194029, 'fpr': 0.0037313432835820895}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.40it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.14it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.04it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5951638065522621, 'auc': 0.9136927723598802, 'precision': 1.0, 'recall': 0.19032761310452417, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_last
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.63it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.87it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 36.90it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7686567164179104, 'auc': 0.9871491423479617, 'precision': 0.9931506849315068, 'recall': 0.5410447761194029, 'fpr': 0.0037313432835820895}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.73GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.87it/s]
[DEBUG] Final encoded shape: (1765, 262144)
Encoded feature matrix shape: (1765, 262144)
[DEBUG] After encoding. Memory: RAM: 11.45GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 262144), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 11.45GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1765, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.45GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.45GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.45GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.12it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.74it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.58it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5491419656786272, 'auc': 0.8946093881196746, 'precision': 0.9846153846153847, 'recall': 0.0998439937597504, 'fpr': 0.0015600624024961}
  🚀 [BATCH] Training probe: attention
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: full
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.98it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.03it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9085820895522388, 'auc': 0.9738805970149254, 'precision': 0.9469387755102041, 'recall': 0.8656716417910447, 'fpr': 0.048507462686567165}
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.01it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.01it/s]
[DEBUG] Final encoded shape: (536, 262144)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.84it/s]
[DEBUG] Final encoded shape: (536, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9085820895522388, 'auc': 0.9738805970149254, 'precision': 0.9469387755102041, 'recall': 0.8656716417910447, 'fpr': 0.048507462686567165}
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 1765 full activations
[DEBUG] Estimated memory requirement: 3.28 GB
[WARNING] Large memory requirement (3.28 GB).
[DEBUG] Max sequence length: 520
[DEBUG] Shape of first activation: (1, 251, 3584)
[DEBUG] Final array shape: (1765, 520, 3584)
[DEBUG] Returning (1765, 520, 3584) activations
[DEBUG] train activations shape: (1765, 520, 3584), dtype: float16
[DEBUG] train activations range: [-1674.000000, 237.000000]
Train activations: (1765, 520, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: attention
  [DEBUG] Creating attention probe
  [DEBUG] Fitting attention probe normally...
Epoch 10/100, Loss: 0.2055
Epoch 20/100, Loss: 0.0715
Epoch 30/100, Loss: 0.0304
Epoch 40/100, Loss: 0.0101
Epoch 50/100, Loss: 0.0051
Early stopping at epoch 54
Saved probe to results/spam_gemma_9b/seed_48/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_15_state.npz
Saved training log to results/spam_gemma_9b/seed_48/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_15_state_train_log.json
  - 🔥 Probe state saved to train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28.39it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.14it/s]
[DEBUG] Final encoded shape: (1282, 262144)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.00it/s]
[DEBUG] Final encoded shape: (1282, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6653666146645866, 'auc': 0.7377634886986744, 'precision': 0.7294372294372294, 'recall': 0.5257410296411856, 'fpr': 0.19500780031201248}
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
Train activations: (1765, 3584)
[DEBUG] Pre-fit sample counts — X: 1765, y: 1765
[DEBUG] y_train class distribution: {0: 1750, 1: 15}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_softmax
  [DEBUG] Fitting SAE probe normally...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1765, 3584)
Input y shape: (1765,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 9.73GB, GPU_allocated: 0.06GB, GPU_reserved: 42.57GB
[DEBUG] Input activations shape: (1765, 3584)
[DEBUG] Using pre-aggregated inputs: (1765, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21.88it/s]
[DEBUG] Final encoded shape: (1765, 262144)
Encoded feature matrix shape: (1765, 262144)
[DEBUG] After encoding. Memory: RAM: 11.45GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1765, 262144), y_train=(1765,)
[DEBUG] Memory at start of feature selection: RAM: 11.45GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Feature matrix for selection shape: (1765, 262144)
[DEBUG] Memory after feature matrix build: RAM: 11.45GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 11.45GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 11.45GB, GPU_allocated: 7.06GB, GPU_reserved: 42.57GB
Selected 3584 features
[DEBUG] X_selected shape: (1765, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_class0_1750_class1_15_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Loaded probe from results/spam_gemma_9b/seed_48/2-spam-pred-auc-increasing-spam-fixed-total/trained/train_on_94_better_spam_attention_L20_resid_post_class0_1750_class1_15_state.npz
Converted attention probe back to bfloat16 for training
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 536 full activations
[DEBUG] Estimated memory requirement: 1.63 GB
[WARNING] Large memory requirement (1.63 GB).
[DEBUG] Max sequence length: 939
[DEBUG] Shape of first activation: (1, 289, 3584)
[DEBUG] Final array shape: (536, 939, 3584)
[DEBUG] Returning (536, 939, 3584) activations
[DEBUG] test activations shape: (536, 939, 3584), dtype: float16
[DEBUG] test activations range: [-1674.000000, 237.500000]
Test activations: (536, 939, 3584)
  - 🤗 Calculating metrics...
    💀💀💀 ERROR evaluating validation on '94_better_spam': Got unsupported ScalarType BFloat16
    [BATCH] Evaluating on 94_better_spam test set
2025-08-17 20:22:46 Caught termination signal. Exiting.
