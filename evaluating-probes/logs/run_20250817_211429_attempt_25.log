2025-08-17 21:14:29 Logging attempt 25 to ./logs/run_20250817_211429_attempt_25.log
2025-08-17 21:14:29 Starting (attempt 25) → python -m src.main -c llama_mask_cpu
Loaded environment variables from /lambda/nfs/riya-probing/pivotal-research/evaluating-probes/.env

=== DEBUG: Environment Variables ===
OPENAI_API_KEY=sk**************************************************************************************************************************************************************VwIA
OMP_NUM_THREADS=1
CUDA_VISIBLE_DEVICES=None
EP_BATCHED_NJOBS=2
====================================

Set CUDA_VISIBLE_DEVICES to 0, updated device to cuda:0

=== DEBUG: Environment Variables ===
OPENAI_API_KEY=sk**************************************************************************************************************************************************************VwIA
OMP_NUM_THREADS=1
CUDA_VISIBLE_DEVICES=0
EP_BATCHED_NJOBS=2
====================================

[model_check] Clearing CUDA memory at start...
Processing 10 seeds: [42, 43, 44, 45, 46, 47, 48, 49, 50, 51]
No llm_upsampling_experiments found, skipping LLM upsampling
Loading model 'meta-llama/Llama-3.3-70B-Instruct' for activation extraction...
Loading checkpoint shards:   0%|                                                              | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:   3%|█▊                                                    | 1/30 [00:00<00:18,  1.57it/s]Loading checkpoint shards:   7%|███▌                                                  | 2/30 [00:01<00:18,  1.49it/s]Loading checkpoint shards:  10%|█████▍                                                | 3/30 [00:02<00:18,  1.42it/s]Loading checkpoint shards:  13%|███████▏                                              | 4/30 [00:02<00:18,  1.40it/s]Loading checkpoint shards:  17%|█████████                                             | 5/30 [00:03<00:17,  1.41it/s]Loading checkpoint shards:  20%|██████████▊                                           | 6/30 [00:04<00:16,  1.42it/s]Loading checkpoint shards:  23%|████████████▌                                         | 7/30 [00:04<00:16,  1.43it/s]Loading checkpoint shards:  27%|██████████████▍                                       | 8/30 [00:05<00:15,  1.41it/s]Loading checkpoint shards:  30%|████████████████▏                                     | 9/30 [00:06<00:15,  1.40it/s]Loading checkpoint shards:  33%|█████████████████▋                                   | 10/30 [00:07<00:14,  1.41it/s]Loading checkpoint shards:  37%|███████████████████▍                                 | 11/30 [00:07<00:13,  1.43it/s]Loading checkpoint shards:  40%|█████████████████████▏                               | 12/30 [00:08<00:12,  1.43it/s]Loading checkpoint shards:  43%|██████████████████████▉                              | 13/30 [00:09<00:12,  1.41it/s]Loading checkpoint shards:  47%|████████████████████████▋                            | 14/30 [00:09<00:11,  1.40it/s]Loading checkpoint shards:  50%|██████████████████████████▌                          | 15/30 [00:10<00:10,  1.40it/s]Loading checkpoint shards:  53%|████████████████████████████▎                        | 16/30 [00:11<00:09,  1.40it/s]Loading checkpoint shards:  57%|██████████████████████████████                       | 17/30 [00:12<00:09,  1.41it/s]Loading checkpoint shards:  60%|███████████████████████████████▊                     | 18/30 [00:12<00:08,  1.38it/s]Loading checkpoint shards:  63%|█████████████████████████████████▌                   | 19/30 [00:13<00:08,  1.37it/s]Loading checkpoint shards:  67%|███████████████████████████████████▎                 | 20/30 [00:14<00:07,  1.39it/s]Loading checkpoint shards:  70%|█████████████████████████████████████                | 21/30 [00:14<00:06,  1.40it/s]Loading checkpoint shards:  73%|██████████████████████████████████████▊              | 22/30 [00:15<00:05,  1.41it/s]Loading checkpoint shards:  77%|████████████████████████████████████████▋            | 23/30 [00:16<00:05,  1.39it/s]Loading checkpoint shards:  80%|██████████████████████████████████████████▍          | 24/30 [00:17<00:04,  1.38it/s]Loading checkpoint shards:  83%|████████████████████████████████████████████▏        | 25/30 [00:17<00:03,  1.39it/s]Loading checkpoint shards:  87%|█████████████████████████████████████████████▉       | 26/30 [00:18<00:02,  1.40it/s]Loading checkpoint shards:  90%|███████████████████████████████████████████████▋     | 27/30 [00:19<00:02,  1.41it/s]Loading checkpoint shards:  93%|█████████████████████████████████████████████████▍   | 28/30 [00:19<00:01,  1.39it/s]Loading checkpoint shards:  97%|███████████████████████████████████████████████████▏ | 29/30 [00:20<00:00,  1.38it/s]Loading checkpoint shards: 100%|█████████████████████████████████████████████████████| 30/30 [00:20<00:00,  1.75it/s]Loading checkpoint shards: 100%|█████████████████████████████████████████████████████| 30/30 [00:20<00:00,  1.44it/s]

========================= ACTIVATION EXTRACTION PHASE =========================
  - Extracting activations for 102_mask_known_facts_honesty, L50, resid_post (on_policy=True)
Initializing ActivationManager with model: meta-llama/Llama-3.3-70B-Instruct, device: cuda:0, cache_dir: activation_cache/meta-llama/Llama-3.3-70B-Instruct/102_mask_known_facts_honesty
[DEBUG] model_name parameter: meta-llama/Llama-3.3-70B-Instruct
[DEBUG] self.model_name: meta-llama/Llama-3.3-70B-Instruct
[DEBUG] model parameter: LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(128256, 8192)
    (layers): ModuleList(
      (0-79): 80 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)
          (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)
          (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)
          (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear4bit(in_features=8192, out_features=28672, bias=False)
          (up_proj): Linear4bit(in_features=8192, out_features=28672, bias=False)
          (down_proj): Linear4bit(in_features=28672, out_features=8192, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((8192,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((8192,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((8192,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=8192, out_features=128256, bias=False)
)
[DEBUG] model type: <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>
Successfully initialized ActivationManager
    - On-policy: extracting activations using format 'qr'...
[DEBUG] Newly added count: 131
[DEBUG] Extracting activations for 131 missing texts
[DEBUG] Memory before extraction: 36.82 GB
Extracting activations for qr:   0%|                                                         | 0/131 [00:00<?, ?it/s]Extracting activations for qr:   0%|                                                         | 0/131 [00:00<?, ?it/s]
[DEBUG] Error during activation extraction: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument other in method wrapper_CUDA__equal)
    - 💀💀💀 ERROR extracting activations for 102_mask_known_facts_honesty: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument other in method wrapper_CUDA__equal)
================================================================================
😜 Run finished. Closing log file.
Traceback (most recent call last):
  File "/usr/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/usr/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/lambda/nfs/riya-probing/pivotal-research/evaluating-probes/src/main.py", line 1002, in <module>
    main()
  File "/lambda/nfs/riya-probing/pivotal-research/evaluating-probes/src/main.py", line 953, in main
    extract_all_activations(
  File "/lambda/nfs/riya-probing/pivotal-research/evaluating-probes/src/main.py", line 390, in extract_all_activations
    extract_activations_for_dataset(
  File "/lambda/nfs/riya-probing/pivotal-research/evaluating-probes/src/utils_training.py", line 136, in extract_activations_for_dataset
    _, newly_added = ds.act_manager.get_activations_for_texts(
  File "/lambda/nfs/riya-probing/pivotal-research/evaluating-probes/src/activations.py", line 135, in get_activations_for_texts
    new_activations = self._extract_activations(
  File "/lambda/nfs/riya-probing/pivotal-research/evaluating-probes/src/activations.py", line 258, in _extract_activations
    if torch.equal(
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument other in method wrapper_CUDA__equal)
2025-08-17 21:15:17 Command exited with status 1. Will restart.
2025-08-17 21:15:17 Sleeping 1s before restart #25...
