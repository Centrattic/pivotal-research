2025-08-17 20:29:24 Logging attempt 3 to ./logs/run_20250817_202924_attempt_3.log
2025-08-17 20:29:24 Starting (attempt 3) â†’ python -m src.main -c llama_mask_cpu
Loaded environment variables from /lambda/nfs/riya-probing/pivotal-research/evaluating-probes/.env

=== DEBUG: Environment Variables ===
OPENAI_API_KEY=sk**************************************************************************************************************************************************************VwIA
OMP_NUM_THREADS=1
CUDA_VISIBLE_DEVICES=None
EP_BATCHED_NJOBS=2
====================================

Set CUDA_VISIBLE_DEVICES to 1, updated device to cuda:0

=== DEBUG: Environment Variables ===
OPENAI_API_KEY=sk**************************************************************************************************************************************************************VwIA
OMP_NUM_THREADS=1
CUDA_VISIBLE_DEVICES=1
EP_BATCHED_NJOBS=2
====================================

Updated device from cuda:1 to cuda:0
Processing 10 seeds: [42, 43, 44, 45, 46, 47, 48, 49, 50, 51]
No llm_upsampling_experiments found, skipping LLM upsampling
Loading model 'meta-llama/Llama-3.3-70B-Instruct' for activation extraction...
The 8-bit optimizer is not available on your device, only available on CUDA for now.
================================================================================
ðŸ˜œ Run finished. Closing log file.
Traceback (most recent call last):
  File "/usr/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/usr/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/lambda/nfs/riya-probing/pivotal-research/evaluating-probes/src/main.py", line 1002, in <module>
    main()
  File "/lambda/nfs/riya-probing/pivotal-research/evaluating-probes/src/main.py", line 938, in main
    model = AutoModelForCausalLM.from_pretrained(
  File "/lambda/nfs/riya-probing/pivotal-research/pivotal_env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 600, in from_pretrained
    return model_class.from_pretrained(
  File "/lambda/nfs/riya-probing/pivotal-research/pivotal_env/lib/python3.10/site-packages/transformers/modeling_utils.py", line 311, in _wrapper
    return func(*args, **kwargs)
  File "/lambda/nfs/riya-probing/pivotal-research/pivotal_env/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4839, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/lambda/nfs/riya-probing/pivotal-research/pivotal_env/lib/python3.10/site-packages/transformers/modeling_utils.py", line 5260, in _load_pretrained_model
    caching_allocator_warmup(model_to_load, expanded_device_map, hf_quantizer)
  File "/lambda/nfs/riya-probing/pivotal-research/pivotal_env/lib/python3.10/site-packages/transformers/modeling_utils.py", line 5861, in caching_allocator_warmup
    device_memory = torch.cuda.mem_get_info(index)[0]
  File "/lambda/nfs/riya-probing/pivotal-research/pivotal_env/lib/python3.10/site-packages/torch/cuda/memory.py", line 738, in mem_get_info
    return torch.cuda.cudart().cudaMemGetInfo(device)
  File "/lambda/nfs/riya-probing/pivotal-research/pivotal_env/lib/python3.10/site-packages/torch/cuda/__init__.py", line 396, in cudart
    _lazy_init()
  File "/lambda/nfs/riya-probing/pivotal-research/pivotal_env/lib/python3.10/site-packages/torch/cuda/__init__.py", line 319, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
2025-08-17 20:29:47 Command exited with status 1. Will restart.
2025-08-17 20:29:47 Sleeping 1s before restart #3...
