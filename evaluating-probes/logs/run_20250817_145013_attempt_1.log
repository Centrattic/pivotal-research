2025-08-17 14:50:13 Logging attempt 1 to ./logs/run_20250817_145013_attempt_1.log
2025-08-17 14:50:13 Starting (attempt 1) → python -m src.main -c gemma_spam_gpu
Loaded environment variables from /lambda/nfs/riya-probing/pivotal-research/evaluating-probes/.env

=== DEBUG: Environment Variables ===
OPENAI_API_KEY=sk**************************************************************************************************************************************************************VwIA
OMP_NUM_THREADS=1
CUDA_VISIBLE_DEVICES=None
EP_BATCHED_NJOBS=32
====================================

Set CUDA_VISIBLE_DEVICES to 0, updated device to cuda:0

=== DEBUG: Environment Variables ===
OPENAI_API_KEY=sk**************************************************************************************************************************************************************VwIA
OMP_NUM_THREADS=1
CUDA_VISIBLE_DEVICES=0
EP_BATCHED_NJOBS=32
====================================

[model_check] Clearing CUDA memory at start...
Processing 10 seeds: [42, 43, 44, 45, 46, 47, 48, 49, 50, 51]

=== Skipping model_check: all runthrough directories already exist ===


=== Running LLM upsampling (external script) ===
Invoking: /lambda/nfs/riya-probing/pivotal-research/pivotal_env/bin/python -m src.llm_upsampling.llm_upsampling_script -c gemma_spam_gpu --api-key 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
Current working directory: /lambda/nfs/riya-probing/pivotal-research/evaluating-probes
Extracted from config:
  Seeds: [42, 43, 44, 45, 46, 47, 48, 49, 50, 51]
  Num real samples: [1, 2, 3, 4, 5, 10]
  Upsampling factors: [1, 2, 3, 4, 5, 10, 20]
  Train on dataset: 94_better_spam
Initializing LLM client with model: gpt-4o-mini
API key length: 164 characters
LLM client initialized successfully

================================================================================
Processing seed 42
================================================================================
Output directory: results/spam_gemma_9b/seed_42/llm_samples_94_better_spam
Extracting samples using seed 42...
Loading dataset: 94_better_spam
No model or model_name provided, not initializing activation manager.
Total positive samples in dataset: 4000
Extracted 1 samples for n_real_samples=1
Extracted 2 samples for n_real_samples=2
Extracted 3 samples for n_real_samples=3
Extracted 4 samples for n_real_samples=4
Extracted 5 samples for n_real_samples=5
Extracted 10 samples for n_real_samples=10

------------------------------------------------------------
Processing n_real_samples = 1
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_42/llm_samples_94_better_spam/samples_1.csv
   Found 20 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 1
   Starting with existing 20 samples
Maximum target samples needed: 20
📁 Using existing dataset for activation extraction
   Debug: final_df has 20 samples, original_df has 1 samples
   Debug: LLM samples (after original) should be 19
📊 Final dataset breakdown:
   - Original samples: 1
   - Total samples in final dataset: 20
   - Generated samples: 19
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 20 samples to: results/spam_gemma_9b/seed_42/llm_samples_94_better_spam/samples_1.csv
   (Includes 1 original + 19 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 2
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_42/llm_samples_94_better_spam/samples_2.csv
   Found 40 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 2
   Starting with existing 40 samples
Maximum target samples needed: 40
📁 Using existing dataset for activation extraction
   Debug: final_df has 40 samples, original_df has 2 samples
   Debug: LLM samples (after original) should be 38
📊 Final dataset breakdown:
   - Original samples: 2
   - Total samples in final dataset: 40
   - Generated samples: 38
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 40 samples to: results/spam_gemma_9b/seed_42/llm_samples_94_better_spam/samples_2.csv
   (Includes 2 original + 38 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 3
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_42/llm_samples_94_better_spam/samples_3.csv
   Found 60 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 3
   Starting with existing 60 samples
Maximum target samples needed: 60
📁 Using existing dataset for activation extraction
   Debug: final_df has 60 samples, original_df has 3 samples
   Debug: LLM samples (after original) should be 57
📊 Final dataset breakdown:
   - Original samples: 3
   - Total samples in final dataset: 60
   - Generated samples: 57
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 60 samples to: results/spam_gemma_9b/seed_42/llm_samples_94_better_spam/samples_3.csv
   (Includes 3 original + 57 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 4
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_42/llm_samples_94_better_spam/samples_4.csv
   Found 80 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 4
   Starting with existing 80 samples
Maximum target samples needed: 80
📁 Using existing dataset for activation extraction
   Debug: final_df has 80 samples, original_df has 4 samples
   Debug: LLM samples (after original) should be 76
📊 Final dataset breakdown:
   - Original samples: 4
   - Total samples in final dataset: 80
   - Generated samples: 76
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 80 samples to: results/spam_gemma_9b/seed_42/llm_samples_94_better_spam/samples_4.csv
   (Includes 4 original + 76 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 5
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_42/llm_samples_94_better_spam/samples_5.csv
   Found 100 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 5
   Starting with existing 100 samples
Maximum target samples needed: 100
📁 Using existing dataset for activation extraction
   Debug: final_df has 100 samples, original_df has 5 samples
   Debug: LLM samples (after original) should be 95
📊 Final dataset breakdown:
   - Original samples: 5
   - Total samples in final dataset: 100
   - Generated samples: 95
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 100 samples to: results/spam_gemma_9b/seed_42/llm_samples_94_better_spam/samples_5.csv
   (Includes 5 original + 95 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 10
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_42/llm_samples_94_better_spam/samples_10.csv
   Found 200 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 10
   Starting with existing 200 samples
Maximum target samples needed: 200
📁 Using existing dataset for activation extraction
   Debug: final_df has 200 samples, original_df has 10 samples
   Debug: LLM samples (after original) should be 190
📊 Final dataset breakdown:
   - Original samples: 10
   - Total samples in final dataset: 200
   - Generated samples: 190
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 200 samples to: results/spam_gemma_9b/seed_42/llm_samples_94_better_spam/samples_10.csv
   (Includes 10 original + 190 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

Seed 42 complete! Files saved to: results/spam_gemma_9b/seed_42/llm_samples_94_better_spam

================================================================================
Processing seed 43
================================================================================
Output directory: results/spam_gemma_9b/seed_43/llm_samples_94_better_spam
Extracting samples using seed 43...
Loading dataset: 94_better_spam
No model or model_name provided, not initializing activation manager.
Total positive samples in dataset: 4000
Extracted 1 samples for n_real_samples=1
Extracted 2 samples for n_real_samples=2
Extracted 3 samples for n_real_samples=3
Extracted 4 samples for n_real_samples=4
Extracted 5 samples for n_real_samples=5
Extracted 10 samples for n_real_samples=10

------------------------------------------------------------
Processing n_real_samples = 1
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_43/llm_samples_94_better_spam/samples_1.csv
   Found 20 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 1
   Starting with existing 20 samples
Maximum target samples needed: 20
📁 Using existing dataset for activation extraction
   Debug: final_df has 20 samples, original_df has 1 samples
   Debug: LLM samples (after original) should be 19
📊 Final dataset breakdown:
   - Original samples: 1
   - Total samples in final dataset: 20
   - Generated samples: 19
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 20 samples to: results/spam_gemma_9b/seed_43/llm_samples_94_better_spam/samples_1.csv
   (Includes 1 original + 19 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 2
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_43/llm_samples_94_better_spam/samples_2.csv
   Found 40 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 2
   Starting with existing 40 samples
Maximum target samples needed: 40
📁 Using existing dataset for activation extraction
   Debug: final_df has 40 samples, original_df has 2 samples
   Debug: LLM samples (after original) should be 38
📊 Final dataset breakdown:
   - Original samples: 2
   - Total samples in final dataset: 40
   - Generated samples: 38
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 40 samples to: results/spam_gemma_9b/seed_43/llm_samples_94_better_spam/samples_2.csv
   (Includes 2 original + 38 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 3
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_43/llm_samples_94_better_spam/samples_3.csv
   Found 60 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 3
   Starting with existing 60 samples
Maximum target samples needed: 60
📁 Using existing dataset for activation extraction
   Debug: final_df has 60 samples, original_df has 3 samples
   Debug: LLM samples (after original) should be 57
📊 Final dataset breakdown:
   - Original samples: 3
   - Total samples in final dataset: 60
   - Generated samples: 57
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 60 samples to: results/spam_gemma_9b/seed_43/llm_samples_94_better_spam/samples_3.csv
   (Includes 3 original + 57 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 4
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_43/llm_samples_94_better_spam/samples_4.csv
   Found 80 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 4
   Starting with existing 80 samples
Maximum target samples needed: 80
📁 Using existing dataset for activation extraction
   Debug: final_df has 80 samples, original_df has 4 samples
   Debug: LLM samples (after original) should be 76
📊 Final dataset breakdown:
   - Original samples: 4
   - Total samples in final dataset: 80
   - Generated samples: 76
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 80 samples to: results/spam_gemma_9b/seed_43/llm_samples_94_better_spam/samples_4.csv
   (Includes 4 original + 76 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 5
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_43/llm_samples_94_better_spam/samples_5.csv
   Found 100 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 5
   Starting with existing 100 samples
Maximum target samples needed: 100
📁 Using existing dataset for activation extraction
   Debug: final_df has 100 samples, original_df has 5 samples
   Debug: LLM samples (after original) should be 95
📊 Final dataset breakdown:
   - Original samples: 5
   - Total samples in final dataset: 100
   - Generated samples: 95
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 100 samples to: results/spam_gemma_9b/seed_43/llm_samples_94_better_spam/samples_5.csv
   (Includes 5 original + 95 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 10
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_43/llm_samples_94_better_spam/samples_10.csv
   Found 200 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 10
   Starting with existing 200 samples
Maximum target samples needed: 200
📁 Using existing dataset for activation extraction
   Debug: final_df has 200 samples, original_df has 10 samples
   Debug: LLM samples (after original) should be 190
📊 Final dataset breakdown:
   - Original samples: 10
   - Total samples in final dataset: 200
   - Generated samples: 190
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 200 samples to: results/spam_gemma_9b/seed_43/llm_samples_94_better_spam/samples_10.csv
   (Includes 10 original + 190 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

Seed 43 complete! Files saved to: results/spam_gemma_9b/seed_43/llm_samples_94_better_spam

================================================================================
Processing seed 44
================================================================================
Output directory: results/spam_gemma_9b/seed_44/llm_samples_94_better_spam
Extracting samples using seed 44...
Loading dataset: 94_better_spam
No model or model_name provided, not initializing activation manager.
Total positive samples in dataset: 4000
Extracted 1 samples for n_real_samples=1
Extracted 2 samples for n_real_samples=2
Extracted 3 samples for n_real_samples=3
Extracted 4 samples for n_real_samples=4
Extracted 5 samples for n_real_samples=5
Extracted 10 samples for n_real_samples=10

------------------------------------------------------------
Processing n_real_samples = 1
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_44/llm_samples_94_better_spam/samples_1.csv
   Found 20 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 1
   Starting with existing 20 samples
Maximum target samples needed: 20
📁 Using existing dataset for activation extraction
   Debug: final_df has 20 samples, original_df has 1 samples
   Debug: LLM samples (after original) should be 19
📊 Final dataset breakdown:
   - Original samples: 1
   - Total samples in final dataset: 20
   - Generated samples: 19
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 20 samples to: results/spam_gemma_9b/seed_44/llm_samples_94_better_spam/samples_1.csv
   (Includes 1 original + 19 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 2
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_44/llm_samples_94_better_spam/samples_2.csv
   Found 40 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 2
   Starting with existing 40 samples
Maximum target samples needed: 40
📁 Using existing dataset for activation extraction
   Debug: final_df has 40 samples, original_df has 2 samples
   Debug: LLM samples (after original) should be 38
📊 Final dataset breakdown:
   - Original samples: 2
   - Total samples in final dataset: 40
   - Generated samples: 38
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 40 samples to: results/spam_gemma_9b/seed_44/llm_samples_94_better_spam/samples_2.csv
   (Includes 2 original + 38 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 3
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_44/llm_samples_94_better_spam/samples_3.csv
   Found 60 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 3
   Starting with existing 60 samples
Maximum target samples needed: 60
📁 Using existing dataset for activation extraction
   Debug: final_df has 60 samples, original_df has 3 samples
   Debug: LLM samples (after original) should be 57
📊 Final dataset breakdown:
   - Original samples: 3
   - Total samples in final dataset: 60
   - Generated samples: 57
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 60 samples to: results/spam_gemma_9b/seed_44/llm_samples_94_better_spam/samples_3.csv
   (Includes 3 original + 57 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 4
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_44/llm_samples_94_better_spam/samples_4.csv
   Found 80 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 4
   Starting with existing 80 samples
Maximum target samples needed: 80
📁 Using existing dataset for activation extraction
   Debug: final_df has 80 samples, original_df has 4 samples
   Debug: LLM samples (after original) should be 76
📊 Final dataset breakdown:
   - Original samples: 4
   - Total samples in final dataset: 80
   - Generated samples: 76
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 80 samples to: results/spam_gemma_9b/seed_44/llm_samples_94_better_spam/samples_4.csv
   (Includes 4 original + 76 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 5
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_44/llm_samples_94_better_spam/samples_5.csv
   Found 100 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 5
   Starting with existing 100 samples
Maximum target samples needed: 100
📁 Using existing dataset for activation extraction
   Debug: final_df has 100 samples, original_df has 5 samples
   Debug: LLM samples (after original) should be 95
📊 Final dataset breakdown:
   - Original samples: 5
   - Total samples in final dataset: 100
   - Generated samples: 95
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 100 samples to: results/spam_gemma_9b/seed_44/llm_samples_94_better_spam/samples_5.csv
   (Includes 5 original + 95 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 10
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_44/llm_samples_94_better_spam/samples_10.csv
   Found 200 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 10
   Starting with existing 200 samples
Maximum target samples needed: 200
📁 Using existing dataset for activation extraction
   Debug: final_df has 200 samples, original_df has 10 samples
   Debug: LLM samples (after original) should be 190
📊 Final dataset breakdown:
   - Original samples: 10
   - Total samples in final dataset: 200
   - Generated samples: 190
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 200 samples to: results/spam_gemma_9b/seed_44/llm_samples_94_better_spam/samples_10.csv
   (Includes 10 original + 190 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

Seed 44 complete! Files saved to: results/spam_gemma_9b/seed_44/llm_samples_94_better_spam

================================================================================
Processing seed 45
================================================================================
Output directory: results/spam_gemma_9b/seed_45/llm_samples_94_better_spam
Extracting samples using seed 45...
Loading dataset: 94_better_spam
No model or model_name provided, not initializing activation manager.
Total positive samples in dataset: 4000
Extracted 1 samples for n_real_samples=1
Extracted 2 samples for n_real_samples=2
Extracted 3 samples for n_real_samples=3
Extracted 4 samples for n_real_samples=4
Extracted 5 samples for n_real_samples=5
Extracted 10 samples for n_real_samples=10

------------------------------------------------------------
Processing n_real_samples = 1
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_45/llm_samples_94_better_spam/samples_1.csv
   Found 20 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 1
   Starting with existing 20 samples
Maximum target samples needed: 20
📁 Using existing dataset for activation extraction
   Debug: final_df has 20 samples, original_df has 1 samples
   Debug: LLM samples (after original) should be 19
📊 Final dataset breakdown:
   - Original samples: 1
   - Total samples in final dataset: 20
   - Generated samples: 19
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 20 samples to: results/spam_gemma_9b/seed_45/llm_samples_94_better_spam/samples_1.csv
   (Includes 1 original + 19 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 2
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_45/llm_samples_94_better_spam/samples_2.csv
   Found 40 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 2
   Starting with existing 40 samples
Maximum target samples needed: 40
📁 Using existing dataset for activation extraction
   Debug: final_df has 40 samples, original_df has 2 samples
   Debug: LLM samples (after original) should be 38
📊 Final dataset breakdown:
   - Original samples: 2
   - Total samples in final dataset: 40
   - Generated samples: 38
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 40 samples to: results/spam_gemma_9b/seed_45/llm_samples_94_better_spam/samples_2.csv
   (Includes 2 original + 38 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 3
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_45/llm_samples_94_better_spam/samples_3.csv
   Found 60 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 3
   Starting with existing 60 samples
Maximum target samples needed: 60
📁 Using existing dataset for activation extraction
   Debug: final_df has 60 samples, original_df has 3 samples
   Debug: LLM samples (after original) should be 57
📊 Final dataset breakdown:
   - Original samples: 3
   - Total samples in final dataset: 60
   - Generated samples: 57
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 60 samples to: results/spam_gemma_9b/seed_45/llm_samples_94_better_spam/samples_3.csv
   (Includes 3 original + 57 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 4
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_45/llm_samples_94_better_spam/samples_4.csv
   Found 80 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 4
   Starting with existing 80 samples
Maximum target samples needed: 80
📁 Using existing dataset for activation extraction
   Debug: final_df has 80 samples, original_df has 4 samples
   Debug: LLM samples (after original) should be 76
📊 Final dataset breakdown:
   - Original samples: 4
   - Total samples in final dataset: 80
   - Generated samples: 76
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 80 samples to: results/spam_gemma_9b/seed_45/llm_samples_94_better_spam/samples_4.csv
   (Includes 4 original + 76 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 5
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_45/llm_samples_94_better_spam/samples_5.csv
   Found 100 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 5
   Starting with existing 100 samples
Maximum target samples needed: 100
📁 Using existing dataset for activation extraction
   Debug: final_df has 100 samples, original_df has 5 samples
   Debug: LLM samples (after original) should be 95
📊 Final dataset breakdown:
   - Original samples: 5
   - Total samples in final dataset: 100
   - Generated samples: 95
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 100 samples to: results/spam_gemma_9b/seed_45/llm_samples_94_better_spam/samples_5.csv
   (Includes 5 original + 95 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 10
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_45/llm_samples_94_better_spam/samples_10.csv
   Found 200 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 10
   Starting with existing 200 samples
Maximum target samples needed: 200
📁 Using existing dataset for activation extraction
   Debug: final_df has 200 samples, original_df has 10 samples
   Debug: LLM samples (after original) should be 190
📊 Final dataset breakdown:
   - Original samples: 10
   - Total samples in final dataset: 200
   - Generated samples: 190
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 200 samples to: results/spam_gemma_9b/seed_45/llm_samples_94_better_spam/samples_10.csv
   (Includes 10 original + 190 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

Seed 45 complete! Files saved to: results/spam_gemma_9b/seed_45/llm_samples_94_better_spam

================================================================================
Processing seed 46
================================================================================
Output directory: results/spam_gemma_9b/seed_46/llm_samples_94_better_spam
Extracting samples using seed 46...
Loading dataset: 94_better_spam
No model or model_name provided, not initializing activation manager.
Total positive samples in dataset: 4000
Extracted 1 samples for n_real_samples=1
Extracted 2 samples for n_real_samples=2
Extracted 3 samples for n_real_samples=3
Extracted 4 samples for n_real_samples=4
Extracted 5 samples for n_real_samples=5
Extracted 10 samples for n_real_samples=10

------------------------------------------------------------
Processing n_real_samples = 1
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_46/llm_samples_94_better_spam/samples_1.csv
   Found 20 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 1
   Starting with existing 20 samples
Maximum target samples needed: 20
📁 Using existing dataset for activation extraction
   Debug: final_df has 20 samples, original_df has 1 samples
   Debug: LLM samples (after original) should be 19
📊 Final dataset breakdown:
   - Original samples: 1
   - Total samples in final dataset: 20
   - Generated samples: 19
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 20 samples to: results/spam_gemma_9b/seed_46/llm_samples_94_better_spam/samples_1.csv
   (Includes 1 original + 19 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 2
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_46/llm_samples_94_better_spam/samples_2.csv
   Found 40 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 2
   Starting with existing 40 samples
Maximum target samples needed: 40
📁 Using existing dataset for activation extraction
   Debug: final_df has 40 samples, original_df has 2 samples
   Debug: LLM samples (after original) should be 38
📊 Final dataset breakdown:
   - Original samples: 2
   - Total samples in final dataset: 40
   - Generated samples: 38
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 40 samples to: results/spam_gemma_9b/seed_46/llm_samples_94_better_spam/samples_2.csv
   (Includes 2 original + 38 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 3
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_46/llm_samples_94_better_spam/samples_3.csv
   Found 60 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 3
   Starting with existing 60 samples
Maximum target samples needed: 60
📁 Using existing dataset for activation extraction
   Debug: final_df has 60 samples, original_df has 3 samples
   Debug: LLM samples (after original) should be 57
📊 Final dataset breakdown:
   - Original samples: 3
   - Total samples in final dataset: 60
   - Generated samples: 57
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 60 samples to: results/spam_gemma_9b/seed_46/llm_samples_94_better_spam/samples_3.csv
   (Includes 3 original + 57 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 4
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_46/llm_samples_94_better_spam/samples_4.csv
   Found 80 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 4
   Starting with existing 80 samples
Maximum target samples needed: 80
📁 Using existing dataset for activation extraction
   Debug: final_df has 80 samples, original_df has 4 samples
   Debug: LLM samples (after original) should be 76
📊 Final dataset breakdown:
   - Original samples: 4
   - Total samples in final dataset: 80
   - Generated samples: 76
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 80 samples to: results/spam_gemma_9b/seed_46/llm_samples_94_better_spam/samples_4.csv
   (Includes 4 original + 76 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 5
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_46/llm_samples_94_better_spam/samples_5.csv
   Found 100 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 5
   Starting with existing 100 samples
Maximum target samples needed: 100
📁 Using existing dataset for activation extraction
   Debug: final_df has 100 samples, original_df has 5 samples
   Debug: LLM samples (after original) should be 95
📊 Final dataset breakdown:
   - Original samples: 5
   - Total samples in final dataset: 100
   - Generated samples: 95
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 100 samples to: results/spam_gemma_9b/seed_46/llm_samples_94_better_spam/samples_5.csv
   (Includes 5 original + 95 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 10
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_46/llm_samples_94_better_spam/samples_10.csv
   Found 200 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 10
   Starting with existing 200 samples
Maximum target samples needed: 200
📁 Using existing dataset for activation extraction
   Debug: final_df has 200 samples, original_df has 10 samples
   Debug: LLM samples (after original) should be 190
📊 Final dataset breakdown:
   - Original samples: 10
   - Total samples in final dataset: 200
   - Generated samples: 190
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 200 samples to: results/spam_gemma_9b/seed_46/llm_samples_94_better_spam/samples_10.csv
   (Includes 10 original + 190 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

Seed 46 complete! Files saved to: results/spam_gemma_9b/seed_46/llm_samples_94_better_spam

================================================================================
Processing seed 47
================================================================================
Output directory: results/spam_gemma_9b/seed_47/llm_samples_94_better_spam
Extracting samples using seed 47...
Loading dataset: 94_better_spam
No model or model_name provided, not initializing activation manager.
Total positive samples in dataset: 4000
Extracted 1 samples for n_real_samples=1
Extracted 2 samples for n_real_samples=2
Extracted 3 samples for n_real_samples=3
Extracted 4 samples for n_real_samples=4
Extracted 5 samples for n_real_samples=5
Extracted 10 samples for n_real_samples=10

------------------------------------------------------------
Processing n_real_samples = 1
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_47/llm_samples_94_better_spam/samples_1.csv
   Found 20 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 1
   Starting with existing 20 samples
Maximum target samples needed: 20
📁 Using existing dataset for activation extraction
   Debug: final_df has 20 samples, original_df has 1 samples
   Debug: LLM samples (after original) should be 19
📊 Final dataset breakdown:
   - Original samples: 1
   - Total samples in final dataset: 20
   - Generated samples: 19
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 20 samples to: results/spam_gemma_9b/seed_47/llm_samples_94_better_spam/samples_1.csv
   (Includes 1 original + 19 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 2
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_47/llm_samples_94_better_spam/samples_2.csv
   Found 40 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 2
   Starting with existing 40 samples
Maximum target samples needed: 40
📁 Using existing dataset for activation extraction
   Debug: final_df has 40 samples, original_df has 2 samples
   Debug: LLM samples (after original) should be 38
📊 Final dataset breakdown:
   - Original samples: 2
   - Total samples in final dataset: 40
   - Generated samples: 38
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 40 samples to: results/spam_gemma_9b/seed_47/llm_samples_94_better_spam/samples_2.csv
   (Includes 2 original + 38 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 3
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_47/llm_samples_94_better_spam/samples_3.csv
   Found 60 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 3
   Starting with existing 60 samples
Maximum target samples needed: 60
📁 Using existing dataset for activation extraction
   Debug: final_df has 60 samples, original_df has 3 samples
   Debug: LLM samples (after original) should be 57
📊 Final dataset breakdown:
   - Original samples: 3
   - Total samples in final dataset: 60
   - Generated samples: 57
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 60 samples to: results/spam_gemma_9b/seed_47/llm_samples_94_better_spam/samples_3.csv
   (Includes 3 original + 57 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 4
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_47/llm_samples_94_better_spam/samples_4.csv
   Found 80 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 4
   Starting with existing 80 samples
Maximum target samples needed: 80
📁 Using existing dataset for activation extraction
   Debug: final_df has 80 samples, original_df has 4 samples
   Debug: LLM samples (after original) should be 76
📊 Final dataset breakdown:
   - Original samples: 4
   - Total samples in final dataset: 80
   - Generated samples: 76
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 80 samples to: results/spam_gemma_9b/seed_47/llm_samples_94_better_spam/samples_4.csv
   (Includes 4 original + 76 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 5
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_47/llm_samples_94_better_spam/samples_5.csv
   Found 100 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 5
   Starting with existing 100 samples
Maximum target samples needed: 100
📁 Using existing dataset for activation extraction
   Debug: final_df has 100 samples, original_df has 5 samples
   Debug: LLM samples (after original) should be 95
📊 Final dataset breakdown:
   - Original samples: 5
   - Total samples in final dataset: 100
   - Generated samples: 95
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 100 samples to: results/spam_gemma_9b/seed_47/llm_samples_94_better_spam/samples_5.csv
   (Includes 5 original + 95 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 10
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_47/llm_samples_94_better_spam/samples_10.csv
   Found 200 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 10
   Starting with existing 200 samples
Maximum target samples needed: 200
📁 Using existing dataset for activation extraction
   Debug: final_df has 200 samples, original_df has 10 samples
   Debug: LLM samples (after original) should be 190
📊 Final dataset breakdown:
   - Original samples: 10
   - Total samples in final dataset: 200
   - Generated samples: 190
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 200 samples to: results/spam_gemma_9b/seed_47/llm_samples_94_better_spam/samples_10.csv
   (Includes 10 original + 190 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

Seed 47 complete! Files saved to: results/spam_gemma_9b/seed_47/llm_samples_94_better_spam

================================================================================
Processing seed 48
================================================================================
Output directory: results/spam_gemma_9b/seed_48/llm_samples_94_better_spam
Extracting samples using seed 48...
Loading dataset: 94_better_spam
No model or model_name provided, not initializing activation manager.
Total positive samples in dataset: 4000
Extracted 1 samples for n_real_samples=1
Extracted 2 samples for n_real_samples=2
Extracted 3 samples for n_real_samples=3
Extracted 4 samples for n_real_samples=4
Extracted 5 samples for n_real_samples=5
Extracted 10 samples for n_real_samples=10

------------------------------------------------------------
Processing n_real_samples = 1
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_48/llm_samples_94_better_spam/samples_1.csv
   Found 20 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 1
   Starting with existing 20 samples
Maximum target samples needed: 20
📁 Using existing dataset for activation extraction
   Debug: final_df has 20 samples, original_df has 1 samples
   Debug: LLM samples (after original) should be 19
📊 Final dataset breakdown:
   - Original samples: 1
   - Total samples in final dataset: 20
   - Generated samples: 19
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 20 samples to: results/spam_gemma_9b/seed_48/llm_samples_94_better_spam/samples_1.csv
   (Includes 1 original + 19 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 2
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_48/llm_samples_94_better_spam/samples_2.csv
   Found 40 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 2
   Starting with existing 40 samples
Maximum target samples needed: 40
📁 Using existing dataset for activation extraction
   Debug: final_df has 40 samples, original_df has 2 samples
   Debug: LLM samples (after original) should be 38
📊 Final dataset breakdown:
   - Original samples: 2
   - Total samples in final dataset: 40
   - Generated samples: 38
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 40 samples to: results/spam_gemma_9b/seed_48/llm_samples_94_better_spam/samples_2.csv
   (Includes 2 original + 38 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 3
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_48/llm_samples_94_better_spam/samples_3.csv
   Found 60 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 3
   Starting with existing 60 samples
Maximum target samples needed: 60
📁 Using existing dataset for activation extraction
   Debug: final_df has 60 samples, original_df has 3 samples
   Debug: LLM samples (after original) should be 57
📊 Final dataset breakdown:
   - Original samples: 3
   - Total samples in final dataset: 60
   - Generated samples: 57
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 60 samples to: results/spam_gemma_9b/seed_48/llm_samples_94_better_spam/samples_3.csv
   (Includes 3 original + 57 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 4
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_48/llm_samples_94_better_spam/samples_4.csv
   Found 80 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 4
   Starting with existing 80 samples
Maximum target samples needed: 80
📁 Using existing dataset for activation extraction
   Debug: final_df has 80 samples, original_df has 4 samples
   Debug: LLM samples (after original) should be 76
📊 Final dataset breakdown:
   - Original samples: 4
   - Total samples in final dataset: 80
   - Generated samples: 76
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 80 samples to: results/spam_gemma_9b/seed_48/llm_samples_94_better_spam/samples_4.csv
   (Includes 4 original + 76 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 5
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_48/llm_samples_94_better_spam/samples_5.csv
   Found 100 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 5
   Starting with existing 100 samples
Maximum target samples needed: 100
📁 Using existing dataset for activation extraction
   Debug: final_df has 100 samples, original_df has 5 samples
   Debug: LLM samples (after original) should be 95
📊 Final dataset breakdown:
   - Original samples: 5
   - Total samples in final dataset: 100
   - Generated samples: 95
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 100 samples to: results/spam_gemma_9b/seed_48/llm_samples_94_better_spam/samples_5.csv
   (Includes 5 original + 95 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 10
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_48/llm_samples_94_better_spam/samples_10.csv
   Found 200 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 10
   Starting with existing 200 samples
Maximum target samples needed: 200
📁 Using existing dataset for activation extraction
   Debug: final_df has 200 samples, original_df has 10 samples
   Debug: LLM samples (after original) should be 190
📊 Final dataset breakdown:
   - Original samples: 10
   - Total samples in final dataset: 200
   - Generated samples: 190
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 200 samples to: results/spam_gemma_9b/seed_48/llm_samples_94_better_spam/samples_10.csv
   (Includes 10 original + 190 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

Seed 48 complete! Files saved to: results/spam_gemma_9b/seed_48/llm_samples_94_better_spam

================================================================================
Processing seed 49
================================================================================
Output directory: results/spam_gemma_9b/seed_49/llm_samples_94_better_spam
Extracting samples using seed 49...
Loading dataset: 94_better_spam
No model or model_name provided, not initializing activation manager.
Total positive samples in dataset: 4000
Extracted 1 samples for n_real_samples=1
Extracted 2 samples for n_real_samples=2
Extracted 3 samples for n_real_samples=3
Extracted 4 samples for n_real_samples=4
Extracted 5 samples for n_real_samples=5
Extracted 10 samples for n_real_samples=10

------------------------------------------------------------
Processing n_real_samples = 1
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_49/llm_samples_94_better_spam/samples_1.csv
   Found 20 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 1
   Starting with existing 20 samples
Maximum target samples needed: 20
📁 Using existing dataset for activation extraction
   Debug: final_df has 20 samples, original_df has 1 samples
   Debug: LLM samples (after original) should be 19
📊 Final dataset breakdown:
   - Original samples: 1
   - Total samples in final dataset: 20
   - Generated samples: 19
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 20 samples to: results/spam_gemma_9b/seed_49/llm_samples_94_better_spam/samples_1.csv
   (Includes 1 original + 19 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 2
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_49/llm_samples_94_better_spam/samples_2.csv
   Found 40 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 2
   Starting with existing 40 samples
Maximum target samples needed: 40
📁 Using existing dataset for activation extraction
   Debug: final_df has 40 samples, original_df has 2 samples
   Debug: LLM samples (after original) should be 38
📊 Final dataset breakdown:
   - Original samples: 2
   - Total samples in final dataset: 40
   - Generated samples: 38
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 40 samples to: results/spam_gemma_9b/seed_49/llm_samples_94_better_spam/samples_2.csv
   (Includes 2 original + 38 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 3
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_49/llm_samples_94_better_spam/samples_3.csv
   Found 60 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 3
   Starting with existing 60 samples
Maximum target samples needed: 60
📁 Using existing dataset for activation extraction
   Debug: final_df has 60 samples, original_df has 3 samples
   Debug: LLM samples (after original) should be 57
📊 Final dataset breakdown:
   - Original samples: 3
   - Total samples in final dataset: 60
   - Generated samples: 57
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 60 samples to: results/spam_gemma_9b/seed_49/llm_samples_94_better_spam/samples_3.csv
   (Includes 3 original + 57 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 4
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_49/llm_samples_94_better_spam/samples_4.csv
   Found 80 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 4
   Starting with existing 80 samples
Maximum target samples needed: 80
📁 Using existing dataset for activation extraction
   Debug: final_df has 80 samples, original_df has 4 samples
   Debug: LLM samples (after original) should be 76
📊 Final dataset breakdown:
   - Original samples: 4
   - Total samples in final dataset: 80
   - Generated samples: 76
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 80 samples to: results/spam_gemma_9b/seed_49/llm_samples_94_better_spam/samples_4.csv
   (Includes 4 original + 76 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 5
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_49/llm_samples_94_better_spam/samples_5.csv
   Found 100 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 5
   Starting with existing 100 samples
Maximum target samples needed: 100
📁 Using existing dataset for activation extraction
   Debug: final_df has 100 samples, original_df has 5 samples
   Debug: LLM samples (after original) should be 95
📊 Final dataset breakdown:
   - Original samples: 5
   - Total samples in final dataset: 100
   - Generated samples: 95
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 100 samples to: results/spam_gemma_9b/seed_49/llm_samples_94_better_spam/samples_5.csv
   (Includes 5 original + 95 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 10
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_49/llm_samples_94_better_spam/samples_10.csv
   Found 200 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 10
   Starting with existing 200 samples
Maximum target samples needed: 200
📁 Using existing dataset for activation extraction
   Debug: final_df has 200 samples, original_df has 10 samples
   Debug: LLM samples (after original) should be 190
📊 Final dataset breakdown:
   - Original samples: 10
   - Total samples in final dataset: 200
   - Generated samples: 190
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 200 samples to: results/spam_gemma_9b/seed_49/llm_samples_94_better_spam/samples_10.csv
   (Includes 10 original + 190 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

Seed 49 complete! Files saved to: results/spam_gemma_9b/seed_49/llm_samples_94_better_spam

================================================================================
Processing seed 50
================================================================================
Output directory: results/spam_gemma_9b/seed_50/llm_samples_94_better_spam
Extracting samples using seed 50...
Loading dataset: 94_better_spam
No model or model_name provided, not initializing activation manager.
Total positive samples in dataset: 4000
Extracted 1 samples for n_real_samples=1
Extracted 2 samples for n_real_samples=2
Extracted 3 samples for n_real_samples=3
Extracted 4 samples for n_real_samples=4
Extracted 5 samples for n_real_samples=5
Extracted 10 samples for n_real_samples=10

------------------------------------------------------------
Processing n_real_samples = 1
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_50/llm_samples_94_better_spam/samples_1.csv
   Found 20 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 1
   Starting with existing 20 samples
Maximum target samples needed: 20
📁 Using existing dataset for activation extraction
   Debug: final_df has 20 samples, original_df has 1 samples
   Debug: LLM samples (after original) should be 19
📊 Final dataset breakdown:
   - Original samples: 1
   - Total samples in final dataset: 20
   - Generated samples: 19
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 20 samples to: results/spam_gemma_9b/seed_50/llm_samples_94_better_spam/samples_1.csv
   (Includes 1 original + 19 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 2
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_50/llm_samples_94_better_spam/samples_2.csv
   Found 40 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 2
   Starting with existing 40 samples
Maximum target samples needed: 40
📁 Using existing dataset for activation extraction
   Debug: final_df has 40 samples, original_df has 2 samples
   Debug: LLM samples (after original) should be 38
📊 Final dataset breakdown:
   - Original samples: 2
   - Total samples in final dataset: 40
   - Generated samples: 38
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 40 samples to: results/spam_gemma_9b/seed_50/llm_samples_94_better_spam/samples_2.csv
   (Includes 2 original + 38 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 3
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_50/llm_samples_94_better_spam/samples_3.csv
   Found 60 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 3
   Starting with existing 60 samples
Maximum target samples needed: 60
📁 Using existing dataset for activation extraction
   Debug: final_df has 60 samples, original_df has 3 samples
   Debug: LLM samples (after original) should be 57
📊 Final dataset breakdown:
   - Original samples: 3
   - Total samples in final dataset: 60
   - Generated samples: 57
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 60 samples to: results/spam_gemma_9b/seed_50/llm_samples_94_better_spam/samples_3.csv
   (Includes 3 original + 57 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 4
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_50/llm_samples_94_better_spam/samples_4.csv
   Found 80 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 4
   Starting with existing 80 samples
Maximum target samples needed: 80
📁 Using existing dataset for activation extraction
   Debug: final_df has 80 samples, original_df has 4 samples
   Debug: LLM samples (after original) should be 76
📊 Final dataset breakdown:
   - Original samples: 4
   - Total samples in final dataset: 80
   - Generated samples: 76
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 80 samples to: results/spam_gemma_9b/seed_50/llm_samples_94_better_spam/samples_4.csv
   (Includes 4 original + 76 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 5
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_50/llm_samples_94_better_spam/samples_5.csv
   Found 100 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 5
   Starting with existing 100 samples
Maximum target samples needed: 100
📁 Using existing dataset for activation extraction
   Debug: final_df has 100 samples, original_df has 5 samples
   Debug: LLM samples (after original) should be 95
📊 Final dataset breakdown:
   - Original samples: 5
   - Total samples in final dataset: 100
   - Generated samples: 95
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 100 samples to: results/spam_gemma_9b/seed_50/llm_samples_94_better_spam/samples_5.csv
   (Includes 5 original + 95 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 10
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_50/llm_samples_94_better_spam/samples_10.csv
   Found 200 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 10
   Starting with existing 200 samples
Maximum target samples needed: 200
📁 Using existing dataset for activation extraction
   Debug: final_df has 200 samples, original_df has 10 samples
   Debug: LLM samples (after original) should be 190
📊 Final dataset breakdown:
   - Original samples: 10
   - Total samples in final dataset: 200
   - Generated samples: 190
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 200 samples to: results/spam_gemma_9b/seed_50/llm_samples_94_better_spam/samples_10.csv
   (Includes 10 original + 190 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

Seed 50 complete! Files saved to: results/spam_gemma_9b/seed_50/llm_samples_94_better_spam

================================================================================
Processing seed 51
================================================================================
Output directory: results/spam_gemma_9b/seed_51/llm_samples_94_better_spam
Extracting samples using seed 51...
Loading dataset: 94_better_spam
No model or model_name provided, not initializing activation manager.
Total positive samples in dataset: 4000
Extracted 1 samples for n_real_samples=1
Extracted 2 samples for n_real_samples=2
Extracted 3 samples for n_real_samples=3
Extracted 4 samples for n_real_samples=4
Extracted 5 samples for n_real_samples=5
Extracted 10 samples for n_real_samples=10

------------------------------------------------------------
Processing n_real_samples = 1
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_51/llm_samples_94_better_spam/samples_1.csv
   Found 20 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 1
   Starting with existing 20 samples
Maximum target samples needed: 20
📁 Using existing dataset for activation extraction
   Debug: final_df has 20 samples, original_df has 1 samples
   Debug: LLM samples (after original) should be 19
📊 Final dataset breakdown:
   - Original samples: 1
   - Total samples in final dataset: 20
   - Generated samples: 19
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 20 samples to: results/spam_gemma_9b/seed_51/llm_samples_94_better_spam/samples_1.csv
   (Includes 1 original + 19 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 2
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_51/llm_samples_94_better_spam/samples_2.csv
   Found 40 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 2
   Starting with existing 40 samples
Maximum target samples needed: 40
📁 Using existing dataset for activation extraction
   Debug: final_df has 40 samples, original_df has 2 samples
   Debug: LLM samples (after original) should be 38
📊 Final dataset breakdown:
   - Original samples: 2
   - Total samples in final dataset: 40
   - Generated samples: 38
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 40 samples to: results/spam_gemma_9b/seed_51/llm_samples_94_better_spam/samples_2.csv
   (Includes 2 original + 38 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 3
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_51/llm_samples_94_better_spam/samples_3.csv
   Found 60 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 3
   Starting with existing 60 samples
Maximum target samples needed: 60
📁 Using existing dataset for activation extraction
   Debug: final_df has 60 samples, original_df has 3 samples
   Debug: LLM samples (after original) should be 57
📊 Final dataset breakdown:
   - Original samples: 3
   - Total samples in final dataset: 60
   - Generated samples: 57
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 60 samples to: results/spam_gemma_9b/seed_51/llm_samples_94_better_spam/samples_3.csv
   (Includes 3 original + 57 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 4
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_51/llm_samples_94_better_spam/samples_4.csv
   Found 80 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 4
   Starting with existing 80 samples
Maximum target samples needed: 80
📁 Using existing dataset for activation extraction
   Debug: final_df has 80 samples, original_df has 4 samples
   Debug: LLM samples (after original) should be 76
📊 Final dataset breakdown:
   - Original samples: 4
   - Total samples in final dataset: 80
   - Generated samples: 76
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 80 samples to: results/spam_gemma_9b/seed_51/llm_samples_94_better_spam/samples_4.csv
   (Includes 4 original + 76 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 5
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_51/llm_samples_94_better_spam/samples_5.csv
   Found 100 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 5
   Starting with existing 100 samples
Maximum target samples needed: 100
📁 Using existing dataset for activation extraction
   Debug: final_df has 100 samples, original_df has 5 samples
   Debug: LLM samples (after original) should be 95
📊 Final dataset breakdown:
   - Original samples: 5
   - Total samples in final dataset: 100
   - Generated samples: 95
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 100 samples to: results/spam_gemma_9b/seed_51/llm_samples_94_better_spam/samples_5.csv
   (Includes 5 original + 95 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

------------------------------------------------------------
Processing n_real_samples = 10
------------------------------------------------------------
📁 Loading existing samples file: results/spam_gemma_9b/seed_51/llm_samples_94_better_spam/samples_10.csv
   Found 200 existing samples
   Current upsampling factor: 20x
✅ Already have enough samples for 20x upsampling
   Skipping LLM generation for n_real_samples = 10
   Starting with existing 200 samples
Maximum target samples needed: 200
📁 Using existing dataset for activation extraction
   Debug: final_df has 200 samples, original_df has 10 samples
   Debug: LLM samples (after original) should be 190
📊 Final dataset breakdown:
   - Original samples: 10
   - Total samples in final dataset: 200
   - Generated samples: 190
   - Highest upsampling factor: 20x
✅ Saved complete dataset with 200 samples to: results/spam_gemma_9b/seed_51/llm_samples_94_better_spam/samples_10.csv
   (Includes 10 original + 190 generated samples)
🎉 Successfully completed all upsampling factors up to 20x

Seed 51 complete! Files saved to: results/spam_gemma_9b/seed_51/llm_samples_94_better_spam

================================================================================
LLM upsampling complete for all seeds!
Results saved to: results/spam_gemma_9b/seed_*/llm_samples/
Upsampling factors processed: [1, 2, 3, 4, 5, 10, 20]
================================================================================
=== LLM upsampling complete ===
Loading model 'google/gemma-2-9b' for activation extraction...
Loading checkpoint shards:   0%|                          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|██▎               | 1/8 [00:00<00:04,  1.75it/s]Loading checkpoint shards:  25%|████▌             | 2/8 [00:01<00:03,  1.76it/s]Loading checkpoint shards:  38%|██████▊           | 3/8 [00:01<00:02,  1.77it/s]Loading checkpoint shards:  50%|█████████         | 4/8 [00:02<00:02,  1.77it/s]Loading checkpoint shards:  62%|███████████▎      | 5/8 [00:02<00:01,  1.78it/s]Loading checkpoint shards:  75%|█████████████▌    | 6/8 [00:03<00:01,  1.77it/s]Loading checkpoint shards:  88%|███████████████▊  | 7/8 [00:03<00:00,  1.77it/s]Loading checkpoint shards: 100%|██████████████████| 8/8 [00:04<00:00,  2.13it/s]Loading checkpoint shards: 100%|██████████████████| 8/8 [00:04<00:00,  1.90it/s]

========================= ACTIVATION EXTRACTION PHASE =========================
  - Extracting activations for 87_is_spam, L20, resid_post (on_policy=False)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: Gemma2ForCausalLM(
  (model): Gemma2Model(
    (embed_tokens): Embedding(256000, 3584, padding_idx=0)
    (layers): ModuleList(
      (0-41): 42 x Gemma2DecoderLayer(
        (self_attn): Gemma2Attention(
          (q_proj): Linear(in_features=3584, out_features=4096, bias=False)
          (k_proj): Linear(in_features=3584, out_features=2048, bias=False)
          (v_proj): Linear(in_features=3584, out_features=2048, bias=False)
          (o_proj): Linear(in_features=4096, out_features=3584, bias=False)
        )
        (mlp): Gemma2MLP(
          (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)
          (up_proj): Linear(in_features=3584, out_features=14336, bias=False)
          (down_proj): Linear(in_features=14336, out_features=3584, bias=False)
          (act_fn): PytorchGELUTanh()
        )
        (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)
        (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)
        (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)
        (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)
      )
    )
    (norm): Gemma2RMSNorm((3584,), eps=1e-06)
    (rotary_emb): Gemma2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3584, out_features=256000, bias=False)
)
[DEBUG] model type: <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'>
Successfully initialized ActivationManager
    - Off-policy: extracting activations from prompts...
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 5157
[DEBUG] Processing 5157 full activations
[DEBUG] Estimated memory requirement: 1.63 GB
[WARNING] Large memory requirement (1.63 GB).
[DEBUG] Max sequence length: 237
[DEBUG] Shape of first activation: (1, 27, 3584)
[DEBUG] Final array shape: (5157, 237, 3584)
[DEBUG] Returning (5157, 237, 3584) activations
    - Cached 0 new activations (dataset)
    - Aggregated activations are up-to-date
  - Extracting activations for 94_better_spam, L20, resid_post (on_policy=False)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: Gemma2ForCausalLM(
  (model): Gemma2Model(
    (embed_tokens): Embedding(256000, 3584, padding_idx=0)
    (layers): ModuleList(
      (0-41): 42 x Gemma2DecoderLayer(
        (self_attn): Gemma2Attention(
          (q_proj): Linear(in_features=3584, out_features=4096, bias=False)
          (k_proj): Linear(in_features=3584, out_features=2048, bias=False)
          (v_proj): Linear(in_features=3584, out_features=2048, bias=False)
          (o_proj): Linear(in_features=4096, out_features=3584, bias=False)
        )
        (mlp): Gemma2MLP(
          (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)
          (up_proj): Linear(in_features=3584, out_features=14336, bias=False)
          (down_proj): Linear(in_features=14336, out_features=3584, bias=False)
          (act_fn): PytorchGELUTanh()
        )
        (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)
        (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)
        (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)
        (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)
      )
    )
    (norm): Gemma2RMSNorm((3584,), eps=1e-06)
    (rotary_emb): Gemma2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3584, out_features=256000, bias=False)
)
[DEBUG] model type: <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'>
Successfully initialized ActivationManager
    - Off-policy: extracting activations from prompts...
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 8000 full activations
[DEBUG] Estimated memory requirement: 19.66 GB
[WARNING] Large memory requirement (19.66 GB).
[DEBUG] Max sequence length: 1167
[DEBUG] Shape of first activation: (1, 47, 3584)
[DEBUG] Final array shape: (8000, 1167, 3584)
[DEBUG] Returning (8000, 1167, 3584) activations
    - Cached 0 new activations (dataset)
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 5000 full activations
[DEBUG] Estimated memory requirement: 5.41 GB
[WARNING] Large memory requirement (5.41 GB).
[DEBUG] Max sequence length: 935
[DEBUG] Shape of first activation: (1, 252, 3584)
[DEBUG] Final array shape: (5000, 935, 3584)
[DEBUG] Returning (5000, 935, 3584) activations
    - Cached 0 new activations (LLM)
    - Aggregated activations are up-to-date

========================= MODEL UNLOADING PHASE =========================
Unloading model to free GPU memory for parallel processing...
Model unloaded successfully

========================= JOB CREATION PHASE =========================
Created 6300 probe jobs

========================= PROBE PROCESSING PHASE (BATCHED) =========================
Processing 6300 probe jobs in batched groups...
Running 6300 jobs in 700 groups (batched)

=== Group 1/700 — ('94_better_spam', 20, 'resid_post', 42, '__none__', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
Split data: 2682 train, 358 val, 537 test
[DEBUG] Class distribution — train: {0: 2146, 1: 536}, val: {0: 287, 1: 71}, test: {0: 430, 1: 107}
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Split data (only_test): 0 train, 0 val, 5157 test (non-balanced)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_262k_l0_259_last
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_262k_l0_259_last_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_262k_l0_259_softmax
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_262k_l0_259_softmax_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: attention
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: full

=== Group 6/700 — ('94_better_spam', 20, 'resid_post', 47, '__none__', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
Split data: 2682 train, 358 val, 537 test
[DEBUG] Class distribution — train: {0: 2146, 1: 536}, val: {0: 287, 1: 71}, test: {0: 430, 1: 107}
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Split data (only_test): 0 train, 0 val, 5157 test (non-balanced)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean

=== Group 10/700 — ('94_better_spam', 20, 'resid_post', 51, '__none__', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
Split data: 2682 train, 358 val, 537 test
[DEBUG] Class distribution — train: {0: 2146, 1: 536}, val: {0: 287, 1: 71}, test: {0: 430, 1: 107}
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Split data (only_test): 0 train, 0 val, 5157 test (non-balanced)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (2682, 3584)
[DEBUG] Returning (2682, 3584) activations
[DEBUG] train activations shape: (2682, 3584), dtype: float16
[DEBUG] train activations range: [-1526.000000, 117.875000]
Train activations: (2682, 3584)
[DEBUG] Pre-fit sample counts — X: 2682, y: 2682
[DEBUG] y_train class distribution: {0: 2146, 1: 536}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_mean

=== Group 2/700 — ('94_better_spam', 20, 'resid_post', 43, '__none__', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
Split data: 2682 train, 358 val, 537 test
[DEBUG] Class distribution — train: {0: 2146, 1: 536}, val: {0: 287, 1: 71}, test: {0: 430, 1: 107}
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Split data (only_test): 0 train, 0 val, 5157 test (non-balanced)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set

=== Group 17/700 — ('94_better_spam', 20, 'resid_post', 48, '{"class_counts": {"0": 1750, "1": 1}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean

=== Group 14/700 — ('94_better_spam', 20, 'resid_post', 45, '{"class_counts": {"0": 1750, "1": 1}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax

=== Group 18/700 — ('94_better_spam', 20, 'resid_post', 49, '{"class_counts": {"0": 1750, "1": 1}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean

=== Group 32/700 — ('94_better_spam', 20, 'resid_post', 43, '{"class_counts": {"0": 1750, "1": 5}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set

=== Group 27/700 — ('94_better_spam', 20, 'resid_post', 48, '{"class_counts": {"0": 1750, "1": 2}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1751, 3584)
[DEBUG] Returning (1751, 3584) activations
[DEBUG] train activations shape: (1751, 3584), dtype: float16
[DEBUG] train activations range: [-1526.000000, 117.875000]
Train activations: (1751, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_mean
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1751, 3584)
[DEBUG] Returning (1751, 3584) activations
[DEBUG] train activations shape: (1751, 3584), dtype: float16
[DEBUG] train activations range: [-1337.000000, 237.000000]
Train activations: (1751, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_softmax

=== Group 8/700 — ('94_better_spam', 20, 'resid_post', 49, '__none__', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
Split data: 2682 train, 358 val, 537 test
[DEBUG] Class distribution — train: {0: 2146, 1: 536}, val: {0: 287, 1: 71}, test: {0: 430, 1: 107}
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Split data (only_test): 0 train, 0 val, 5157 test (non-balanced)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean

=== Group 24/700 — ('94_better_spam', 20, 'resid_post', 45, '{"class_counts": {"0": 1750, "1": 2}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set

=== Group 25/700 — ('94_better_spam', 20, 'resid_post', 46, '{"class_counts": {"0": 1750, "1": 2}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (1751, 3584)
[DEBUG] Returning (1751, 3584) activations
[DEBUG] train activations shape: (1751, 3584), dtype: float16
[DEBUG] train activations range: [-1526.000000, 117.875000]
Train activations: (1751, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_mean
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1752, 3584)
[DEBUG] Returning (1752, 3584) activations
[DEBUG] train activations shape: (1752, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 231.875000]
Train activations: (1752, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_last
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (537, 3584)
[DEBUG] Returning (537, 3584) activations
[DEBUG] test activations shape: (537, 3584), dtype: float16
[DEBUG] test activations range: [-1043.000000, 73.000000]
Test activations: (537, 3584)
  - 🤗 Calculating metrics...

=== Group 21/700 — ('94_better_spam', 20, 'resid_post', 42, '{"class_counts": {"0": 1750, "1": 2}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  [DEBUG] Fitting SAE probe normally...
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (2682, 3584)
[DEBUG] Returning (2682, 3584) activations
[DEBUG] train activations shape: (2682, 3584), dtype: float16
[DEBUG] train activations range: [-1526.000000, 117.875000]
Train activations: (2682, 3584)
[DEBUG] Pre-fit sample counts — X: 2682, y: 2682
[DEBUG] y_train class distribution: {0: 2146, 1: 536}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_mean

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1751, 3584)
Input y shape: (1751,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 0.71GB, GPU_allocated: 0.00GB, GPU_reserved: 0.00GB
[DEBUG] Input activations shape: (1751, 3584)
[DEBUG] Using pre-aggregated inputs: (1751, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                               | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████| 2/2 [00:00<00:00, 26.34it/s]
[DEBUG] Final encoded shape: (1751, 16384)
Encoded feature matrix shape: (1751, 16384)
[DEBUG] After encoding. Memory: RAM: 1.20GB, GPU_allocated: 0.47GB, GPU_reserved: 1.10GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1751, 16384), y_train=(1751,)
[DEBUG] Memory at start of feature selection: RAM: 1.20GB, GPU_allocated: 0.47GB, GPU_reserved: 1.10GB
[DEBUG] Feature matrix for selection shape: (1751, 16384)
[DEBUG] Memory after feature matrix build: RAM: 1.20GB, GPU_allocated: 0.47GB, GPU_reserved: 1.10GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 1.20GB, GPU_allocated: 0.47GB, GPU_reserved: 1.10GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 1.20GB, GPU_allocated: 0.47GB, GPU_reserved: 1.10GB
Selected 3584 features
[DEBUG] X_selected shape: (1751, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  [DEBUG] Fitting SAE probe normally...

=== Group 29/700 — ('94_better_spam', 20, 'resid_post', 50, '{"class_counts": {"0": 1750, "1": 2}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set

=== Group 16/700 — ('94_better_spam', 20, 'resid_post', 47, '{"class_counts": {"0": 1750, "1": 1}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                               | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████| 1/1 [00:00<00:00,  9.17it/s]Encoding activations: 100%|███████████████████████| 1/1 [00:00<00:00,  9.15it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                               | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████| 1/1 [00:00<00:00, 36.96it/s]
[DEBUG] Final encoded shape: (537, 262144)
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                               | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████| 1/1 [00:00<00:00, 36.99it/s]
[DEBUG] Final encoded shape: (537, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.9813780260707635, 'auc': 0.9918713323190611, 'precision': 0.9532710280373832, 'recall': 0.9532710280373832, 'fpr': 0.011627906976744186}

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1751, 3584)
Input y shape: (1751,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 0.72GB, GPU_allocated: 0.00GB, GPU_reserved: 0.00GB
[DEBUG] Input activations shape: (1751, 3584)
[DEBUG] Using pre-aggregated inputs: (1751, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                               | 0/2 [00:00<?, ?it/s]Encoding activations:  50%|███████████▌           | 1/2 [00:00<00:00,  6.35it/s]Encoding activations: 100%|███████████████████████| 2/2 [00:00<00:00, 11.05it/s]
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (5157, 3584)
[DEBUG] Returning (5157, 3584) activations
[DEBUG] test activations shape: (5157, 3584), dtype: float16
[DEBUG] test activations range: [-1522.000000, 113.625000]
Test activations: (5157, 3584)
  - 🤗 Calculating metrics...

=== Group 11/700 — ('94_better_spam', 20, 'resid_post', 42, '{"class_counts": {"0": 1750, "1": 1}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-1337.000000, 219.750000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Final encoded shape: (1751, 262144)
Encoded feature matrix shape: (1751, 262144)
[DEBUG] After encoding. Memory: RAM: 2.82GB, GPU_allocated: 7.03GB, GPU_reserved: 17.51GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1751, 262144), y_train=(1751,)
[DEBUG] Memory at start of feature selection: RAM: 2.82GB, GPU_allocated: 7.03GB, GPU_reserved: 17.51GB
[DEBUG] Feature matrix for selection shape: (1751, 262144)
[DEBUG] Memory after feature matrix build: RAM: 2.82GB, GPU_allocated: 7.03GB, GPU_reserved: 17.51GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 2.82GB, GPU_allocated: 7.03GB, GPU_reserved: 17.51GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 2.82GB, GPU_allocated: 7.03GB, GPU_reserved: 17.51GB
Selected 3584 features
[DEBUG] X_selected shape: (1751, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                               | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████| 1/1 [00:00<00:00, 16.62it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                               | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████| 1/1 [00:00<00:00, 312.94it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                               | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████| 1/1 [00:00<00:00, 318.11it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.7985074626865671, 'auc': 0.9595539095566942, 'precision': 0.9819277108433735, 'recall': 0.6082089552238806, 'fpr': 0.011194029850746268}
  [DEBUG] Fitting SAE probe normally...

=== Group 22/700 — ('94_better_spam', 20, 'resid_post', 43, '{"class_counts": {"0": 1750, "1": 2}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1751, 3584)
Input y shape: (1751,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 0.71GB, GPU_allocated: 0.00GB, GPU_reserved: 0.00GB
[DEBUG] Input activations shape: (1751, 3584)
[DEBUG] Using pre-aggregated inputs: (1751, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                               | 0/2 [00:00<?, ?it/s]Encoding activations:  50%|███████████▌           | 1/2 [00:00<00:00,  6.33it/s][DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-252.250000, 76.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
Encoding activations: 100%|███████████████████████| 2/2 [00:00<00:00, 11.03it/s]
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                               | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████| 2/2 [00:00<00:00, 291.61it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                               | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████| 2/2 [00:00<00:00, 323.29it/s]
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                               | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████| 2/2 [00:00<00:00, 323.45it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5335413416536662, 'auc': 0.6702962658287923, 'precision': 0.5981735159817352, 'recall': 0.20436817472698907, 'fpr': 0.1372854914196568}
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                               | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|█████████▏             | 2/5 [00:00<00:00, 16.47it/s]Encoding activations:  80%|██████████████████▍    | 4/5 [00:00<00:00, 16.15it/s]Encoding activations: 100%|███████████████████████| 5/5 [00:00<00:00, 19.92it/s]
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-227.500000, 81.187500]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Final encoded shape: (5157, 262144)
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                               | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|█████████▏             | 2/5 [00:00<00:00, 17.69it/s]
=== Group 13/700 — ('94_better_spam', 20, 'resid_post', 44, '{"class_counts": {"0": 1750, "1": 1}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
Encoding activations:  80%|██████████████████▍    | 4/5 [00:00<00:00, 17.67it/s]Encoding activations: 100%|███████████████████████| 5/5 [00:00<00:00, 21.70it/s]
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-120.625000, 237.000000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                               | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████| 1/1 [00:00<00:00, 265.03it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                               | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████| 1/1 [00:00<00:00, 140.93it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                               | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████| 1/1 [00:00<00:00, 308.59it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.8612859211405659, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Final encoded shape: (5157, 262144)
[DEBUG] Input activations shape: (5157, 3584)
[DEBUG] Using pre-aggregated inputs: (5157, 3584)
[DEBUG] Processing 5 batches of size 1280
Encoding activations:   0%|                               | 0/5 [00:00<?, ?it/s]Encoding activations:  40%|█████████▏             | 2/5 [00:00<00:00, 17.70it/s]Encoding activations:  80%|██████████████████▍    | 4/5 [00:00<00:00, 17.67it/s]Encoding activations: 100%|███████████████████████| 5/5 [00:00<00:00, 21.70it/s]
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                               | 0/1 [00:00<?, ?it/s]  [DEBUG] Fitting SAE probe normally...
Encoding activations: 100%|███████████████████████| 1/1 [00:00<00:00, 17.50it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                               | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████| 1/1 [00:00<00:00, 338.44it/s]
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-231.750000, 90.750000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                               | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████| 1/1 [00:00<00:00, 344.08it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5652985074626866, 'auc': 0.8538371574961016, 'precision': 0.8723404255319149, 'recall': 0.15298507462686567, 'fpr': 0.022388059701492536}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Final encoded shape: (5157, 262144)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.8956757804925344, 'auc': 0.9135543721128828, 'precision': 0.5695006747638327, 'recall': 0.6583463338533542, 'fpr': 0.07063773250664304}
  🚀 [BATCH] Training probe: sae_262k_l0_259_max
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_max
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1751, 3584)
[DEBUG] Returning (1751, 3584) activations
[DEBUG] train activations shape: (1751, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 231.875000]
Train activations: (1751, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_last

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: mean
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (2682, 3584)
Input y shape: (2682,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 0.72GB, GPU_allocated: 0.00GB, GPU_reserved: 0.00GB
[DEBUG] Input activations shape: (2682, 3584)
[DEBUG] Using pre-aggregated inputs: (2682, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 3 batches of size 1280
Encoding activations:   0%|                               | 0/3 [00:00<?, ?it/s]
=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: last
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1752, 3584)
Input y shape: (1752,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 0.68GB, GPU_allocated: 0.00GB, GPU_reserved: 0.00GB
[DEBUG] Input activations shape: (1752, 3584)
[DEBUG] Using pre-aggregated inputs: (1752, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                               | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████| 2/2 [00:00<00:00, 20.75it/s]
Encoding activations:  33%|███████▋               | 1/3 [00:00<00:00,  3.54it/s]Encoding activations: 100%|███████████████████████| 3/3 [00:00<00:00,  8.64it/s]
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                               | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████| 1/1 [00:00<00:00, 15.30it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                               | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████| 1/1 [00:00<00:00, 225.99it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                               | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████| 1/1 [00:00<00:00, 226.72it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.6697761194029851, 'auc': 0.8941161728670082, 'precision': 0.989247311827957, 'recall': 0.34328358208955223, 'fpr': 0.0037313432835820895}
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Final encoded shape: (1752, 16384)
Encoded feature matrix shape: (1752, 16384)
[DEBUG] After encoding. Memory: RAM: 1.18GB, GPU_allocated: 0.47GB, GPU_reserved: 1.10GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1752, 16384), y_train=(1752,)
[DEBUG] Memory at start of feature selection: RAM: 1.18GB, GPU_allocated: 0.47GB, GPU_reserved: 1.10GB
[DEBUG] Feature matrix for selection shape: (1752, 16384)
[DEBUG] Memory after feature matrix build: RAM: 1.18GB, GPU_allocated: 0.47GB, GPU_reserved: 1.10GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 1.18GB, GPU_allocated: 0.47GB, GPU_reserved: 1.10GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 1.18GB, GPU_allocated: 0.47GB, GPU_reserved: 1.10GB
Selected 3584 features
[DEBUG] X_selected shape: (1752, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Final encoded shape: (2682, 262144)
Encoded feature matrix shape: (2682, 262144)
[DEBUG] After encoding. Memory: RAM: 3.73GB, GPU_allocated: 7.03GB, GPU_reserved: 17.51GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(2682, 262144), y_train=(2682,)
[DEBUG] Memory at start of feature selection: RAM: 3.73GB, GPU_allocated: 7.03GB, GPU_reserved: 17.51GB
[DEBUG] Feature matrix for selection shape: (2682, 262144)
[DEBUG] Memory after feature matrix build: RAM: 3.73GB, GPU_allocated: 7.03GB, GPU_reserved: 17.51GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 3.73GB, GPU_allocated: 7.03GB, GPU_reserved: 17.51GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 3.73GB, GPU_allocated: 7.03GB, GPU_reserved: 17.51GB
Selected 3584 features
[DEBUG] X_selected shape: (2682, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-1337.000000, 219.750000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-567.000000, 51.500000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (1751, 3584)
[DEBUG] Returning (1751, 3584) activations
[DEBUG] train activations shape: (1751, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1751, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_max
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
    💀💀💀 ERROR evaluating test on '94_better_spam': CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1752, 3584)
[DEBUG] Returning (1752, 3584) activations
[DEBUG] train activations shape: (1752, 3584), dtype: float16
[DEBUG] train activations range: [-162.375000, 237.000000]
Train activations: (1752, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_softmax
[DEBUG] Final encoded shape: (1751, 262144)
Encoded feature matrix shape: (1751, 262144)
[DEBUG] After encoding. Memory: RAM: 2.80GB, GPU_allocated: 7.03GB, GPU_reserved: 17.51GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1751, 262144), y_train=(1751,)
[DEBUG] Memory at start of feature selection: RAM: 2.80GB, GPU_allocated: 7.03GB, GPU_reserved: 17.51GB
[DEBUG] Feature matrix for selection shape: (1751, 262144)
[DEBUG] Memory after feature matrix build: RAM: 2.80GB, GPU_allocated: 7.03GB, GPU_reserved: 17.51GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 2.80GB, GPU_allocated: 7.03GB, GPU_reserved: 17.51GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 2.80GB, GPU_allocated: 7.03GB, GPU_reserved: 17.51GB
Selected 3584 features
[DEBUG] X_selected shape: (1751, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_mean_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set

=== Group 19/700 — ('94_better_spam', 20, 'resid_post', 50, '{"class_counts": {"0": 1750, "1": 1}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1755, 3584)
[DEBUG] Returning (1755, 3584) activations
[DEBUG] train activations shape: (1755, 3584), dtype: float16
[DEBUG] train activations range: [-162.375000, 237.000000]
Train activations: (1755, 3584)
[DEBUG] Pre-fit sample counts — X: 1755, y: 1755
[DEBUG] y_train class distribution: {0: 1750, 1: 5}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_softmax
  [DEBUG] Fitting SAE probe normally...
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_last
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-259.000000, 76.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-256.250000, 90.750000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1755, 3584)
Input y shape: (1755,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 1.16GB, GPU_allocated: 0.47GB, GPU_reserved: 1.75GB
[DEBUG] Input activations shape: (1755, 3584)
[DEBUG] Using pre-aggregated inputs: (1755, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                               | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████| 2/2 [00:00<00:00, 188.94it/s]
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
    💀💀💀 ERROR evaluating on '87_is_spam': CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
    💀💀💀 ERROR evaluating validation on '94_better_spam': CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Final encoded shape: (1755, 16384)
Encoded feature matrix shape: (1755, 16384)
[DEBUG] After encoding. Memory: RAM: 1.28GB, GPU_allocated: 0.91GB, GPU_reserved: 1.75GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(1755, 16384), y_train=(1755,)
[DEBUG] Memory at start of feature selection: RAM: 1.28GB, GPU_allocated: 0.91GB, GPU_reserved: 1.75GB
[DEBUG] Feature matrix for selection shape: (1755, 16384)
[DEBUG] Memory after feature matrix build: RAM: 1.28GB, GPU_allocated: 0.91GB, GPU_reserved: 1.75GB
[DEBUG] Difference vector shape: (16384,)
[DEBUG] Memory after difference calculation: RAM: 1.28GB, GPU_allocated: 0.91GB, GPU_reserved: 1.75GB
Selected top 3584 features from 16384 total features
[DEBUG] Memory at end of feature selection: RAM: 1.28GB, GPU_allocated: 0.91GB, GPU_reserved: 1.75GB
Selected 3584 features
[DEBUG] X_selected shape: (1755, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
    💀💀💀 ERROR evaluating validation on '94_better_spam': CUDA out of memory. Tried to allocate 3.50 GiB. GPU 0 has a total capacity of 94.50 GiB of which 317.19 MiB is free. Process 271183 has 638.00 MiB memory in use. Process 271303 has 28.70 GiB memory in use. Process 271298 has 2.44 GiB memory in use. Including non-PyTorch memory, this process has 18.19 GiB memory in use. Process 271278 has 2.44 GiB memory in use. Process 271304 has 1.78 GiB memory in use. Process 271294 has 1.78 GiB memory in use. Process 271302 has 18.19 GiB memory in use. Process 271296 has 18.19 GiB memory in use. Process 271289 has 1.78 GiB memory in use. Of the allocated memory 14.04 GiB is allocated by PyTorch, and 3.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
    [BATCH] Evaluating on 94_better_spam test set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                               | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████| 1/1 [00:00<00:00, 204.31it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                               | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████| 1/1 [00:00<00:00, 231.21it/s]
[DEBUG] Final encoded shape: (536, 16384)
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                               | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████| 1/1 [00:00<00:00, 222.86it/s]
[DEBUG] Final encoded shape: (536, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.8612859211405659, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
    💀💀💀 ERROR evaluating test on '94_better_spam': CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 94.50 GiB of which 93.75 MiB is free. Process 271183 has 638.00 MiB memory in use. Process 271303 has 28.70 GiB memory in use. Process 271298 has 2.44 GiB memory in use. Process 271283 has 18.19 GiB memory in use. Process 271278 has 2.44 GiB memory in use. Including non-PyTorch memory, this process has 2.00 GiB memory in use. Process 271294 has 1.78 GiB memory in use. Process 271302 has 18.19 GiB memory in use. Process 271296 has 18.19 GiB memory in use. Process 271289 has 1.78 GiB memory in use. Of the allocated memory 1.13 GiB is allocated by PyTorch, and 193.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

=== Group 28/700 — ('94_better_spam', 20, 'resid_post', 49, '{"class_counts": {"0": 1750, "1": 2}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_2_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_max aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_max.npz
[DEBUG] Loaded sae_max aggregated activations: (2682, 3584)
[DEBUG] Returning (2682, 3584) activations
[DEBUG] train activations shape: (2682, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.500000]
Train activations: (2682, 3584)
[DEBUG] Pre-fit sample counts — X: 2682, y: 2682
[DEBUG] y_train class distribution: {0: 2146, 1: 536}
  [DEBUG] Creating probe for architecture: sae_262k_l0_259_max
  [DEBUG] Fitting SAE probe normally...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1336.000000, 237.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1336.000000, 221.000000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
Loaded SAE: layer_20/width_16k/average_l0_408
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                               | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████| 2/2 [00:00<00:00, 252.03it/s]
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
    💀💀💀 ERROR evaluating on '87_is_spam': CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 94.50 GiB of which 94.06 MiB is free. Process 271183 has 638.00 MiB memory in use. Process 271303 has 28.70 GiB memory in use. Process 271298 has 2.44 GiB memory in use. Process 271283 has 18.19 GiB memory in use. Process 271278 has 2.44 GiB memory in use. Including non-PyTorch memory, this process has 2.00 GiB memory in use. Process 271294 has 1.78 GiB memory in use. Process 271302 has 18.19 GiB memory in use. Process 271296 has 18.19 GiB memory in use. Process 271289 has 1.78 GiB memory in use. Of the allocated memory 1.13 GiB is allocated by PyTorch, and 193.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                               | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████| 2/2 [00:00<00:00, 267.05it/s]
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
[DEBUG] Final encoded shape: (1282, 16384)
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
[DEBUG] Processing 2 batches of size 1280
Encoding activations:   0%|                               | 0/2 [00:00<?, ?it/s]Encoding activations: 100%|██████████████████████| 2/2 [00:00<00:00, 265.96it/s]
[DEBUG] Final encoded shape: (1282, 16384)
  - ❤️‍🔥 Success! Metrics: {'acc': 0.5, 'auc': 0.5524567940595939, 'precision': 0.0, 'recall': 0.0, 'fpr': 0.0}
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
    💀💀💀 ERROR evaluating test on '94_better_spam': CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 94.50 GiB of which 94.19 MiB is free. Process 271183 has 638.00 MiB memory in use. Process 271303 has 28.70 GiB memory in use. Process 271298 has 2.44 GiB memory in use. Process 271283 has 18.19 GiB memory in use. Process 271278 has 2.44 GiB memory in use. Process 271304 has 2.00 GiB memory in use. Including non-PyTorch memory, this process has 1.78 GiB memory in use. Process 271302 has 18.19 GiB memory in use. Process 271296 has 18.19 GiB memory in use. Process 271289 has 1.78 GiB memory in use. Of the allocated memory 928.29 MiB is allocated by PyTorch, and 193.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-256.250000, 81.875000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (536, 3584)
[DEBUG] Using pre-aggregated inputs: (536, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
    💀💀💀 ERROR evaluating validation on '94_better_spam': CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 94.50 GiB of which 94.38 MiB is free. Process 271183 has 638.00 MiB memory in use. Process 271303 has 28.70 GiB memory in use. Process 271298 has 2.44 GiB memory in use. Process 271283 has 18.19 GiB memory in use. Process 271278 has 2.44 GiB memory in use. Process 271304 has 2.00 GiB memory in use. Process 271294 has 1.78 GiB memory in use. Process 271302 has 18.19 GiB memory in use. Process 271296 has 18.19 GiB memory in use. Including non-PyTorch memory, this process has 1.78 GiB memory in use. Of the allocated memory 928.29 MiB is allocated by PyTorch, and 193.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
    [BATCH] Evaluating on 94_better_spam test set
[DEBUG] Newly added count: 0
[DEBUG] Loading full activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post.npz
[DEBUG] Existing: 12750
[DEBUG] Processing 2682 full activations
[DEBUG] Estimated memory requirement: 6.09 GB
[WARNING] Large memory requirement (6.09 GB).
[DEBUG] Max sequence length: 1167
[DEBUG] Shape of first activation: (1, 61, 3584)
[DEBUG] Final array shape: (2682, 1167, 3584)
[DEBUG] Returning (2682, 1167, 3584) activations
[DEBUG] train activations shape: (2682, 1167, 3584), dtype: float16
[DEBUG] train activations range: [-1674.000000, 237.500000]
Train activations: (2682, 1167, 3584)
[DEBUG] Pre-fit sample counts — X: 2682, y: 2682
[DEBUG] y_train class distribution: {0: 2146, 1: 536}
  [DEBUG] Creating probe for architecture: attention
  [DEBUG] Creating attention probe
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/87_is_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1282, 3584)
[DEBUG] Returning (1282, 3584) activations
[DEBUG] test activations shape: (1282, 3584), dtype: float16
[DEBUG] test activations range: [-1306.000000, 211.500000]
Test activations: (1282, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (1282, 3584)
[DEBUG] Using pre-aggregated inputs: (1282, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408
    💀💀💀 ERROR evaluating on '87_is_spam': CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 94.50 GiB of which 94.38 MiB is free. Process 271183 has 638.00 MiB memory in use. Process 271303 has 28.70 GiB memory in use. Process 271298 has 2.44 GiB memory in use. Process 271283 has 18.19 GiB memory in use. Process 271278 has 2.44 GiB memory in use. Process 271304 has 2.00 GiB memory in use. Including non-PyTorch memory, this process has 1.78 GiB memory in use. Process 271302 has 18.19 GiB memory in use. Process 271296 has 18.19 GiB memory in use. Process 271289 has 1.78 GiB memory in use. Of the allocated memory 928.29 MiB is allocated by PyTorch, and 193.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_softmax
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (537, 3584)
[DEBUG] Returning (537, 3584) activations
[DEBUG] test activations shape: (537, 3584), dtype: float16
[DEBUG] test activations range: [-663.000000, 51.000000]
Test activations: (537, 3584)
  - 🤗 Calculating metrics...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: max
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (2682, 3584)
Input y shape: (2682,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 1.17GB, GPU_allocated: 7.03GB, GPU_reserved: 28.01GB
[DEBUG] Input activations shape: (2682, 3584)
[DEBUG] Using pre-aggregated inputs: (2682, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 3 batches of size 1280
Encoding activations:   0%|                               | 0/3 [00:00<?, ?it/s]Encoding activations:  67%|███████████████▎       | 2/3 [00:00<00:00, 15.50it/s]Encoding activations: 100%|███████████████████████| 3/3 [00:00<00:00, 21.76it/s]
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_mean aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_mean.npz
[DEBUG] Loaded sae_mean aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-813.000000, 51.906250]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Final encoded shape: (2682, 262144)
Encoded feature matrix shape: (2682, 262144)
[DEBUG] After encoding. Memory: RAM: 3.79GB, GPU_allocated: 14.04GB, GPU_reserved: 28.01GB
Selecting top features...
[DEBUG] Feature selection input shapes: X_train=(2682, 262144), y_train=(2682,)
[DEBUG] Memory at start of feature selection: RAM: 3.79GB, GPU_allocated: 14.04GB, GPU_reserved: 28.01GB
[DEBUG] Feature matrix for selection shape: (2682, 262144)
[DEBUG] Memory after feature matrix build: RAM: 3.79GB, GPU_allocated: 14.04GB, GPU_reserved: 28.01GB
[DEBUG] Difference vector shape: (262144,)
[DEBUG] Memory after difference calculation: RAM: 3.79GB, GPU_allocated: 14.04GB, GPU_reserved: 28.01GB
Selected top 3584 features from 262144 total features
[DEBUG] Memory at end of feature selection: RAM: 3.79GB, GPU_allocated: 14.04GB, GPU_reserved: 28.01GB
Selected 3584 features
[DEBUG] X_selected shape: (2682, 3584)
  - 🔥 Probe state saved to train_on_94_better_spam_sae_262k_l0_259_max_L20_resid_post_topk3584_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set

=== Group 15/700 — ('94_better_spam', 20, 'resid_post', 46, '{"class_counts": {"0": 1750, "1": 1}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_max
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_max_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_last
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_last_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_16k_l0_408_softmax
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_softmax_L20_resid_post_topk3584_class0_1750_class1_1_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
  - 😋 Using cached evaluation result 
    [BATCH] Evaluating on 94_better_spam test set
  - 😋 Using cached evaluation result 
  - 😋 Using cached evaluation result 
  🚀 [BATCH] Training probe: sae_262k_l0_259_mean
  - Training new probe …
  [DEBUG] Starting dataset preparation...
  [DEBUG] Using activation_type: sae_mean
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_last aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_last.npz
[DEBUG] Loaded sae_last aggregated activations: (1751, 3584)
[DEBUG] Returning (1751, 3584) activations
[DEBUG] train activations shape: (1751, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 231.875000]
Train activations: (1751, 3584)
[DEBUG] Pre-fit sample counts — X: 1751, y: 1751
[DEBUG] y_train class distribution: {0: 1750, 1: 1}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_last
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (1752, 3584)
[DEBUG] Returning (1752, 3584) activations
[DEBUG] train activations shape: (1752, 3584), dtype: float16
[DEBUG] train activations range: [-1377.000000, 237.000000]
Train activations: (1752, 3584)
[DEBUG] Pre-fit sample counts — X: 1752, y: 1752
[DEBUG] y_train class distribution: {0: 1750, 1: 2}
  [DEBUG] Creating probe for architecture: sae_16k_l0_408_softmax
  [DEBUG] Fitting SAE probe normally...
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Newly added count: 0
[DEBUG] Loading sae_softmax aggregated activations from activation_cache/google/gemma-2-9b/94_better_spam/blocks‑20‑hook_resid_post_aggregated_softmax.npz
[DEBUG] Loaded sae_softmax aggregated activations: (536, 3584)
[DEBUG] Returning (536, 3584) activations
[DEBUG] test activations shape: (536, 3584), dtype: float16
[DEBUG] test activations range: [-1337.000000, 237.500000]
Test activations: (536, 3584)
  - 🤗 Calculating metrics...

=== SAE PROBE TRAINING START (sklearn) ===
Model: google/gemma-2-9b, Layer: 20
SAE source: sae_lens
Aggregation: softmax
Top-k features: 3584
SAE encoding batch size: 1280
Input X shape: (1752, 3584)
Input y shape: (1752,)
Encoding activations through SAE...
[DEBUG] Before encoding. Memory: RAM: 0.77GB, GPU_allocated: 0.00GB, GPU_reserved: 0.00GB
[DEBUG] Input activations shape: (1752, 3584)
[DEBUG] Using pre-aggregated inputs: (1752, 3584)
Loading SAE via sae_lens: layer_20/width_16k/average_l0_408

=== Group 33/700 — ('94_better_spam', 20, 'resid_post', 44, '{"class_counts": {"0": 1750, "1": 5}}', False, 0.75, 0.1, 0.15) ===
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/94_better_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
Filtered dataset 94_better_spam: 3577 examples passed the filter
[DEBUG] Class distribution after filter: {0: 2863, 1: 714}
No overlap between train, val, and test sets.
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
Initializing ActivationManager with model: google/gemma-2-9b, device: cuda:0, cache_dir: activation_cache/google/gemma-2-9b/87_is_spam
[DEBUG] model_name parameter: google/gemma-2-9b
[DEBUG] self.model_name: google/gemma-2-9b
[DEBUG] model parameter: None
[DEBUG] model type: <class 'NoneType'>
[DEBUG] Creating read-only activation manager for model_name=google/gemma-2-9b
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
[DEBUG] Successfully created read-only activation manager
Successfully initialized ActivationManager
[INFO] Created read-only ActivationManager for google/gemma-2-9b (no tokenizer needed)
  🚀 [BATCH] Training probe: sae_16k_l0_408_mean
  - [SKIP] Probe already trained: train_on_94_better_spam_sae_16k_l0_408_mean_L20_resid_post_topk3584_class0_1750_class1_5_state.npz
  🤔 [BATCH] Evaluating on 2 datasets…
    [BATCH] Evaluating on 94_better_spam validation set
Test activations: (536, 3584)
  - 🤗 Calculating metrics...
[DEBUG] Input activations shape: (537, 3584)
[DEBUG] Using pre-aggregated inputs: (537, 3584)
Loading SAE via sae_lens: layer_20/width_262k/average_l0_259
Loaded SAE: layer_20/width_262k/average_l0_259
[DEBUG] Processing 1 batches of size 1280
Encoding activations:   0%|                               | 0/1 [00:00<?, ?it/s]Encoding activations: 100%|███████████████████████| 1/1 [00:00<00:00, 32.07it/s]
Completed 0/700 groups successfully
================================================================================
😜 Run finished. Closing log file.
joblib.externals.loky.process_executor._RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/lambda/nfs/riya-probing/pivotal-research/pivotal_env/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py", line 490, in _process_worker
    r = call_item()
  File "/lambda/nfs/riya-probing/pivotal-research/pivotal_env/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py", line 291, in __call__
    return self.fn(*self.args, **self.kwargs)
  File "/lambda/nfs/riya-probing/pivotal-research/pivotal_env/lib/python3.10/site-packages/joblib/parallel.py", line 607, in __call__
    return [func(*args, **kwargs) for func, args, kwargs in self.items]
  File "/lambda/nfs/riya-probing/pivotal-research/pivotal_env/lib/python3.10/site-packages/joblib/parallel.py", line 607, in <listcomp>
    return [func(*args, **kwargs) for func, args, kwargs in self.items]
  File "/lambda/nfs/riya-probing/pivotal-research/evaluating-probes/src/main.py", line 671, in _process_group
    train_single_probe(
  File "/lambda/nfs/riya-probing/pivotal-research/evaluating-probes/src/utils_training.py", line 446, in train_single_probe
    probe.fit(train_acts, y_train, train_masks)
  File "/lambda/nfs/riya-probing/pivotal-research/evaluating-probes/src/probes/sae_probe.py", line 389, in fit
    X_encoded = self._encode_activations(X)
  File "/lambda/nfs/riya-probing/pivotal-research/evaluating-probes/src/probes/sae_probe.py", line 302, in _encode_activations
    sae = self._load_sae()
  File "/lambda/nfs/riya-probing/pivotal-research/evaluating-probes/src/probes/sae_probe.py", line 254, in _load_sae
    self.sae, _, _ = SAE.from_pretrained(
  File "/lambda/nfs/riya-probing/pivotal-research/pivotal_env/lib/python3.10/site-packages/sae_lens/saes/sae.py", line 582, in from_pretrained
    return cls.from_pretrained_with_cfg_and_sparsity(
  File "/lambda/nfs/riya-probing/pivotal-research/pivotal_env/lib/python3.10/site-packages/sae_lens/saes/sae.py", line 654, in from_pretrained_with_cfg_and_sparsity
    cfg_dict, state_dict, log_sparsities = conversion_loader(
  File "/lambda/nfs/riya-probing/pivotal-research/pivotal_env/lib/python3.10/site-packages/sae_lens/loading/pretrained_sae_loaders.py", line 484, in gemma_2_sae_huggingface_loader
    torch.tensor(data[key]).to(dtype=torch.float32).to(device)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/usr/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/lambda/nfs/riya-probing/pivotal-research/evaluating-probes/src/main.py", line 988, in <module>
    main()
  File "/lambda/nfs/riya-probing/pivotal-research/evaluating-probes/src/main.py", line 969, in main
    run_batched_jobs(
  File "/lambda/nfs/riya-probing/pivotal-research/evaluating-probes/src/main.py", line 746, in run_batched_jobs
    results = Parallel(
  File "/lambda/nfs/riya-probing/pivotal-research/pivotal_env/lib/python3.10/site-packages/joblib/parallel.py", line 2072, in __call__
    return output if self.return_generator else list(output)
  File "/lambda/nfs/riya-probing/pivotal-research/pivotal_env/lib/python3.10/site-packages/joblib/parallel.py", line 1682, in _get_outputs
    yield from self._retrieve()
  File "/lambda/nfs/riya-probing/pivotal-research/pivotal_env/lib/python3.10/site-packages/joblib/parallel.py", line 1784, in _retrieve
    self._raise_error_fast()
  File "/lambda/nfs/riya-probing/pivotal-research/pivotal_env/lib/python3.10/site-packages/joblib/parallel.py", line 1859, in _raise_error_fast
    error_job.get_result(self.timeout)
  File "/lambda/nfs/riya-probing/pivotal-research/pivotal_env/lib/python3.10/site-packages/joblib/parallel.py", line 758, in get_result
    return self._return_or_raise()
  File "/lambda/nfs/riya-probing/pivotal-research/pivotal_env/lib/python3.10/site-packages/joblib/parallel.py", line 773, in _return_or_raise
    raise self._result
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

/usr/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 9 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
2025-08-17 14:54:45 Command exited with status 1. Will restart.
